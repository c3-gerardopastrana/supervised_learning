diff --git a/lda.py b/lda.py
index 5b8bae9..d99fab3 100644
--- a/lda.py
+++ b/lda.py
@@ -165,9 +165,6 @@ class LDA(nn.Module):
         self.running_stats = None  # Stores cumulative LDA stats
 
     def forward(self, X, y):
-        X = X.view(X.shape[0], -1).detach()
-        y = y.detach()
-
         # Initialize or update running stats
         if self.running_stats is None:
             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
@@ -190,6 +187,7 @@ class LDA(nn.Module):
 
         Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
 
+        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
         temp = torch.linalg.solve(Sw, Sb)
         evals_complex, evecs_complex = torch.linalg.eig(temp)
 
@@ -246,8 +244,8 @@ class RunningLDAStats:
 
     @torch.no_grad()
     def update(self, X, y):
-        X = X.view(X.shape[0], -1).cpu()
-        y = y.cpu()
+        X = X.view(X.shape[0], -1).detach().to('cpu')
+        y = y.detach().to('cpu')
 
         for cls in range(self.n_classes):
             mask = (y == cls)
diff --git a/train.py b/train.py
index 146c137..9b70f2e 100644
--- a/train.py
+++ b/train.py
@@ -1,200 +1,73 @@
 import os
 import random
-import numpy as np
+import gc
 from collections import defaultdict
+from functools import partial
+
+import numpy as np
 np.set_printoptions(precision=4, suppress=True)
-from sklearn.metrics import accuracy_score
+
+from PIL import Image
 from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from PIL import Image
-import torchvision
-from torchvision import transforms, datasets
 import torch.optim as optim
-import wandb
-from functools import partial
-from lda import LDA, lda_loss, sina_loss, SphericalLDA
 import torch.distributed as dist
 import torch.multiprocessing as mp
 from torch.nn.parallel import DistributedDataParallel as DDP
-from torch.utils.data.distributed import DistributedSampler
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
 from torch.utils.data import DataLoader, random_split, Sampler, Subset
-from torch.utils.checkpoint import checkpoint
-
-
-class BasicBlock(nn.Module):
-    expansion = 1
-    def __init__(self, in_planes, planes, stride=1):
-        super(BasicBlock, self).__init__()
-        self.conv1 = nn.Conv2d(
-            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
-        self.bn1 = nn.BatchNorm2d(planes)
-        self.conv2 = nn.Conv2d(
-            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm2d(planes)
-        self.shortcut = nn.Sequential()
-        if stride != 1 or in_planes != self.expansion * planes:
-            self.shortcut = nn.Sequential(
-                nn.Conv2d(in_planes, self.expansion * planes,
-                          kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(self.expansion * planes)
-            )
-    
-    def _forward_impl(self, x):
-        out = F.relu(self.bn1(self.conv1(x)))
-        out = self.bn2(self.conv2(out))
-        out += self.shortcut(x)
-        out = F.relu(out)
-        return out
-        
-    def forward(self, x):
-        return checkpoint(self._forward_impl, x)
-
-class ResNet(nn.Module):
-    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
-        super(ResNet, self).__init__()
-        self.lda_args = lda_args
-        self.in_planes = 64
-        self.use_checkpoint = use_checkpoint
-        
-        # ImageNet-style initial conv layer
-        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
-                               stride=2, padding=3, bias=False)
-        self.bn1 = nn.BatchNorm2d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-        
-        # Residual layers
-        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
-        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
-        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
-        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
-        
-        # Global average pooling and output
-        self.avgpool = nn.AdaptiveAvgPool2d(1)
-        self.linear = nn.Linear(512 * block.expansion, num_classes)
-        
-        # LDA branch (if enabled)
-        if self.lda_args:
-            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
-    
-    def _make_layer(self, block, planes, num_blocks, stride):
-        strides = [stride] + [1] * (num_blocks - 1)
-        layers = []
-        for stride in strides:
-            layers.append(block(self.in_planes, planes, stride))
-            self.in_planes = planes * block.expansion
-        return nn.Sequential(*layers)
-    
-    def _forward_features(self, x):
-        out = self.relu(self.bn1(self.conv1(x)))
-        out = self.maxpool(out)
-        
-        if self.use_checkpoint:
-            out = checkpoint(lambda x: self.layer1(x), out)
-            out = checkpoint(lambda x: self.layer2(x), out)
-            out = checkpoint(lambda x: self.layer3(x), out)
-            out = checkpoint(lambda x: self.layer4(x), out)
-        else:
-            out = self.layer1(out)
-            out = self.layer2(out)
-            out = self.layer3(out)
-            out = self.layer4(out)
-            
-        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
-        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
-        return fea
-    
-    def forward(self, x, y=None, epoch=0):
-        fea = self._forward_features(x)
-        
-        if self.lda_args:
-            fea = F.normalize(fea, p=2, dim=1)
-            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
-            return hasComplexEVal, fea, out, sigma_w_inv_b
-        else:
-            out = self.linear(fea)
-            return out
-
-
-def ResNet18(num_classes=1000, lda_args=None):
-    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
-
-
-def ResNet34():
-    return ResNet(BasicBlock, [3, 4, 6, 3])
-
-
-def ResNet50():
-    return ResNet(Bottleneck, [3, 4, 6, 3])
-
-
-def ResNet101():
-    return ResNet(Bottleneck, [3, 4, 23, 3])
-
-
-def ResNet152():
-    return ResNet(Bottleneck, [3, 8, 36, 3])
-
-
-class CIFAR10:
-    def __init__(self, img_names, class_map, transform):
-        self.img_names = img_names
-        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
-        self.transform = transform
-    def __len__(self):
-        return len(self.img_names)
-    def __getitem__(self, idx):
-        img = Image.open(self.img_names[idx])
-        img = self.transform(img)
-        clazz = self.classes[idx]
-        return img, clazz
-
+from torch.utils.data.distributed import DistributedSampler
 
+import torchvision
+from torchvision import transforms, datasets
 
-def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
-    """
-    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+import wandb
 
-    Args:
-        batch_size (int): new batch size
-        base_batch_size (int): original batch size corresponding to base_lr
-        base_lr (float): base learning rate at base_batch_size
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
 
-    Returns:
-        float: scaled learning rate
-    """
-    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
-    return base_lr * scale.item()
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
 
 
 class Solver:
-    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
         self.dataloaders = dataloaders
         self.local_rank = local_rank
         self.world_size = world_size
         self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
         
-        self.net = ResNet18(n_classes, lda_args)
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
         self.net = self.net.to(self.device)
         
         # Wrap model with DDP
         if world_size > 1:
-            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
         
         self.use_lda = True if lda_args else False
         if self.use_lda:
-            self.criterion = partial(lda_loss, n_classes=n_classes, 
-                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
-            self.criterion = sina_loss
+            self.criterion = sina_loss  # Assuming this is defined elsewhere
         else:
             self.criterion = nn.CrossEntropyLoss()
         
         if local_rank == 0:
-            print(self.criterion)
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
 
         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
         self.model_path = model_path
         self.n_classes = n_classes
 
@@ -211,82 +84,110 @@ class Solver:
         entropy_sum = 0.0
         entropy_count = 0
 
+        # Clear CUDA cache before each epoch
+        torch.cuda.empty_cache()
+        gc.collect()
         
         for batch_idx, (inputs, targets) in enumerate(dataloader):
-            inputs = inputs.to(self.device)
-            targets = targets.to(self.device)
-            self.optimizer.zero_grad()
-        
-            if self.use_lda:
-                if isinstance(self.net, DDP):
-                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
-                else:
-                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+            # Move data to device
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+            
+            # For training with gradient accumulation
+            if phase == 'train':
+                effective_batch_idx = batch_idx % self.gradient_accumulation_steps
                 
-                if not hasComplexEVal:
-                    #stats
-                    eigvals_norm = outputs / outputs.sum()
-                    eps = 1e-10 
-                    max_eigval_norm = eigvals_norm.max().item()
-                    min_eigval_norm = eigvals_norm.min().item()
-                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
-                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
-                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
-                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
-                    eigvals_norm /= eigvals_norm.sum()
-                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
-                    entropy_sum += entropy
-                    entropy_count += 1
-                    trace = torch.trace(sigma_w_inv_b)
-                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
-                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
-                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
-                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
-                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
-    
-                    loss = self.criterion(sigma_w_inv_b)
-
-                    if isinstance(self.net, DDP):
-                        outputs = self.net.module.lda.predict_proba(feas)
+                #if effective_batch_idx == 0:
+                self.optimizer.zero_grad(set_to_none=True)  # More memory efficient
+                
+                # Apply mixed precision for training
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            # Stats calculation (same as original)
+                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+                            entropy_sum += metrics["entropy"]
+                            entropy_count += 1
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                            
+                            # Only log on rank 0 for efficiency
+                            if phase == 'train' and self.local_rank == 0:
+                                wandb.log(metrics, commit=False)
+                                wandb.log({
+                                    'loss': loss.item(),
+                                    'epoch': epoch,
+                                }, commit=False)
+                        else:
+                            if self.local_rank == 0:
+                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+                            continue
                     else:
-                        outputs = self.net.lda.predict_proba(feas)
-
-                    if phase == 'train' and self.local_rank == 0:
-                        wandb.log({
-                            'loss': loss,
-                            "rank simga": rank_sigma,
-                            "condition simga": condition_sigma,
-                            "entropy": entropy,
-                            "sum_squared_off_diag": sum_squared_off_diag,
-                            "diag_var": diag_var,
-                            "trace": trace,
-                            "max normalized eigenvalue": max_eigval_norm,
-                            "min normalized eigenvalue": min_eigval_norm,
-                            "quantile_25": quantile_25,
-                            "quantile_50": quantile_50,
-                            "quantile_75": quantile_75,
-                            "epoch": epoch,
-                        })
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+                
+                # Scale loss for gradient accumulation
+                #loss = loss / self.gradient_accumulation_steps
+                
+                if phase == 'train':
+                    # Use gradient scaler for mixed precision
+                    self.scaler.scale(loss).backward()
+                    
+                    # Step optimizer at effective batch boundaries
+                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+                    # Unscale before clipping
+                    self.scaler.unscale_(self.optimizer)
+                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                    
+                    # Update with scaler
+                    self.scaler.step(self.optimizer)
+                    self.scaler.update()
                     
-                else:
                     if self.local_rank == 0:
-                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
-                    continue
+                        wandb.log({"grad_norm": grad_norm.item()})
             else:
-                outputs = self.net(inputs, targets, epoch)
-                loss = nn.CrossEntropyLoss()(outputs, targets)
-        
-            if phase == 'train':
-                loss.backward()
-                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
-                self.optimizer.step()
-                if self.local_rank == 0:
-                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
-            total_loss += loss.item()
-    
+                # Validation phase - no gradients needed
+                with torch.no_grad():
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                        else:
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+            
+            # Accumulate metrics
+            total_loss += loss.item()  if phase == 'train' else loss.item()
+            
             outputs = torch.argmax(outputs.detach(), dim=1)
             total += targets.size(0)
             correct += outputs.eq(targets).sum().item()
+            
+            # Free memory after each batch
+            del inputs, targets, outputs
+            if phase == 'train' and self.use_lda and not hasComplexEVal:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
         
         # Sync metrics across GPUs
         if self.world_size > 1:
@@ -300,17 +201,17 @@ class Solver:
         else:
             total_acc = 0 
         
+        # Log metrics
         if self.local_rank == 0:
             if entropy_count > 0:
                 average_entropy = entropy_sum / entropy_count
                 print(f'Average Entropy: {average_entropy:.4f}')
             
-            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
-                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
             wandb.log({
-                "epoch"+phase: epoch,
-                "total"+phase: total_loss,
-                "total_acc_train"+phase: 100.*total_acc
+                f"epoch_{phase}": epoch,
+                f"loss_{phase}": total_loss,
+                f"acc_{phase}": 100.*total_acc
             }) 
         return total_loss, total_acc
 
@@ -319,18 +220,24 @@ class Solver:
         best_loss = float('inf')
         for epoch in range(epochs):
             # Set epoch for distributed samplers
-            
             if self.world_size > 1:
                 for phase in self.dataloaders:
                     if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
                         self.dataloaders[phase].sampler.set_epoch(epoch)
-                
+            
+            # Training phase
             self.iterate(epoch, 'train')
+            
+            # Validation phase
             with torch.no_grad():
-                self.net.module.lda.finalize_running_stats()
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+                    self.net.module.lda.finalize_running_stats()
                 val_loss, val_acc = self.iterate(epoch, 'val')
-                self.net.module.lda.reset_running_stats()
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+                    self.net.module.lda.reset_running_stats()
                 
+                
+            # Save best model
             if val_loss < best_loss and self.local_rank == 0:
                 best_loss = val_loss
                 if isinstance(self.net, DDP):
@@ -351,104 +258,6 @@ class Solver:
                 checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
             torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
 
-    def test_iterate(self, epoch, phase):
-        self.net.module.lda.finalize_running_stats()
-        if isinstance(self.net, DDP):
-            self.net.module.eval()
-        else:
-            self.net.eval()
-            
-        dataloader = self.dataloaders[phase]
-        y_pred = []
-        y_true = []
-        
-        with torch.no_grad():
-            for inputs, targets in dataloader:
-                inputs = inputs.to(self.device)
-                targets = targets.to(self.device)
-                
-                if self.use_lda:
-                    if isinstance(self.net, DDP):
-                        _, feas, outputs = self.net.module(inputs, targets, epoch)
-                        outputs = self.net.module.lda.predict_proba(feas)
-                    else:
-                        _, feas, outputs = self.net(inputs, targets, epoch)
-                        outputs = self.net.lda.predict_proba(feas)
-                else:
-                    outputs = self.net(inputs, targets, epoch)
-                    
-                outputs = torch.argmax(outputs, dim=1)
-                y_pred.append(outputs.detach().cpu().numpy())
-                y_true.append(targets.detach().cpu().numpy())
-                
-        # Gather predictions from all GPUs
-        if self.world_size > 1:
-            all_y_pred = []
-            all_y_true = []
-            
-            # Convert lists to tensors for gathering
-            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
-            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
-            
-            # Get sizes from all processes
-            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
-            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
-            dist.all_gather(all_sizes, size_tensor)
-            
-            # Prepare tensors for gathering
-            max_size = max(size.item() for size in all_sizes)
-            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
-            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
-            
-            # Copy data to padded tensors
-            size = local_y_pred.size(0)
-            padded_pred[:size] = local_y_pred
-            padded_true[:size] = local_y_true
-            
-            # Gather padded tensors
-            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
-            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
-            
-            dist.all_gather(gathered_pred, padded_pred)
-            dist.all_gather(gathered_true, padded_true)
-            
-            # Truncate according to original sizes and convert to numpy
-            for i, size in enumerate(all_sizes):
-                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
-                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
-                
-            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
-        else:
-            return np.concatenate(y_pred), np.concatenate(y_true)
-        
-    def test(self):
-        if self.local_rank == 0:
-            checkpoint = torch.load(self.model_path)
-            epoch = checkpoint['epoch']
-            val_loss = checkpoint['val_loss']
-            
-            if isinstance(self.net, DDP):
-                self.net.module.load_state_dict(checkpoint['state_dict'])
-            else:
-                self.net.load_state_dict(checkpoint['state_dict'])
-                
-            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
-            
-        # Synchronize all processes to ensure the model is loaded
-        if self.world_size > 1:
-            dist.barrier()
-            
-        y_pred, y_true = self.test_iterate(epoch, 'test')
-        
-        if self.local_rank == 0:
-            print(y_pred.shape, y_true.shape)
-            print('total', accuracy_score(y_true, y_pred))
-            for i in range(self.n_classes):
-                idx = y_true == i
-                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
-                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
-
-
 def setup(rank, world_size):
     os.environ['MASTER_ADDR'] = 'localhost'
     os.environ['MASTER_PORT'] = '12355'
@@ -459,10 +268,8 @@ def setup(rank, world_size):
 
 def cleanup():
     dist.destroy_process_group()
-
-
-def train_worker(rank, world_size, config):
     
+def train_worker(rank, world_size, config):
     class ClassBalancedBatchSampler(Sampler):
         def __init__(self, dataset, k_classes, n_samples,
                      world_size=1, rank=0, seed=42):
@@ -516,50 +323,90 @@ def train_worker(rank, world_size, config):
         def __iter__(self):
             g = torch.Generator()
             g.manual_seed(self.seed + self.epoch + self.rank)
-    
-            all_batches = []
-    
-            while len(all_batches) < self.batches_per_epoch:
-                # Pick k_classes randomly
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
                 selected_classes = torch.tensor(self.available_classes)
                 selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
-    
+            
                 batch = []
                 for cls in selected_classes.tolist():
                     indices = self.class_to_indices[cls]
                     indices_tensor = torch.tensor(indices)
                     chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
                     batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+    
+            # all_batches = []
+    
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
     
-                all_batches.append(batch)
+            #     all_batches.append(batch)
     
-            # Shard batches across GPUs
-            local_batches = all_batches[self.rank::self.world_size]
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
     
-            for batch in local_batches:
-                yield batch
+            # for batch in local_batches:
+            #     yield batch
     
         def __len__(self):
             return self.batches_per_epoch // self.world_size
-
-
-
+            
+    # Configure CUDA
+    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
     # Setup process group
     setup(rank, world_size)
     
     # Set the device
     torch.cuda.set_device(rank)
     
+    # Initialize wandb only on rank 0
     if rank == 0:
         wandb.init(
             project=config['wandb_project'],
             entity=config['wandb_entity'],
             group=config['wandb_group'],
+            config=config,  # Track configuration
         )
     
-    # Set seed for reproducibility
-    torch.manual_seed(config['seed'])
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
     
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
     # ImageNet normalization
     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                      std=[0.229, 0.224, 0.225])
@@ -603,7 +450,8 @@ def train_worker(rank, world_size, config):
         trainset,
         batch_sampler=train_sampler,
         num_workers=config['num_workers'],
-        pin_memory=True
+        pin_memory=True,
+        #persistent_workers=False
     )
 
     
@@ -624,20 +472,24 @@ def train_worker(rank, world_size, config):
     )
 
     dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
-
+    
     if config['loss'] == 'LDA':
         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
     else:
         lda_args = {}
-
-    # Create solver with distributed info
+        
+    # Create solver with optimized parameters
     solver = Solver(
         dataloaders=dataloaders, 
         model_path=config['model_path'],
         n_classes=config['n_classes'],
-        lda_args=lda_args,
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
         local_rank=rank,
-        world_size=world_size
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
     )
     
     # Train
@@ -651,35 +503,39 @@ def train_worker(rank, world_size, config):
 
 
 if __name__ == '__main__':
-    from pathlib import Path
-
-    home = Path('/data')
-    
+    # Configuration with memory optimizations
     config = {
-        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_project': "DELETEME",
         'wandb_entity': "gerardo-pastrana-c3-ai",
         'wandb_group': "gapLoss",
         'seed': 42,
         'n_classes': 1000,
         'train_val_split': 0.1,
-        'batch_size': 4096,
-        'num_workers': 1,  
-        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
-        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
-        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
         'model_path': 'models/deeplda_best.pth',
         'loss': 'LDA',
         'lamb': 0.1,
         'n_eig': 4,
         'margin': None,
-        'epochs': 100,
-        'k_classes': 64, 
-        'n_samples': 64, 
+        'epochs': 20,
+        'k_classes':128 ,
+        'n_samples': 64,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
     }
-
     
     # Number of available GPUs
-    n_gpus = 8
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
     
     # Launch processes
     mp.spawn(
diff --git a/wandb/latest-run b/wandb/latest-run
index 538ff58..409e3b2 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250429_053315-pm5qboq7
\ No newline at end of file
+run-20250430_152714-naxvk16q
\ No newline at end of file
diff --git a/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb b/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb
index 686db59..42fa731 100644
Binary files a/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb and b/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb differ
