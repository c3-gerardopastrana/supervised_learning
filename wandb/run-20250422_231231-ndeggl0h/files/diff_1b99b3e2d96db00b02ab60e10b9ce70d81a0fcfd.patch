diff --git a/lda.py b/lda.py
index cc368cf..0d12420 100644
--- a/lda.py
+++ b/lda.py
@@ -1,7 +1,7 @@
 import torch
 import torch.nn as nn
 from functools import partial
-
+import torch.nn.functional as F
 
 def lda(X, y, n_classes, lamb):
     # flatten X
@@ -27,41 +27,96 @@ def lda(X, y, n_classes, lamb):
 
     # cope for numerical instability
     Sw += torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+    #Sw = (1-lamb)*Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+
+    # compute eigen decomposition
+    temp = torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) #torch.linalg.solve(sigma_b, Sw ) 
+    # # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
+    # evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
+    # print(evals.shape, evecs.shape)
+
+    # # remove complex eigen values and sort
+    # noncomplex_idx = evals[:, 1] == 0
+    # evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
+    # evecs = evecs[:, noncomplex_idx]
+    # evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
+    # evecs = evecs[:, inc_idx]
+    # print(evals.shape, evecs.shape)
+
+    # # flag to indicate if to skip backpropagation
+    # hasComplexEVal = evecs.shape[1] < evecs.shape[0]
 
+    # return hasComplexEVal, Xc_mean, evals, evecs
     # compute eigen decomposition
-    temp = Sw.pinverse().matmul(Sb)
     # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
-    evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
-    print(evals.shape, evecs.shape)
+    # Use the new torch.linalg.eig for general matrices
+    # It returns complex eigenvalues and eigenvectors by default
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
 
-    # remove complex eigen values and sort
-    noncomplex_idx = evals[:, 1] == 0
-    evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
-    evecs = evecs[:, noncomplex_idx]
-    evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
-    evecs = evecs[:, inc_idx]
-    print(evals.shape, evecs.shape)
+    # Process complex eigenvalues returned by torch.linalg.eig
+    # Check for eigenvalues with non-negligible imaginary parts
+    tol = 1e-6 # Tolerance for considering imaginary part zero
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex) # Flag if *any* eigenvalue was complex beyond tolerance
 
-    # flag to indicate if to skip backpropagation
-    hasComplexEVal = evecs.shape[1] < evecs.shape[0]
+    if hasComplexEVal:
+         # Optional: Print a warning if complex eigenvalues are detected
+         print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real eigenvalues.")
 
-    return hasComplexEVal, Xc_mean, evals, evecs
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real 
+    evecs = evecs_complex[:, real_idx].real
+
+    if evals.numel() > 0: # Check if any real eigenvalues are left
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex. Eigenvalue/vector tensors are empty.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        D = temp.shape[0]
+        evecs = torch.tensor([[] for _ in range(D)], dtype=temp.dtype, device=temp.device)
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
 
 
 def lda_loss(evals, n_classes, n_eig=None, margin=None):
     n_components = n_classes - 1
     evals = evals[-n_components:]
     # evecs = evecs[:, -n_components:]
-    print('evals', evals.shape, evals)
+    # print('evals', evals.shape, evals)
     # print('evecs', evecs.shape)
 
     # calculate loss
     if margin is not None:
         threshold = torch.min(evals) + margin
         n_eig = torch.sum(evals < threshold)
-    loss = -torch.mean(evals[:n_eig]) # small eigen values are on left
+   
+    probs = evals / evals.sum()
+    entropy = -torch.sum(probs * torch.log(probs.clamp(min=1e-9)))
+
+    loss = -entropy
+    #loss = torch.mean(evals[:n_eig]) # small eigen values are on left
+    
+    # eigvals_norm = evals / evals.sum()
+    # eps = 1e-10 
+    # eigvals_norm = torch.clamp(eigvals_norm, min=eps)
+    # oss = (eigvals_norm * eigvals_norm.log()).sum()
+    #loss = torch.log(eigvals_norm.max()-eigvals_norm.min())
     return loss
+    
+def sina_loss(sigma_w_inv_b):
+    n = torch.tensor(sigma_w_inv_b.shape[0], dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
 
+    max_frobenius_norm = max_frobenius_norm = torch.trace(sigma_w_inv_b @ sigma_w_inv_b)
+    max_frobenius_norm = torch.sqrt(max_frobenius_norm.abs()) * torch.sqrt(n)
+    trace = torch.trace(sigma_w_inv_b).abs() 
+    loss = torch.log(max_frobenius_norm / trace)
+    #trace = torch.trace(1/2*(sigma_w_inv_b + sigma_w_inv_b.T))
+    #loss = torch.log(max_frobenius_norm) - torch.log(trace)
+    
+    # off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+    # sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+    # loss = sum_squared_off_diag - trace
+    return loss
 
 class LDA(nn.Module):
     def __init__(self, n_classes, lamb):
@@ -73,7 +128,7 @@ class LDA(nn.Module):
 
     def forward(self, X, y):
         # perform LDA
-        hasComplexEVal, Xc_mean, evals, evecs = self.lda_layer(X, y)  # CxD, D, DxD
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)  # CxD, D, DxD
 
         # compute LDA statistics
         self.scalings_ = evecs  # projection matrix, DxD
@@ -81,7 +136,7 @@ class LDA(nn.Module):
         self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t())) # C
 
         # return self.transform(X)
-        return hasComplexEVal, evals
+        return hasComplexEVal, evals, sigma_w_inv_b
 
     def transform(self, X):
         """ transform data """
@@ -104,6 +159,161 @@ class LDA(nn.Module):
 
 
 
+def spherical_lda(X, y, n_classes, lamb):
+    N, D = X.shape
+    labels, counts = torch.unique(y, return_counts=True)
+    assert len(labels) == n_classes  # require all classes to be present
+    
+    # Compute global mean direction and normalize
+    global_mean = torch.mean(X, 0)
+    global_mean = F.normalize(global_mean, p=2, dim=0)
+    
+    # Initialize containers
+    class_means_list = []
+    Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    Sb = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    
+    # Calculate all class means
+    for c in labels:
+        class_idx = int(c)
+        Xc = X[y == c]
+        class_mean = F.normalize(torch.mean(Xc, dim=0), p=2, dim=0)
+        class_means_list.append(class_mean)
+    
+    Xc_mean = torch.stack(class_means_list)
+    
+    # Compute scatter matrices
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        Xc = X[y == c]
+        class_mean = Xc_mean[i]
+
+        # Vectorized cosine similarities
+        cos_similarities = Xc @ class_mean
+        cos_similarities = torch.clamp(cos_similarities, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        # Vectorized difference: samples projected away from mean direction
+        diffs = Xc - cos_similarities.unsqueeze(1) * class_mean.unsqueeze(0)
+
+        # Vectorized scatter matrix
+        class_scatter = diffs.T @ diffs
+
+        Sw = Sw + class_scatter
+    
+    Sw = Sw / N  # Normalize by total number of samples
+    
+    # Compute between-class scatter
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        class_mean = Xc_mean[i]
+        cos_sim = torch.dot(class_mean, global_mean)
+        cos_sim = torch.clamp(cos_sim, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        diff = class_mean.unsqueeze(1) - cos_sim * global_mean.unsqueeze(1)
+        diff_outer = diff @ diff.T
+        Sb = Sb + (Nc / N) * diff_outer
+
+    
+    # import numpy as np
+    # from sklearn.covariance import LedoitWolf
+    
+    # X_np = Sw.cpu().detach().numpy()
+
+    # lw = LedoitWolf()
+    # lw.fit(X_np)
+    # shrinkage = lw.shrinkage_
+    # mu = torch.trace(Sw) / D
+    # shrinkage = torch.nn.Parameter(torch.tensor(0.0, dtype=Sw.dtype, device=Sw.device))
+    # shrinkage = torch.sigmoid(shrinkage)
+    
+    Sw_reg = Sw + torch.eye(D, dtype=X.dtype, device=X.device) * lamb
+    #Sw_reg = (1-shrinkage) * Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * shrinkage * mu
+    
+    
+    # Generalized eigenvalue problem
+    temp = torch.linalg.pinv(Sw_reg, hermitian=True) @ Sb
+    
+    # Eigen decomposition
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    tol = 1e-6
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex)
+    
+    if hasComplexEVal:
+        print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real ones.")
+    
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real
+    evecs = evecs_complex[:, real_idx].real
+    
+    if evals.numel() > 0:
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        evecs = torch.zeros((D, 0), dtype=temp.dtype, device=temp.device)
+    
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
+
+
+class SphericalLDA(nn.Module):
+    def __init__(self, n_classes, lamb=1e-4):
+        super(SphericalLDA, self).__init__()
+        self.n_classes = n_classes
+        self.n_components = n_classes - 1  # Maximum meaningful LDA dimensions
+        self.lamb = lamb
+        self.lda_layer = partial(spherical_lda, n_classes=n_classes, lamb=lamb)
+    
+    def forward(self, X, y):
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+        
+        # Store projection matrix 
+        self.scalings_ = evecs
+        
+        # Project class means and normalize to create prototypes
+        projected_means = Xc_mean.matmul(evecs)
+        
+        # Project back to original space and normalize to ensure they're on the hypersphere
+        self.coef_ = F.normalize(projected_means.matmul(evecs.t()), p=2, dim=1)
+        
+        # Intercept is not meaningful in spherical space when using cosine similarity
+        self.intercept_ = torch.zeros(self.n_classes, dtype=X.dtype, device=X.device)
+        
+        return hasComplexEVal, evals, sigma_w_inv_b
+    
+    def transform(self, X):
+        # Normalize input
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Project data
+        X_new = X.matmul(self.scalings_)
+        
+        # Return only the most discriminative components
+        return X_new[:, :self.n_components]
+    
+    def predict(self, X):
+        # Normalize input embeddings
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Compute cosine similarities with class prototypes
+        similarities = X.matmul(self.coef_.t())
+        
+        # Return class with highest similarity
+        return torch.argmax(similarities, dim=1)
+    
+    def predict_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        
+        # Convert similarities to probabilities using softmax
+        proba = nn.functional.softmax(similarities, dim=1)
+        return proba
+    
+    def predict_log_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        log_proba = nn.functional.log_softmax(similarities, dim=1)
+        return log_proba
+
 if __name__ == '__main__':
     import numpy as np
     np.set_printoptions(precision=4, suppress=True)
diff --git a/train.py b/train.py
index 7520acb..b91626e 100644
--- a/train.py
+++ b/train.py
@@ -11,9 +11,174 @@ from PIL import Image
 import torchvision
 import torchvision.transforms as transforms
 import torch.optim as optim
-
+import wandb
 from functools import partial
-from lda import LDA, lda_loss
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+
+class Lamb(optim.Optimizer):
+    r"""Implements Lamb algorithm.
+
+    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.
+
+    Arguments:
+        params (iterable): iterable of parameters to optimize or dicts defining
+            parameter groups
+        lr (float, optional): learning rate (default: 1e-3)
+        betas (Tuple[float, float], optional): coefficients used for computing
+            running averages of gradient and its square (default: (0.9, 0.999))
+        eps (float, optional): term added to the denominator to improve
+            numerical stability (default: 1e-8)
+        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
+        adam (bool, optional): always use trust ratio = 1, which turns this into
+            Adam. Useful for comparison purposes.
+
+    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:
+        https://arxiv.org/abs/1904.00962
+    """
+
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,
+                 weight_decay=0, adam=False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay)
+        self.adam = adam
+        super(Lamb, self).__init__(params, defaults)
+
+    def step(self, closure=None):
+        """Performs a single optimization step.
+
+        Arguments:
+            closure (callable, optional): A closure that reevaluates the model
+                and returns the loss.
+        """
+        loss = None
+        if closure is not None:
+            loss = closure()
+
+        for group in self.param_groups:
+            for p in group['params']:
+                if p.grad is None:
+                    continue
+                grad = p.grad.data
+                if grad.is_sparse:
+                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')
+
+                state = self.state[p]
+
+                # State initialization
+                if len(state) == 0:
+                    state['step'] = 0
+                    # Exponential moving average of gradient values
+                    state['exp_avg'] = torch.zeros_like(p.data)
+                    # Exponential moving average of squared gradient values
+                    state['exp_avg_sq'] = torch.zeros_like(p.data)
+
+                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
+                beta1, beta2 = group['betas']
+
+                state['step'] += 1
+
+                # Decay the first and second moment running average coefficient
+                # m_t
+                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
+                # v_t
+                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
+
+                # Paper v3 does not use debiasing.
+                # bias_correction1 = 1 - beta1 ** state['step']
+                # bias_correction2 = 1 - beta2 ** state['step']
+                # Apply bias to lr to avoid broadcast.
+                step_size = group['lr'] # * math.sqrt(bias_correction2) / bias_correction1
+
+                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)
+
+                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])
+                if group['weight_decay'] != 0:
+                    adam_step.add_(p.data, alpha=group['weight_decay'])
+
+                adam_norm = adam_step.pow(2).sum().sqrt()
+                if weight_norm == 0 or adam_norm == 0:
+                    trust_ratio = 1
+                else:
+                    trust_ratio = weight_norm / adam_norm
+                state['weight_norm'] = weight_norm
+                state['adam_norm'] = adam_norm
+                state['trust_ratio'] = trust_ratio
+                if self.adam:
+                    trust_ratio = 1
+
+                p.data.add_(adam_step, alpha=-step_size * trust_ratio)
+
+        return loss
+class LARS(optim.Optimizer):
+    def __init__(
+        self,
+        params,
+        lr,
+        weight_decay=0,
+        momentum=0.9,
+        eta=0.001,
+        weight_decay_filter=None,
+        lars_adaptation_filter=None,
+    ):
+        defaults = dict(
+            lr=lr,
+            weight_decay=weight_decay,
+            momentum=momentum,
+            eta=eta,
+            weight_decay_filter=weight_decay_filter,
+            lars_adaptation_filter=lars_adaptation_filter,
+        )
+        super().__init__(params, defaults)
+
+    @torch.no_grad()
+    def step(self):
+        for g in self.param_groups:
+            for p in g["params"]:
+                dp = p.grad
+
+                if dp is None:
+                    continue
+
+                if g["weight_decay_filter"] is None or not g["weight_decay_filter"](p):
+                    dp = dp.add(p, alpha=g["weight_decay"])
+
+                if g["lars_adaptation_filter"] is None or not g[
+                    "lars_adaptation_filter"
+                ](p):
+                    param_norm = torch.norm(p)
+                    update_norm = torch.norm(dp)
+                    one = torch.ones_like(param_norm)
+                    q = torch.where(
+                        param_norm > 0.0,
+                        torch.where(
+                            update_norm > 0, (g["eta"] * param_norm / update_norm), one
+                        ),
+                        one,
+                    )
+                    dp = dp.mul(q)
+
+                param_state = self.state[p]
+                if "mu" not in param_state:
+                    param_state["mu"] = torch.zeros_like(p)
+                mu = param_state["mu"]
+                mu.mul_(g["momentum"]).add_(dp)
+
+                p.add_(mu, alpha=-g["lr"])
+
+
+
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
 
 
 class BasicBlock(nn.Module):
@@ -24,16 +189,16 @@ class BasicBlock(nn.Module):
         self.conv1 = nn.Conv2d(
             in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
         self.bn1 = nn.BatchNorm2d(planes)
-        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
-                               stride=1, padding=1, bias=False)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
         self.bn2 = nn.BatchNorm2d(planes)
 
         self.shortcut = nn.Sequential()
-        if stride != 1 or in_planes != self.expansion*planes:
+        if stride != 1 or in_planes != self.expansion * planes:
             self.shortcut = nn.Sequential(
-                nn.Conv2d(in_planes, self.expansion*planes,
+                nn.Conv2d(in_planes, self.expansion * planes,
                           kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(self.expansion*planes)
+                nn.BatchNorm2d(self.expansion * planes)
             )
 
     def forward(self, x):
@@ -44,86 +209,65 @@ class BasicBlock(nn.Module):
         return out
 
 
-class Bottleneck(nn.Module):
-    expansion = 4
-
-    def __init__(self, in_planes, planes, stride=1):
-        super(Bottleneck, self).__init__()
-        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
-        self.bn1 = nn.BatchNorm2d(planes)
-        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
-                               stride=stride, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm2d(planes)
-        self.conv3 = nn.Conv2d(planes, self.expansion *
-                               planes, kernel_size=1, bias=False)
-        self.bn3 = nn.BatchNorm2d(self.expansion*planes)
-
-        self.shortcut = nn.Sequential()
-        if stride != 1 or in_planes != self.expansion*planes:
-            self.shortcut = nn.Sequential(
-                nn.Conv2d(in_planes, self.expansion*planes,
-                          kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(self.expansion*planes)
-            )
-
-    def forward(self, x):
-        out = F.relu(self.bn1(self.conv1(x)))
-        out = F.relu(self.bn2(self.conv2(out)))
-        out = self.bn3(self.conv3(out))
-        out += self.shortcut(x)
-        out = F.relu(out)
-        return out
-
-
 class ResNet(nn.Module):
-    def __init__(self, block, num_blocks, n_classes, lda_args):
+    def __init__(self, block, num_blocks, num_classes=10, lda_args=None):
         super(ResNet, self).__init__()
         self.lda_args = lda_args
-        if self.lda_args:  # LDA
-            self.in_planes = 32
-            self.out_planes = 16
-        else:  # Usual CNN with CE loss
-            self.in_planes = 32
-            self.out_planes = 16 # 64
-
-        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,
-                               stride=1, padding=1, bias=False)
-        self.bn1 = nn.BatchNorm2d(self.in_planes)
-        self.layer1 = self._make_layer(block, self.out_planes*1, num_blocks[0], stride=1)
-        self.layer2 = self._make_layer(block, self.out_planes*2, num_blocks[1], stride=2)
-        self.layer3 = self._make_layer(block, self.out_planes*4, num_blocks[2], stride=2)
-        self.layer4 = self._make_layer(block, self.out_planes*8, num_blocks[3], stride=2)
+        self.in_planes = 64
+
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
+                               stride=1, padding=1, bias=False)  # CIFAR-style
+        self.bn1 = nn.BatchNorm2d(64)
+
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        self.projector = nn.Linear(512 * block.expansion, 128)
+        
         if self.lda_args:
-            self.lda = LDA(n_classes, lda_args['lamb'])
+            self.lda = SphericalLDA(num_classes, lda_args['lamb'])  # your LDA class
         else:
-            self.linear = nn.Linear(self.out_planes*8*block.expansion, n_classes)
+            self.linear = nn.Linear(512 * block.expansion, num_classes)
 
     def _make_layer(self, block, planes, num_blocks, stride):
-        strides = [stride] + [1]*(num_blocks-1)
+        strides = [stride] + [1] * (num_blocks - 1)
         layers = []
         for stride in strides:
             layers.append(block(self.in_planes, planes, stride))
             self.in_planes = planes * block.expansion
         return nn.Sequential(*layers)
 
-    def forward(self, X, y):
-        out = F.relu(self.bn1(self.conv1(X)))
+    def forward(self, x, y=None, epoch =0):
+        out = F.relu(self.bn1(self.conv1(x)))
         out = self.layer1(out)
         out = self.layer2(out)
         out = self.layer3(out)
         out = self.layer4(out)
-        out = F.avg_pool2d(out, 4)
-        fea = out.view(out.size(0), -1)  # NxC
-        if self.lda_args:
-            hasComplexEVal, out = self.lda(fea, y)  # evals
-            return hasComplexEVal, fea, out
+
+        out = F.avg_pool2d(out, 4)  # CIFAR final feature map is 4x4
+        fea = out.view(out.size(0), -1)  # Nx512
+
+        
+        #fea = self.projector(fea)        # Nx128
+        fea = F.normalize(fea, p=2, dim=1)  # L2 normalization
+
+        
+
+        if self.lda_args and epoch>20:
+            
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
         else:
             out = self.linear(fea)
             return out
 
 
-def ResNet18(n_classes, lda_args):
-    return ResNet(BasicBlock, [2, 2, 2, 2], n_classes, lda_args)
+def ResNet18(num_classes=10, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
 
 
 def ResNet34():
@@ -170,9 +314,22 @@ class Solver:
         if self.use_lda:
             self.criterion = partial(lda_loss, n_classes=n_classes, 
                                     n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
         else:
             self.criterion = nn.CrossEntropyLoss()
-        self.optimizer = optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
+        print(self.criterion)
+        
+        #self.optimizer = optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
+        
+        def exclude_bias_and_norm(p):
+            return p.ndim == 1  # biases and BN params are 1D
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=1e-3, weight_decay=5e-4)
+        # self.optimizer = Lamb(
+        #    self.net.parameters(),
+        #     lr=1e-2,
+        #     weight_decay=5e-4, #Try decreasing it to 1e-6
+        # )
+
         self.model_path = model_path
         self.n_classes = n_classes
 
@@ -182,39 +339,98 @@ class Solver:
         total_loss = 0
         correct = 0
         total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+        
         for batch_idx, (inputs, targets) in enumerate(dataloader):
             inputs = inputs.to(self.device)
             targets = targets.to(self.device)
             self.optimizer.zero_grad()
-            
-            if self.use_lda:
-                hasComplexEVal, feas, outputs = self.net(inputs, targets)
+        
+            if self.use_lda and epoch>20:
+                hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets)
                 if not hasComplexEVal:
-                    loss = self.criterion(outputs)
+
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+        
+                    #Loss
+                    loss = self.criterion(sigma_w_inv_b)
+                    #loss = (eigvals_norm * eigvals_norm.log()).sum()
                     outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train':
+                        wandb.log({
+                            'loss': loss,
+                            
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75})
+                    
                 else:
                     print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
                     continue
             else:
                 outputs = self.net(inputs, targets)
-                loss = self.criterion(outputs, targets)            
-            # print('\noutputs shape:', outputs.shape)
-            # print('loss:', loss)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
             if phase == 'train':
+               
                 loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
                 self.optimizer.step()
+                wandb.log({"total_grad_norm_encoder":grad_norm.item()})
             total_loss += loss.item()
-
+    
             outputs = torch.argmax(outputs.detach(), dim=1)
-            # _, outputs = outputs.max(1)
             total += targets.size(0)
             correct += outputs.eq(targets).sum().item()
+        
         total_loss /= (batch_idx + 1)
-        total_acc = correct/total
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if entropy_count > 0:
+            average_entropy = entropy_sum / entropy_count
+            print(f'Average Entropy: {average_entropy:.4f}')
+        
         print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
                      % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+        wandb.log({
+            "epoch"+phase:epoch,
+             "total"+phase:total_loss,
+             "total_acc_train"+phase: 100.*total_acc
+        }) 
         return total_loss, total_acc
 
+
     def train(self, epochs):
         best_loss = float('inf')
         for epoch in range(epochs):
@@ -279,6 +495,12 @@ def parse_dir(img_dir, classes, randnum=-1):
 
 
 if __name__ == '__main__':
+    wandb.init(
+    project="DeepLDA",
+    entity="gerardo-pastrana-c3-ai",
+    group="gapLoss",
+    )
+    
     transform_train = transforms.Compose([
         transforms.RandomCrop(32, padding=2),
         transforms.RandomHorizontalFlip(),
@@ -294,19 +516,20 @@ if __name__ == '__main__':
     seed = 42
     n_classes = 10
     train_val_split = 0.2
-    batch_size = 5000
+    batch_size = 7500
     num_workers = 4
-    gpu = -1
+    gpu = 2
 
     train_dir = '../data/cifar10/imgs/train'
     test_dir = '../data/cifar10/imgs/test'
     model_path = '../data/cifar10/exp1015/deeplda_best.pth'
 
     loss = 'LDA' # CE or LDA
-    lamb = 0.0001
+    lamb = 0.1#0.0001
     n_eig = 4
     margin = None
     lda_args = {'lamb':lamb, 'n_eig':n_eig, 'margin':margin} if loss == 'LDA' else {}
+    
 
     class_map = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 
                  'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}
@@ -330,5 +553,6 @@ if __name__ == '__main__':
 
     dataloaders = {'train':trainloader, 'val':valloader, 'test':testloader}
     solver = Solver(dataloaders, model_path, n_classes, lda_args, gpu)
-    solver.train(20)
+    
+    solver.train(500)
     solver.test()
