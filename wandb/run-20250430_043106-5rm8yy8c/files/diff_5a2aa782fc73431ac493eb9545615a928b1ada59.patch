diff --git a/lda.py b/lda.py
index 5b8bae9..e8db7c7 100644
--- a/lda.py
+++ b/lda.py
@@ -162,24 +162,27 @@ class LDA(nn.Module):
         self.n_components = n_classes - 1
         self.lamb = lamb
         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
-        self.running_stats = None  # Stores cumulative LDA stats
+       
 
-    def forward(self, X, y):
-        X = X.view(X.shape[0], -1).detach()
-        y = y.detach()
-
-        # Initialize or update running stats
-        if self.running_stats is None:
-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
-        self.running_stats.update(X, y)
+        self.coef_ = torch.zeros((n_classes, 512), device="cuda")
+        self.intercept_ = torch.zeros(n_classes, device="cuda")
 
+    def forward(self, X, y):
         # Perform batch-wise LDA (temporary, not global yet)
         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
 
-        # Save batch-wise scalings (not necessarily global yet)
-        self.scalings_ = evecs
-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+        # normalize Xc_mean rows that are not zero
+        norms = Xc_mean.norm(dim=1, keepdim=True)  # Cx1
+        nonzero_mask = norms.squeeze() > 0  # C
+        normalized_mean = Xc_mean.clone()
+        normalized_mean[nonzero_mask] = Xc_mean[nonzero_mask] / norms[nonzero_mask]
+
+        # simplified since evecs = I
+        self.scalings_ = torch.eye(X.size(1), device=X.device)  # DxD
+        self.coef_[nonzero_mask] = normalized_mean[nonzero_mask]
+
+        # intercept is just -0.5 * ||mean||^2 for nonzero rows
+        self.intercept_[nonzero_mask] = -0.5
 
         return hasComplexEVal, evals, sigma_w_inv_b
 
@@ -246,8 +249,8 @@ class RunningLDAStats:
 
     @torch.no_grad()
     def update(self, X, y):
-        X = X.view(X.shape[0], -1).cpu()
-        y = y.cpu()
+        X = X.view(X.shape[0], -1).detach().to('cpu')
+        y = y.detach().to('cpu')
 
         for cls in range(self.n_classes):
             mask = (y == cls)
diff --git a/train.py b/train.py
index 146c137..001e976 100644
--- a/train.py
+++ b/train.py
@@ -22,151 +22,10 @@ from torch.utils.data.distributed import DistributedSampler
 from torch.utils.data import DataLoader, random_split, Sampler, Subset
 from torch.utils.checkpoint import checkpoint
 
+from models import ResNet, BasicBlock
 
-class BasicBlock(nn.Module):
-    expansion = 1
-    def __init__(self, in_planes, planes, stride=1):
-        super(BasicBlock, self).__init__()
-        self.conv1 = nn.Conv2d(
-            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
-        self.bn1 = nn.BatchNorm2d(planes)
-        self.conv2 = nn.Conv2d(
-            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
-        self.bn2 = nn.BatchNorm2d(planes)
-        self.shortcut = nn.Sequential()
-        if stride != 1 or in_planes != self.expansion * planes:
-            self.shortcut = nn.Sequential(
-                nn.Conv2d(in_planes, self.expansion * planes,
-                          kernel_size=1, stride=stride, bias=False),
-                nn.BatchNorm2d(self.expansion * planes)
-            )
-    
-    def _forward_impl(self, x):
-        out = F.relu(self.bn1(self.conv1(x)))
-        out = self.bn2(self.conv2(out))
-        out += self.shortcut(x)
-        out = F.relu(out)
-        return out
-        
-    def forward(self, x):
-        return checkpoint(self._forward_impl, x)
-
-class ResNet(nn.Module):
-    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
-        super(ResNet, self).__init__()
-        self.lda_args = lda_args
-        self.in_planes = 64
-        self.use_checkpoint = use_checkpoint
-        
-        # ImageNet-style initial conv layer
-        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
-                               stride=2, padding=3, bias=False)
-        self.bn1 = nn.BatchNorm2d(64)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-        
-        # Residual layers
-        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
-        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
-        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
-        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
-        
-        # Global average pooling and output
-        self.avgpool = nn.AdaptiveAvgPool2d(1)
-        self.linear = nn.Linear(512 * block.expansion, num_classes)
-        
-        # LDA branch (if enabled)
-        if self.lda_args:
-            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
-    
-    def _make_layer(self, block, planes, num_blocks, stride):
-        strides = [stride] + [1] * (num_blocks - 1)
-        layers = []
-        for stride in strides:
-            layers.append(block(self.in_planes, planes, stride))
-            self.in_planes = planes * block.expansion
-        return nn.Sequential(*layers)
-    
-    def _forward_features(self, x):
-        out = self.relu(self.bn1(self.conv1(x)))
-        out = self.maxpool(out)
-        
-        if self.use_checkpoint:
-            out = checkpoint(lambda x: self.layer1(x), out)
-            out = checkpoint(lambda x: self.layer2(x), out)
-            out = checkpoint(lambda x: self.layer3(x), out)
-            out = checkpoint(lambda x: self.layer4(x), out)
-        else:
-            out = self.layer1(out)
-            out = self.layer2(out)
-            out = self.layer3(out)
-            out = self.layer4(out)
-            
-        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
-        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
-        return fea
-    
-    def forward(self, x, y=None, epoch=0):
-        fea = self._forward_features(x)
-        
-        if self.lda_args:
-            fea = F.normalize(fea, p=2, dim=1)
-            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
-            return hasComplexEVal, fea, out, sigma_w_inv_b
-        else:
-            out = self.linear(fea)
-            return out
-
-
-def ResNet18(num_classes=1000, lda_args=None):
-    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
-
-
-def ResNet34():
-    return ResNet(BasicBlock, [3, 4, 6, 3])
-
-
-def ResNet50():
-    return ResNet(Bottleneck, [3, 4, 6, 3])
-
-
-def ResNet101():
-    return ResNet(Bottleneck, [3, 4, 23, 3])
-
-
-def ResNet152():
-    return ResNet(Bottleneck, [3, 8, 36, 3])
-
-
-class CIFAR10:
-    def __init__(self, img_names, class_map, transform):
-        self.img_names = img_names
-        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
-        self.transform = transform
-    def __len__(self):
-        return len(self.img_names)
-    def __getitem__(self, idx):
-        img = Image.open(self.img_names[idx])
-        img = self.transform(img)
-        clazz = self.classes[idx]
-        return img, clazz
-
-
-
-def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
-    """
-    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
-
-    Args:
-        batch_size (int): new batch size
-        base_batch_size (int): original batch size corresponding to base_lr
-        base_lr (float): base learning rate at base_batch_size
-
-    Returns:
-        float: scaled learning rate
-    """
-    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
-    return base_lr * scale.item()
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
 
 
 class Solver:
@@ -516,28 +375,47 @@ def train_worker(rank, world_size, config):
         def __iter__(self):
             g = torch.Generator()
             g.manual_seed(self.seed + self.epoch + self.rank)
-    
-            all_batches = []
-    
-            while len(all_batches) < self.batches_per_epoch:
-                # Pick k_classes randomly
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
                 selected_classes = torch.tensor(self.available_classes)
                 selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
-    
+            
                 batch = []
                 for cls in selected_classes.tolist():
                     indices = self.class_to_indices[cls]
                     indices_tensor = torch.tensor(indices)
                     chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
                     batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+    
+            # all_batches = []
+    
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
     
-                all_batches.append(batch)
+            #     all_batches.append(batch)
     
-            # Shard batches across GPUs
-            local_batches = all_batches[self.rank::self.world_size]
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
     
-            for batch in local_batches:
-                yield batch
+            # for batch in local_batches:
+            #     yield batch
     
         def __len__(self):
             return self.batches_per_epoch // self.world_size
@@ -673,13 +551,13 @@ if __name__ == '__main__':
         'n_eig': 4,
         'margin': None,
         'epochs': 100,
-        'k_classes': 64, 
+        'k_classes': 128, 
         'n_samples': 64, 
     }
 
     
     # Number of available GPUs
-    n_gpus = 8
+    n_gpus = 4
     
     # Launch processes
     mp.spawn(
diff --git a/wandb/latest-run b/wandb/latest-run
index 538ff58..9546bba 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250429_053315-pm5qboq7
\ No newline at end of file
+run-20250430_043106-5rm8yy8c
\ No newline at end of file
diff --git a/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb b/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb
index 686db59..42fa731 100644
Binary files a/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb and b/wandb/run-20250428_213549-4si4f9qg/run-4si4f9qg.wandb differ
