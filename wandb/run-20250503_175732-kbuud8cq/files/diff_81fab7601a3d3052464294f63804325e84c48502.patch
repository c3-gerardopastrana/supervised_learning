diff --git a/eval.py b/eval.py
index 16237a4..cd5f21d 100644
--- a/eval.py
+++ b/eval.py
@@ -57,8 +57,8 @@ def run_linear_probe_on_embeddings(train_loader, val_loader, model, device=None,
         train_ds = TensorDataset(X_train, y_train)
         val_ds = TensorDataset(X_val, y_val)
 
-        train_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)
-        val_loader = DataLoader(val_ds, batch_size=4096)
+        train_loader = DataLoader(train_ds, batch_size=8192, shuffle=True)
+        val_loader = DataLoader(val_ds, batch_size=8192)
 
         # Define linear classifier
         classifier = LinearClassifier(X_train.shape[1], int(y_train.max()) + 1).to(device)
diff --git a/lda.py b/lda.py
index 89aac69..61bc0eb 100644
--- a/lda.py
+++ b/lda.py
@@ -2,6 +2,36 @@ import torch
 import torch.nn as nn
 from functools import partial
 import torch.nn.functional as F
+from sklearn.covariance import ledoit_wolf_shrinkage
+
+
+
+def oas_shrinkage(S: torch.Tensor, n: int) -> torch.Tensor:
+    """
+    Computes the Oracle Approximating Shrinkage (OAS) estimator.
+    
+    Args:
+        S (torch.Tensor): Empirical covariance matrix of shape (p, p).
+        n (int): Number of samples used to compute S.
+    
+    Returns:
+        torch.Tensor: Shrinkage covariance matrix of shape (p, p).
+    """
+    p = S.shape[0]
+    trace_S = torch.trace(S)
+    trace_S2 = torch.sum(S * S)
+
+    num = (1 - 2/n) * trace_S2 + trace_S**2
+    denom = (n + 1 - 2/n) * (trace_S2 - (trace_S**2) / p)
+
+    alpha = num / denom
+    alpha = torch.clamp(alpha, max=1.0)  # clip to 1
+    print(alpha)
+
+    F = (trace_S / p) * torch.eye(p, dtype=S.dtype, device=S.device)
+    S_oas = (1 - alpha) * S + alpha * F
+
+    return S_oas
 
 def lda(X, y, n_classes, lamb):
     X = X.view(X.shape[0], -1)
@@ -39,11 +69,24 @@ def lda(X, y, n_classes, lamb):
     Sb = St - Sw
     
     # Add regularization to Sw
-    Sw = Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
-    
+    #lamb = torch.trace(Sw) / D
+    Sw = oas_shrinkage(Sw, N)
     
+    #Sw = Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+    #Sw_np = Sw.cpu().detach().numpy()
+    #lw = ledoit_wolf().fit(Sw_np)  # add a sample axis if needed
+    # shrinkage = lamb#ledoit_wolf_shrinkage(Sw_np)
+    # mu = torch.trace(Sw) / Sw.shape[0]
 
-    temp = torch.linalg.solve(Sw, Sb) #torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) 
+    # Apply shrinkage — differentiable
+    #Sw = (1 - shrinkage) * Sw + shrinkage * mu * torch.eye(Sw.shape[0], device=Sw.device, dtype=Sw.dtype)
+    #Sw =  Sw +  mu * torch.eye(Sw.shape[0], device=Sw.device, dtype=Sw.dtype)
+    #Sw =  mu * torch.eye(Sw.shape[0], device=Sw.device, dtype=Sw.dtype)
+    # diag_values = torch.rand(D, device="cuda") * 0.0001  # Uniform[0, lam)
+    # Sw = Sw + torch.diag(diag_values)
+    
+    
+    temp =  torch.linalg.solve(Sw, Sb)# #torch.linalg.lstsq(Sw, Sb).solution #torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) 
     # # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
     # evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
     # print(evals.shape, evecs.shape)
@@ -64,7 +107,30 @@ def lda(X, y, n_classes, lamb):
     # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
     # Use the new torch.linalg.eig for general matrices
     # It returns complex eigenvalues and eigenvectors by default
-    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    try:
+        evals_complex, evecs_complex = torch.linalg.eig(temp)
+    except:
+        #print(Sw)
+        print(torch.trace(Sw))
+        print(torch.linalg.eigvalsh(Sw))
+        eigvals = torch.linalg.eigvalsh(Sw)
+        
+        # Sort to ensure consistent behavior
+        eigvals_sorted = eigvals.sort().values
+        
+        # Compute pairwise differences (broadcasted)
+        diffs = eigvals_sorted.unsqueeze(0) - eigvals_sorted.unsqueeze(1)
+        
+        # Avoid division by zero: set diagonal to inf (or mask it)
+        diffs.fill_diagonal_(float('inf'))
+        
+        # Compute reciprocal and take the max of absolute values
+        inv_diffs = (1.0 / diffs).abs()
+        max_val = inv_diffs.max()
+        
+        print("Eigenvalues:", eigvals_sorted)
+        print("Max 1/|λ_i - λ_j|:", max_val)
+
 
     # Process complex eigenvalues returned by torch.linalg.eig
     # Check for eigenvalues with non-negligible imaginary parts
@@ -135,8 +201,11 @@ def sina_loss(sigma_w_inv_b):
     # # loss = torch.norm(diff, p='fro')**2
 
     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
-    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
-    penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+    # lambda_target = torch.tensor(2**10, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    # penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+    lambda_target = torch.tensor(2**8, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    diff = lambda_target - trace  # positive if trace < lambda_target
+    penalty = F.relu(diff).pow(2) / lambda_target.pow(2)  # scale-free, no penalty if trace >= lambda_target
 
     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
     
diff --git a/train.py b/train.py
index ee8d316..afb91b0 100644
--- a/train.py
+++ b/train.py
@@ -81,7 +81,7 @@ class Solver:
     
         if hasComplexEVal:
             print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
-            return None, None, None
+            #return None
     
         metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
         loss = self.criterion(sigma_w_inv_b)
@@ -208,20 +208,21 @@ class Solver:
                 val_loss, val_acc = self.iterate(epoch, 'val')
             
             # All processes run this to contribute their part of the embeddings
-            import time
-            start_time = time.time()
-            lda_accuracy = run_linear_probe_on_embeddings(
-                self.dataloaders['complete_train'],
-                self.dataloaders['val'],
-                self.get_net(),
-                use_amp=self.use_amp
-            )
-            
-            # Only rank 0 gets accuracy; others get None
-            if self.local_rank == 0 and lda_accuracy is not None:
-                wandb.log({'lda_accuracy': lda_accuracy})
-                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
-                print(f"Total time: {elapsed_time:.2f} minutes")
+            if epoch % 7 == 0:
+                import time
+                start_time = time.time()
+                lda_accuracy = run_linear_probe_on_embeddings(
+                    self.dataloaders['complete_train'],
+                    self.dataloaders['val'],
+                    self.get_net(),
+                    use_amp=self.use_amp
+                )
+                
+                # Only rank 0 gets accuracy; others get None
+                if self.local_rank == 0 and lda_accuracy is not None:
+                    wandb.log({'lda_accuracy': lda_accuracy})
+                    elapsed_time = (time.time() - start_time) / 60  # convert to minutes
+                    print(f"Total time: {elapsed_time:.2f} minutes")
 
     
             # Save best model
@@ -388,14 +389,40 @@ def train_worker(rank, world_size, config):
     ])
     
     
-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
-    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
-    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    # trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    # valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    # testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+
+
+    # Load the full datasets
+    trainset_full = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset_full = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset_full = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    
+    # Select 10 class indices (e.g., 10 random or specific ones)
+    selected_classes = list(range(10))  # or any 10 specific indices you want
+    
+    # Map class name to index
+    class_to_idx = trainset_full.class_to_idx
+    idx_to_class = {v: k for k, v in class_to_idx.items()}
+    
+    # Create a filter function
+    def filter_by_class(dataset, allowed_classes):
+        indices = [i for i, (_, label) in enumerate(dataset.samples) if label in allowed_classes]
+        return Subset(dataset, indices)
+    
+    # Create filtered datasets
+    trainset = filter_by_class(trainset_full, selected_classes)
+    valset = filter_by_class(valset_full, selected_classes)
+    testset = filter_by_class(testset_full, selected_classes)
+
+    
 
     # Create subset
     transit_size = int(0.1 * len(trainset))
     indices = random.sample(range(len(trainset)), transit_size)
-    transit_subset = Subset(trainset, indices)
+    transit_subset = trainset#Subset(trainset, indices)
 
     # Create distributed samplers
     train_sampler = ClassBalancedBatchSampler(
@@ -494,12 +521,12 @@ if __name__ == '__main__':
         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
         'model_path': 'models/deeplda_best.pth',
         'loss': 'LDA',
-        'lamb': 0.1,
+        'lamb': 0.01,
         'n_eig': 4,
         'margin': None,
-        'epochs': 25,
-        'k_classes': 64,
-        'n_samples': 128,
+        'epochs': 50,
+        'k_classes': 10,
+        'n_samples': 64,
         # Memory optimization parameters
         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
         'use_amp': True,                   # Use automatic mixed precision
diff --git a/wandb/latest-run b/wandb/latest-run
index 710b4c2..2256e5c 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250501_172906-uyn62eav
\ No newline at end of file
+run-20250503_175732-kbuud8cq
\ No newline at end of file
