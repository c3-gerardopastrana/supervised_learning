Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/train.py b/train.py
index 4307a4b..1f89899 100644
--- a/train.py
+++ b/train.py
@@ -317,6 +317,7 @@ class Solver:
         best_loss = float('inf')
         for epoch in range(epochs):
             # Set epoch for distributed samplers
+            
             if self.world_size > 1:
                 for phase in self.dataloaders:
                     if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
@@ -474,56 +475,77 @@ def train_worker(rank, world_size, config):
     
     class ClassBalancedBatchSampler(Sampler):
         def __init__(self, dataset, k_classes, n_samples, world_size=1, rank=0, seed=42):
+            """
+            Simple class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes to sample per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes in distributed training
+                rank: Rank of current process
+                seed: Random seed
+            """
+            super().__init__(dataset)
             self.dataset = dataset
             self.k_classes = k_classes
             self.n_samples = n_samples
             self.world_size = world_size
             self.rank = rank
             self.seed = seed
-    
-            # Get targets (handle Subset)
+            self.epoch = 0
+            
+            # Get class-to-index mapping
             if isinstance(dataset, torch.utils.data.Subset):
                 targets = [dataset.dataset.targets[i] for i in dataset.indices]
-                indices = dataset.indices
             else:
                 targets = dataset.targets
-                indices = list(range(len(targets)))
-    
+                
             # Build class to index mapping
-            self.class_to_indices = defaultdict(list)
-            for idx in indices:
-                label = targets[idx]
-                self.class_to_indices[label].append(idx)
-    
-            self.classes = sorted(self.class_to_indices.keys())
-            self.epoch = 0
+            self.class_to_indices = {}
+            for i, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(i)
+                
+            # Get list of classes that have enough samples
+            self.classes = []
+            for cls, indices in self.class_to_indices.items():
+                if len(indices) >= n_samples:
+                    self.classes.append(cls)
+                    
+            assert len(self.classes) >= k_classes, f"Only {len(self.classes)} classes have {n_samples}+ samples"
     
         def __iter__(self):
-            random.seed(self.seed + self.epoch + self.rank)
-    
-            selected_classes = random.sample(self.classes, self.k_classes)
-    
+            # Set random seed for this epoch
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+            
+            # Sample k_classes from available classes
+            classes = random.sample(self.classes, self.k_classes)
+            
+            # For each class, sample n_samples instances
             batch = []
-            for cls in selected_classes:
-                samples = random.choices(self.class_to_indices[cls], k=self.n_samples)
-                batch.extend(samples)
-    
-            # Split the batch for different GPUs
-            # Make sure total batch is divisible by world_size
-            assert len(batch) % self.world_size == 0, "Batch size must be divisible by world_size"
-    
-            local_batch = batch[self.rank::self.world_size]
-            yield local_batch
+            for cls in classes:
+                indices = self.class_to_indices[cls]
+                batch.extend(random.sample(indices, self.n_samples))
+                
+            # Shuffle the samples
+            batch = torch.tensor(batch)[torch.randperm(len(batch), generator=g)].tolist()
+            
+            # Each process takes its portion
+            local_indices = batch[self.rank::self.world_size]
+            
+            yield local_indices
     
         def __len__(self):
-            # 1 batch per epoch in this simple version
+            # One batch per epoch in this simplified version
             return 1
     
         def set_epoch(self, epoch):
             self.epoch = epoch
 
 
-
     # Setup process group
     setup(rank, world_size)
     
@@ -576,18 +598,19 @@ def train_worker(rank, world_size, config):
         n_samples=config['n_samples'],
         world_size=world_size,
         rank=rank,
-        seed=config['seed'],
+        seed=config['seed']
     )
 
+
     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
 
     # Create dataloaders
-    trainloader = trainloader = torch.utils.data.DataLoader(
+    trainloader = torch.utils.data.DataLoader(
         trainset,
         batch_sampler=train_sampler,
         num_workers=config['num_workers'],
-        pin_memory=True,
+        pin_memory=True
     )
 
     
diff --git a/wandb/latest-run b/wandb/latest-run
index a67637e..94661d4 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250428_195803-u71mgr7j
\ No newline at end of file
+run-20250428_201939-nqrxwywu
\ No newline at end of file
