Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/train.py b/train.py
index 4307a4b..22a0753 100644
--- a/train.py
+++ b/train.py
@@ -220,7 +220,7 @@ class Solver:
                     hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
                 else:
                     hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
-                print(feas.size())
+                print("bins",torch.bincount(targets))
                 if not hasComplexEVal:
                     #stats
                     eigvals_norm = outputs / outputs.sum()
@@ -317,6 +317,7 @@ class Solver:
         best_loss = float('inf')
         for epoch in range(epochs):
             # Set epoch for distributed samplers
+            
             if self.world_size > 1:
                 for phase in self.dataloaders:
                     if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
@@ -472,55 +473,87 @@ def train_worker(rank, world_size, config):
     import random
     from collections import defaultdict
     
+    from torch.utils.data import Sampler
+    import torch
+
     class ClassBalancedBatchSampler(Sampler):
-        def __init__(self, dataset, k_classes, n_samples, world_size=1, rank=0, seed=42):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
             self.dataset = dataset
             self.k_classes = k_classes
             self.n_samples = n_samples
             self.world_size = world_size
             self.rank = rank
             self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
     
-            # Get targets (handle Subset)
+            # Build mapping from class to list of indices
             if isinstance(dataset, torch.utils.data.Subset):
                 targets = [dataset.dataset.targets[i] for i in dataset.indices]
-                indices = dataset.indices
             else:
                 targets = dataset.targets
-                indices = list(range(len(targets)))
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
     
-            # Build class to index mapping
-            self.class_to_indices = defaultdict(list)
-            for idx in indices:
-                label = targets[idx]
-                self.class_to_indices[label].append(idx)
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
     
-            self.classes = sorted(self.class_to_indices.keys())
-            self.epoch = 0
+        def set_epoch(self, epoch):
+            self.epoch = epoch
     
         def __iter__(self):
-            random.seed(self.seed + self.epoch + self.rank)
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
     
-            selected_classes = random.sample(self.classes, self.k_classes)
+            all_batches = []
     
-            batch = []
-            for cls in selected_classes:
-                samples = random.choices(self.class_to_indices[cls], k=self.n_samples)
-                batch.extend(samples)
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
     
-            # Split the batch for different GPUs
-            # Make sure total batch is divisible by world_size
-            assert len(batch) % self.world_size == 0, "Batch size must be divisible by world_size"
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
     
-            local_batch = batch[self.rank::self.world_size]
-            yield local_batch
+                all_batches.append(batch)
     
-        def __len__(self):
-            # 1 batch per epoch in this simple version
-            return 1
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
     
-        def set_epoch(self, epoch):
-            self.epoch = epoch
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
 
 
 
@@ -576,18 +609,19 @@ def train_worker(rank, world_size, config):
         n_samples=config['n_samples'],
         world_size=world_size,
         rank=rank,
-        seed=config['seed'],
+        seed=config['seed']
     )
+    print("tot", train_sampler.batches_per_epoch * 100)
 
     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
 
     # Create dataloaders
-    trainloader = trainloader = torch.utils.data.DataLoader(
+    trainloader = torch.utils.data.DataLoader(
         trainset,
         batch_sampler=train_sampler,
         num_workers=config['num_workers'],
-        pin_memory=True,
+        pin_memory=True
     )
 
     
diff --git a/wandb/latest-run b/wandb/latest-run
index a67637e..4ae3ba9 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250428_195803-u71mgr7j
\ No newline at end of file
+run-20250428_205416-11bz94wj
\ No newline at end of file
