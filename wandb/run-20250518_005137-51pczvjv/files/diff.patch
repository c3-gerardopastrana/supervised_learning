diff --git a/lda.py b/lda.py
index 733c9a1..7a08f3f 100644
--- a/lda.py
+++ b/lda.py
@@ -54,9 +54,10 @@ def lda(X, y, n_classes, lamb):
     # shrinkage = 0.9
     # Sw = (1-shrinkage) * Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * shrinkage * mu
 
-    
+    #lambda_ = (1.0 / D) * (1 - torch.trace(Sw))
     Sw_reg = Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
     temp = torch.linalg.solve(Sw_reg, Sb) #torch.linalg.pinv(Sw, hermitian=True).matmul(Sb)
+    temp = (temp + temp.T) / 2
     
     return Xc_mean, temp, Sw, Sb, St
 
@@ -90,17 +91,30 @@ def sina_loss(sigma_w_inv_b, sigma_w, sigma_b, xc_mean, sigma_t):
     mu = xc_mean.mean(dim=0)       # (D,)
     mean_term = torch.sum(mu ** 2)
     # loss = (torch.log(torch.trace(sigma_t)) - torch.log(torch.trace(sigma_b))) + mean_term
-    # n = torch.tensor(512, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    n = torch.tensor(512, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+
+    max_frobenius_norm = torch.trace(sigma_w_inv_b @ sigma_w_inv_b)
+    max_frobenius_norm = torch.sqrt(max_frobenius_norm.abs()) 
+    trace = torch.trace(sigma_w_inv_b).abs()
+    lambda_target = torch.tensor(2**6, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    penalty = (trace - lambda_target).pow(2) / lambda_target.pow(2)
+    # penalty = 0.01 * (torch.log(torch.trace(sigma_w)) - torch.log(torch.trace(sigma_b)))
+    loss = torch.log(max_frobenius_norm) -  torch.log(trace) + penalty
 
-    # max_frobenius_norm = torch.trace(sigma_w_inv_b @ sigma_w_inv_b)
-    # max_frobenius_norm = torch.sqrt(max_frobenius_norm.abs()) 
-    # trace = torch.trace(sigma_w_inv_b).abs()
-    # lambda_target = torch.tensor(2**8, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
-    # penalty = (trace - lambda_target).pow(2) / lambda_target
-    # # penalty = 0.01 * (torch.log(torch.trace(sigma_w)) - torch.log(torch.trace(sigma_b)))
-    # loss = torch.log(max_frobenius_norm) -  torch.log(trace) + penalty
-    loss = torch.log(torch.norm(sigma_t, p='fro')) - torch.log(torch.trace(sigma_t)) + mean_term
+    # trace_w = torch.trace(sigma_w)
+    # frob_norm_sq_w = torch.sum(sigma_w ** 2)
+    # d = sigma_w.shape[0]
+    
     
+    # w_sphericity = frob_norm_sq_w - (trace_w ** 2) / d
+
+    # b_sphericity = torch.log(torch.norm(sigma_b, p='fro')) - torch.log(torch.trace(sigma_b))
+    # loss = triangle_loss(xc_mean, sigma_b, sigma_w, epsilon = 0.15)
+    #mean_term + (1-torch.trace(sigma_b + sigma_w)) + 10 * ((sigma_w.sum() - torch.diagonal(sigma_w).sum()))
+    #loss = wasserstein_loss(xc_mean, sigma_t) # + w_sphericity
+    # + torch.norm(sigma_w, p='fro') ** 2 / n
+    #triangle_loss(Xc_mean, Sb, Sw, epsilon = 0.15)
+    #loss = mean_term - torch.trace(sigma_w + sigma_b) + w_sphericity
 
     
     
@@ -306,7 +320,62 @@ def spherical_lda(X, y, n_classes, lamb):
         evecs = torch.zeros((D, 0), dtype=temp.dtype, device=temp.device)
     
     return hasComplexEVal, Xc_mean, evals, evecs, temp
+    
+def triangle_loss(Xc_mean, Sb, Sw, epsilon = 0.15):
+    """
+    Wasserstein proxy loss:
+    Penalizes deviation of class means from 0 and total scatter matrix from (1/n) * I.
+
+    Args:
+        Xc_mean: (n_classes, D) tensor of class means
+        St: (D, D) total scatter matrix
+        n_classes: int, number of classes (used to scale identity)
+
+    Returns:
+        Scalar proxy Wasserstein^2 loss
+    """
+    D = Sw.shape[0]
+    device = Sw.device
+
+    # 1. Mean penalty: encourage mean of class means to be near 0
+    mu = Xc_mean.mean(dim=0)  # (D,)
+    mean_term = torch.sum(mu ** 2)
+
+    # 2. Frobenius norm penalty: encourage St ≈ (1/n) * I
+    target = (1.0 / D) * torch.eye(D, device=device)
+    frob_term_b = torch.norm(Sb - epsilon * target, p='fro') ** 0.1
+    frob_term_w = torch.norm(Sw - (1-epsilon) * target, p='fro') **0.1
+    
+
+    return mean_term + frob_term_b + frob_term_w
+    
+def wasserstein_proxy_loss(Xc_mean, St):
+    """
+    Wasserstein proxy loss:
+    Penalizes deviation of class means from 0 and total scatter matrix from (1/n) * I.
+
+    Args:
+        Xc_mean: (n_classes, D) tensor of class means
+        St: (D, D) total scatter matrix
+        n_classes: int, number of classes (used to scale identity)
+
+    Returns:
+        Scalar proxy Wasserstein^2 loss
+    """
+    D = St.shape[0]
+    device = St.device
+
+    # 1. Mean penalty: encourage mean of class means to be near 0
+    mu = Xc_mean.mean(dim=0)  # (D,)
+    mean_term = torch.sum(mu ** 2)
+
+    # 2. Frobenius norm penalty: encourage St ≈ (1/n) * I
+    target = (1.0 / D) * torch.eye(D, device=device)
+    frob_term = torch.norm(St - target, p='fro') ** 2
 
+    return mean_term
+    #return mean_term + frob_term
+    
 def wasserstein_loss(Xc_mean, St):
     """
     Computes the squared 2-Wasserstein distance between 
@@ -319,25 +388,60 @@ def wasserstein_loss(Xc_mean, St):
     Returns:
         Scalar Wasserstein^2 loss
     """
-    D = St.shape[0]  # dimension
+    D = St.shape[0]
     device = St.device
 
     # 1. Mean penalty
-    mu = Xc_mean.mean(dim=0)       # (D,)
+    mu = Xc_mean.mean(dim=0)
     mean_term = torch.sum(mu ** 2)
 
     # 2. Trace of covariance
     trace_term = torch.trace(St)
 
-    # 3. Trace of sqrt of covariance
-    eigvals = torch.linalg.eigvalsh(St.to(torch.float32))              # (D,)
-    sqrt_trace = torch.sum(torch.sqrt(torch.clamp(eigvals, min=1e-6)))  # stability
+    # 3. Approximate trace of sqrt of covariance using partial eigendecomposition
+    
+    
+    with torch.cuda.amp.autocast(enabled=False):
+        k = 100 #D // 3  # <= D // 3 required by torch.lobpcg
+        St_fp32 = St.to(dtype=torch.float32)
+        eps = 1e-4
+        St_fp32 = St.to(dtype=torch.float32) + eps * torch.eye(St.shape[0], device=St.device)
+        init = torch.randn(St_fp32.shape[0], k, device=St.device, dtype=torch.float32)
+        eigvals, _ = torch.lobpcg(St_fp32, k=k, X=init, niter=100, largest=True, method="ortho")
+        sqrt_trace = torch.sum(torch.sqrt(torch.clamp(eigvals, min=1e-6)))
 
     # Wasserstein^2
     wasserstein_sq = mean_term + trace_term - (2 / D**0.5) * sqrt_trace
 
     return wasserstein_sq
 
+    
+def kl_divergence_loss(Xc_mean, St):
+    """
+    Computes the KL divergence (up to constants) between
+    N(mu, St) and N(0, 1/D * I), where D = dim.
+
+    Args:
+        Xc_mean: (n_classes, D) tensor of class means
+        St: (D, D) total scatter (covariance) matrix
+    Returns:
+        Scalar KL loss (up to constants)
+    """
+    D = St.shape[0]
+    n = D  # target is N(0, 1/D * I)
+
+    mu = Xc_mean.mean(dim=0)  # (D,)
+    mean_term = n * torch.sum(mu ** 2)
+
+    trace_term = n * torch.trace(St)
+
+    # Cholesky-based log-determinant
+    chol = torch.linalg.cholesky(St)
+    log_det_term = -2 * torch.log(chol.diagonal()).sum()
+
+    kl_loss = 0.5 * (trace_term + mean_term + log_det_term)
+    return kl_loss
+
 class SphericalLDA(nn.Module):
     def __init__(self, n_classes, lamb=1e-4):
         super(SphericalLDA, self).__init__()
diff --git a/train.py b/train.py
index e17b5cc..9a6f447 100644
--- a/train.py
+++ b/train.py
@@ -187,7 +187,7 @@ class Solver:
     
             # Training phase (we ignore returned values here)
             self.iterate(epoch, 'train')
-            if epoch % 5 == 0:
+            if epoch % 2 == 0:
                 import time
                 start_time = time.time()
                 lda_accuracy = run_linear_probe_on_embeddings(
@@ -221,7 +221,7 @@ class Solver:
 
 def setup(rank, world_size):
     os.environ['MASTER_ADDR'] = 'localhost'
-    os.environ['MASTER_PORT'] = '12354'
+    os.environ['MASTER_PORT'] = '12355'
     
     # Initialize the process group
     dist.init_process_group("nccl", rank=rank, world_size=world_size)
@@ -369,37 +369,37 @@ def train_worker(rank, world_size, config):
     ])
     
     
-     # trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
-    # valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
-    # testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
 
 
 
-    # Load the full datasets
-    trainset_full = datasets.ImageFolder(config['train_dir'], transform=transform_train)
-    valset_full = datasets.ImageFolder(config['val_dir'], transform=transform_test)
-    testset_full = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    # # Load the full datasets
+    # trainset_full = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    # valset_full = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    # testset_full = datasets.ImageFolder(config['test_dir'], transform=transform_test)
     
-    # Select 10 class indices (e.g., 10 random or specific ones)
-    selected_classes = list(range(100))  # or any 10 specific indices you want
+    # # Select 10 class indices (e.g., 10 random or specific ones)
+    # selected_classes = list(range(100))  # or any 10 specific indices you want
     
-    # Map class name to index
-    class_to_idx = trainset_full.class_to_idx
-    idx_to_class = {v: k for k, v in class_to_idx.items()}
+    # # Map class name to index
+    # class_to_idx = trainset_full.class_to_idx
+    # idx_to_class = {v: k for k, v in class_to_idx.items()}
 
     
-    # Create a filter function
-    def filter_by_class(dataset, allowed_classes):
-        indices = [i for i, (_, label) in enumerate(dataset.samples) if label in allowed_classes]
-        return Subset(dataset, indices)
+    # # Create a filter function
+    # def filter_by_class(dataset, allowed_classes):
+    #     indices = [i for i, (_, label) in enumerate(dataset.samples) if label in allowed_classes]
+    #     return Subset(dataset, indices)
     
-    # Create filtered datasets
-    trainset = filter_by_class(trainset_full, selected_classes)
-    valset = filter_by_class(valset_full, selected_classes)
-    testset = filter_by_class(testset_full, selected_classes)
+    # # Create filtered datasets
+    # trainset = filter_by_class(trainset_full, selected_classes)
+    # valset = filter_by_class(valset_full, selected_classes)
+    # testset = filter_by_class(testset_full, selected_classes)
 
     # Create subset
-    transit_size = int(1.0 * len(trainset))
+    transit_size = int(0.1 * len(trainset))
     indices = random.sample(range(len(trainset)), transit_size)
     transit_subset = Subset(trainset, indices)
 
@@ -491,7 +491,7 @@ if __name__ == '__main__':
         'wandb_entity': "gerardo-pastrana-c3-ai",
         'wandb_group': "gapLoss",
         'seed': 42,
-        'n_classes': 100,
+        'n_classes': 1000,
         'train_val_split': 0.1,
         'batch_size': 1024,  # Global batch size
         'num_workers': 1,  # Adjust based on CPU cores
@@ -503,9 +503,9 @@ if __name__ == '__main__':
         'lamb': 0.0002,
         'n_eig': 4,
         'margin': None,
-        'epochs': 50,
-        'k_classes': 100,
-        'n_samples': 64,
+        'epochs': 100,
+        'k_classes': 180,
+        'n_samples': 64, #32
         # Memory optimization parameters
         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
         'use_amp': True,                   # Use automatic mixed precision
diff --git a/wandb/latest-run b/wandb/latest-run
index 724b935..18febb5 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250514_023337-no5goqf3
\ No newline at end of file
+run-20250518_005137-51pczvjv
\ No newline at end of file
diff --git a/wandb/run-20250514_023337-no5goqf3/run-no5goqf3.wandb b/wandb/run-20250514_023337-no5goqf3/run-no5goqf3.wandb
index e54b50a..c4a98cd 100644
Binary files a/wandb/run-20250514_023337-no5goqf3/run-no5goqf3.wandb and b/wandb/run-20250514_023337-no5goqf3/run-no5goqf3.wandb differ
