Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/train.py b/train.py
index 4307a4b..e242964 100644
--- a/train.py
+++ b/train.py
@@ -317,6 +317,7 @@ class Solver:
         best_loss = float('inf')
         for epoch in range(epochs):
             # Set epoch for distributed samplers
+            train_sampler.set_epoch(epoch) 
             if self.world_size > 1:
                 for phase in self.dataloaders:
                     if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
@@ -473,51 +474,94 @@ def train_worker(rank, world_size, config):
     from collections import defaultdict
     
     class ClassBalancedBatchSampler(Sampler):
-        def __init__(self, dataset, k_classes, n_samples, world_size=1, rank=0, seed=42):
+        def __init__(self, dataset, k_classes, n_samples, world_size=1, rank=0, seed=42, iterations_per_epoch=100):
+            """
+            Class-balanced batch sampler that can be used with distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes to sample per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes/GPUs in distributed training
+                rank: Rank of current process
+                seed: Random seed
+                iterations_per_epoch: Number of batches to generate per epoch
+            """
             self.dataset = dataset
             self.k_classes = k_classes
             self.n_samples = n_samples
             self.world_size = world_size
             self.rank = rank
             self.seed = seed
-    
+            self.iterations_per_epoch = iterations_per_epoch
+            
             # Get targets (handle Subset)
             if isinstance(dataset, torch.utils.data.Subset):
-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+                if hasattr(dataset.dataset, 'targets'):
+                    targets = [dataset.dataset.targets[i] for i in dataset.indices]
+                else:  # Handle case where targets are in another attribute or format
+                    targets = [dataset.dataset.samples[i][1] for i in dataset.indices]
                 indices = dataset.indices
             else:
-                targets = dataset.targets
+                if hasattr(dataset, 'targets'):
+                    targets = dataset.targets
+                else:  # Handle case where targets are in another attribute or format
+                    targets = [sample[1] for sample in dataset.samples]
                 indices = list(range(len(targets)))
     
             # Build class to index mapping
             self.class_to_indices = defaultdict(list)
-            for idx in indices:
-                label = targets[idx]
-                self.class_to_indices[label].append(idx)
+            for idx, target in zip(indices, targets):
+                self.class_to_indices[target].append(idx)
     
-            self.classes = sorted(self.class_to_indices.keys())
+            self.classes = [cls for cls in self.class_to_indices.keys() 
+                           if len(self.class_to_indices[cls]) >= n_samples]
+            
+            if len(self.classes) < k_classes:
+                raise ValueError(f"Only {len(self.classes)} classes have {n_samples} or more samples. "
+                               f"Cannot sample {k_classes} classes.")
+                
             self.epoch = 0
+            self.samples_per_gpu = k_classes * n_samples // world_size
+            
+            if k_classes * n_samples % world_size != 0:
+                raise ValueError(f"k_classes ({k_classes}) * n_samples ({n_samples}) = {k_classes * n_samples} "
+                               f"must be divisible by world_size ({world_size})")
     
         def __iter__(self):
-            random.seed(self.seed + self.epoch + self.rank)
-    
-            selected_classes = random.sample(self.classes, self.k_classes)
-    
-            batch = []
-            for cls in selected_classes:
-                samples = random.choices(self.class_to_indices[cls], k=self.n_samples)
-                batch.extend(samples)
-    
-            # Split the batch for different GPUs
-            # Make sure total batch is divisible by world_size
-            assert len(batch) % self.world_size == 0, "Batch size must be divisible by world_size"
-    
-            local_batch = batch[self.rank::self.world_size]
-            yield local_batch
+            # Create new random generator with seed dependent on epoch and rank
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+            
+            # For each batch
+            for _ in range(self.iterations_per_epoch):
+                batch = []
+                
+                # Sample k_classes with replacement
+                selected_classes = random.sample(self.classes, self.k_classes)
+                
+                # For each class, sample n_samples instances
+                for cls in selected_classes:
+                    # Sample with replacement if necessary
+                    available_indices = self.class_to_indices[cls]
+                    if len(available_indices) < self.n_samples:
+                        samples = random.choices(available_indices, k=self.n_samples)
+                    else:
+                        samples = random.sample(available_indices, self.n_samples)
+                    batch.extend(samples)
+                
+                # Ensure consistent shuffling across processes
+                indices = torch.tensor(batch, dtype=torch.int64)
+                indices = indices[torch.randperm(len(indices), generator=g)]
+                batch = indices.tolist()
+                
+                # Each GPU gets an equal portion of the batch
+                local_batch = batch[self.rank::self.world_size]
+                yield local_batch
     
         def __len__(self):
-            # 1 batch per epoch in this simple version
-            return 1
+            # Return number of batches per epoch
+            return self.iterations_per_epoch
     
         def set_epoch(self, epoch):
             self.epoch = epoch
@@ -577,13 +621,14 @@ def train_worker(rank, world_size, config):
         world_size=world_size,
         rank=rank,
         seed=config['seed'],
+        iterations_per_epoch=config.get('iterations_per_epoch', 100)  # Default to 100 batches per epoch
     )
 
     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
 
     # Create dataloaders
-    trainloader = trainloader = torch.utils.data.DataLoader(
+    trainloader = torch.utils.data.DataLoader(
         trainset,
         batch_sampler=train_sampler,
         num_workers=config['num_workers'],
diff --git a/wandb/latest-run b/wandb/latest-run
index a67637e..d2f7d48 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250428_195803-u71mgr7j
\ No newline at end of file
+run-20250428_201229-so82nni2
\ No newline at end of file
