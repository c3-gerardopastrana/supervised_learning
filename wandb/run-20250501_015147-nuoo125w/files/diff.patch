Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/eval.py b/eval.py
index eb92927..b40d118 100644
--- a/eval.py
+++ b/eval.py
@@ -3,6 +3,8 @@ import torch.nn.functional as F
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 from sklearn.metrics import accuracy_score
 import torch.distributed as dist
+from contextlib import nullcontext
+from tqdm import tqdm
 
 def gather_tensor(tensor):
     world_size = dist.get_world_size()
@@ -10,19 +12,20 @@ def gather_tensor(tensor):
     dist.all_gather(tensors_gather, tensor)
     return torch.cat(tensors_gather, dim=0)
 
-from tqdm import tqdm
+
 
 def run_lda_on_embeddings(train_loader, val_loader, model, device=None, use_amp=True):
     device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
     model.to(device)
     model.eval()
 
-    def extract_embeddings(loader, split_name):
+    def extract_embeddings(loader):
         embeddings, labels = [], []
         rank = dist.get_rank() if dist.is_initialized() else 0
-        desc = f"[{split_name}] Rank {rank}"
+        progress = tqdm(loader) if rank == 0 else nullcontext(loader)
+    
         with torch.no_grad():
-            for x, y in tqdm(loader, desc=desc, leave=False):
+            for x, y in progress:
                 x = x.to(device, non_blocking=True)
                 y = y.to(device, non_blocking=True)
                 with torch.cuda.amp.autocast(enabled=use_amp):
@@ -34,17 +37,18 @@ def run_lda_on_embeddings(train_loader, val_loader, model, device=None, use_amp=
         embeddings = torch.cat(embeddings)
         labels = torch.cat(labels)
     
-        # Gather across processes
         if dist.is_initialized():
             embeddings = gather_tensor(embeddings)
             labels = gather_tensor(labels)
     
-        return embeddings.cpu().numpy(), labels.cpu().numpy()
+    
+            return embeddings.cpu().numpy(), labels.cpu().numpy()
 
     X_train, y_train = extract_embeddings(train_loader)
     X_val, y_val = extract_embeddings(val_loader)
     print("rank done",dist.get_rank())
     if dist.get_rank() == 0:
+        print("LDA on ",X_train.shape)
         lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')
         lda.fit(X_train, y_train)
         y_pred = lda.predict(X_val)
diff --git a/train.py b/train.py
index 0b980dd..0f4bfd8 100644
--- a/train.py
+++ b/train.py
@@ -387,11 +387,16 @@ def train_worker(rank, world_size, config):
         normalize,
     ])
     
-    # Create subset
+    
     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
 
+    # Create subset
+    transit_size = int(0.1 * len(trainset))
+    indices = random.sample(range(len(trainset)), transit_size)
+    transit_subset = Subset(trainset, indices)
+
     # Create distributed samplers
     train_sampler = ClassBalancedBatchSampler(
         dataset=trainset,
@@ -405,7 +410,7 @@ def train_worker(rank, world_size, config):
 
     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
-    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(transit_subset, num_replicas=world_size, rank=rank, shuffle=False)
     
 
     # Create dataloaders
@@ -435,7 +440,7 @@ def train_worker(rank, world_size, config):
     )
         
     complete_train_loader = torch.utils.data.DataLoader(
-        trainset, 
+        transit_subset, 
         batch_size=config['batch_size'],
         sampler=complete_train_sampler,
         num_workers=config['num_workers'],
diff --git a/wandb/latest-run b/wandb/latest-run
index f3087a6..bc89c32 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250430_230948-up74d2pj
\ No newline at end of file
+run-20250501_015147-nuoo125w
\ No newline at end of file
diff --git a/wandb/run-20250430_230948-up74d2pj/run-up74d2pj.wandb b/wandb/run-20250430_230948-up74d2pj/run-up74d2pj.wandb
index 93ae6ef..80d0c0e 100644
Binary files a/wandb/run-20250430_230948-up74d2pj/run-up74d2pj.wandb and b/wandb/run-20250430_230948-up74d2pj/run-up74d2pj.wandb differ
