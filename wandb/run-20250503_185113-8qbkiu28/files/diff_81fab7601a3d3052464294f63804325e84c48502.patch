diff --git a/eval.py b/eval.py
index 16237a4..cd5f21d 100644
--- a/eval.py
+++ b/eval.py
@@ -57,8 +57,8 @@ def run_linear_probe_on_embeddings(train_loader, val_loader, model, device=None,
         train_ds = TensorDataset(X_train, y_train)
         val_ds = TensorDataset(X_val, y_val)
 
-        train_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)
-        val_loader = DataLoader(val_ds, batch_size=4096)
+        train_loader = DataLoader(train_ds, batch_size=8192, shuffle=True)
+        val_loader = DataLoader(val_ds, batch_size=8192)
 
         # Define linear classifier
         classifier = LinearClassifier(X_train.shape[1], int(y_train.max()) + 1).to(device)
diff --git a/lda.py b/lda.py
index 89aac69..30d9857 100644
--- a/lda.py
+++ b/lda.py
@@ -2,6 +2,64 @@ import torch
 import torch.nn as nn
 from functools import partial
 import torch.nn.functional as F
+from sklearn.covariance import ledoit_wolf_shrinkage
+
+
+
+
+def soft_thresholding(X, lmbda):
+    return torch.sign(X) * torch.clamp(torch.abs(X) - lmbda, min=0.0)
+
+def graphical_lasso(S, lmbda=1, max_iter=5, tol=1e-4):
+    """
+    Sparse inverse covariance estimation via graphical lasso (block coordinate descent).
+    
+    Args:
+        S (torch.Tensor): Empirical covariance matrix, shape (p, p)
+        lmbda (float): Sparsity regularization strength (λ)
+        max_iter (int): Max iterations
+        tol (float): Convergence tolerance
+    
+    Returns:
+        Theta (torch.Tensor): Estimated sparse inverse covariance (precision matrix)
+        Sigma (torch.Tensor): Estimated covariance matrix (inverse of Theta)
+    """
+    print('start')
+    p = S.shape[0]
+    W = S.clone()
+    Theta = torch.inverse(W)
+    
+    for iteration in range(max_iter):
+        Theta_old = Theta.clone()
+
+        for j in range(p):
+            # Partition the matrix
+            idx = [i for i in range(p) if i != j]
+            S11 = S[idx][:, idx]
+            s12 = S[idx, j]
+            theta12 = Theta[idx, j]
+
+            # Solve lasso subproblem: β = argmin ½ βᵀ S11 β - s12ᵀ β + λ‖β‖₁
+            # Using coordinate descent on β
+            beta = theta12.clone()
+            for _ in range(10):  # inner loop for convergence
+                for k in range(p - 1):
+                    tmp = s12[k] - S11[k, :].dot(beta) + S11[k, k] * beta[k]
+                    beta[k] = soft_thresholding(tmp, lmbda) / S11[k, k]
+            
+            # Update precision matrix
+            Theta[j, idx] = Theta[idx, j] = -beta
+            Theta[j, j] = 1.0 / (S[j, j] - s12.dot(beta))
+
+        # Check convergence
+        delta = torch.norm(Theta - Theta_old, p='fro') / torch.norm(Theta_old, p='fro')
+        if delta < tol:
+            break
+
+    #Sigma = torch.inverse(Theta)
+    return Theta
+
+
 
 def lda(X, y, n_classes, lamb):
     X = X.view(X.shape[0], -1)
@@ -39,11 +97,24 @@ def lda(X, y, n_classes, lamb):
     Sb = St - Sw
     
     # Add regularization to Sw
-    Sw = Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
-    
+    #lamb = torch.trace(Sw) / D
+    #Sw = graphical_lasso(Sw)
     
+    #Sw = Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+    #Sw_np = Sw.cpu().detach().numpy()
+    #lw = ledoit_wolf().fit(Sw_np)  # add a sample axis if needed
+    shrinkage = 0.9
+    mu = torch.trace(Sw) / D
 
-    temp = torch.linalg.solve(Sw, Sb) #torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) 
+    # Apply shrinkage — differentiable
+    Sw = (1 - shrinkage) * Sw + shrinkage * mu * torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False)
+    #Sw =  Sw +  mu * torch.eye(Sw.shape[0], device=Sw.device, dtype=Sw.dtype)
+    #Sw =  mu * torch.eye(Sw.shape[0], device=Sw.device, dtype=Sw.dtype)
+    # diag_values = torch.rand(D, device="cuda") * 0.0001  # Uniform[0, lam)
+    # Sw = Sw + torch.diag(diag_values)
+    
+    
+    temp =  torch.linalg.lstsq(Sw, Sb).solution #torch.linalg.solve(Sw, Sb)# #torch.linalg.lstsq(Sw, Sb).solution #torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) 
     # # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
     # evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
     # print(evals.shape, evecs.shape)
@@ -64,7 +135,30 @@ def lda(X, y, n_classes, lamb):
     # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
     # Use the new torch.linalg.eig for general matrices
     # It returns complex eigenvalues and eigenvectors by default
-    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    try:
+        evals_complex, evecs_complex = torch.linalg.eig(temp)
+    except:
+        #print(Sw)
+        print(torch.trace(Sw))
+        print(torch.linalg.eigvalsh(Sw))
+        eigvals = torch.linalg.eigvalsh(Sw)
+        
+        # Sort to ensure consistent behavior
+        eigvals_sorted = eigvals.sort().values
+        
+        # Compute pairwise differences (broadcasted)
+        diffs = eigvals_sorted.unsqueeze(0) - eigvals_sorted.unsqueeze(1)
+        
+        # Avoid division by zero: set diagonal to inf (or mask it)
+        diffs.fill_diagonal_(float('inf'))
+        
+        # Compute reciprocal and take the max of absolute values
+        inv_diffs = (1.0 / diffs).abs()
+        max_val = inv_diffs.max()
+        
+        print("Eigenvalues:", eigvals_sorted)
+        print("Max 1/|λ_i - λ_j|:", max_val)
+
 
     # Process complex eigenvalues returned by torch.linalg.eig
     # Check for eigenvalues with non-negligible imaginary parts
@@ -135,8 +229,11 @@ def sina_loss(sigma_w_inv_b):
     # # loss = torch.norm(diff, p='fro')**2
 
     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
-    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
-    penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+    # lambda_target = torch.tensor(2**10, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    # penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+    lambda_target = torch.tensor(2**8, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    diff = lambda_target - trace  # positive if trace < lambda_target
+    penalty = F.relu(diff).pow(2) / lambda_target.pow(2)  # scale-free, no penalty if trace >= lambda_target
 
     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
     
diff --git a/train.py b/train.py
index ee8d316..afb91b0 100644
--- a/train.py
+++ b/train.py
@@ -81,7 +81,7 @@ class Solver:
     
         if hasComplexEVal:
             print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
-            return None, None, None
+            #return None
     
         metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
         loss = self.criterion(sigma_w_inv_b)
@@ -208,20 +208,21 @@ class Solver:
                 val_loss, val_acc = self.iterate(epoch, 'val')
             
             # All processes run this to contribute their part of the embeddings
-            import time
-            start_time = time.time()
-            lda_accuracy = run_linear_probe_on_embeddings(
-                self.dataloaders['complete_train'],
-                self.dataloaders['val'],
-                self.get_net(),
-                use_amp=self.use_amp
-            )
-            
-            # Only rank 0 gets accuracy; others get None
-            if self.local_rank == 0 and lda_accuracy is not None:
-                wandb.log({'lda_accuracy': lda_accuracy})
-                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
-                print(f"Total time: {elapsed_time:.2f} minutes")
+            if epoch % 7 == 0:
+                import time
+                start_time = time.time()
+                lda_accuracy = run_linear_probe_on_embeddings(
+                    self.dataloaders['complete_train'],
+                    self.dataloaders['val'],
+                    self.get_net(),
+                    use_amp=self.use_amp
+                )
+                
+                # Only rank 0 gets accuracy; others get None
+                if self.local_rank == 0 and lda_accuracy is not None:
+                    wandb.log({'lda_accuracy': lda_accuracy})
+                    elapsed_time = (time.time() - start_time) / 60  # convert to minutes
+                    print(f"Total time: {elapsed_time:.2f} minutes")
 
     
             # Save best model
@@ -388,14 +389,40 @@ def train_worker(rank, world_size, config):
     ])
     
     
-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
-    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
-    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    # trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    # valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    # testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+
+
+    # Load the full datasets
+    trainset_full = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset_full = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset_full = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+    
+    # Select 10 class indices (e.g., 10 random or specific ones)
+    selected_classes = list(range(10))  # or any 10 specific indices you want
+    
+    # Map class name to index
+    class_to_idx = trainset_full.class_to_idx
+    idx_to_class = {v: k for k, v in class_to_idx.items()}
+    
+    # Create a filter function
+    def filter_by_class(dataset, allowed_classes):
+        indices = [i for i, (_, label) in enumerate(dataset.samples) if label in allowed_classes]
+        return Subset(dataset, indices)
+    
+    # Create filtered datasets
+    trainset = filter_by_class(trainset_full, selected_classes)
+    valset = filter_by_class(valset_full, selected_classes)
+    testset = filter_by_class(testset_full, selected_classes)
+
+    
 
     # Create subset
     transit_size = int(0.1 * len(trainset))
     indices = random.sample(range(len(trainset)), transit_size)
-    transit_subset = Subset(trainset, indices)
+    transit_subset = trainset#Subset(trainset, indices)
 
     # Create distributed samplers
     train_sampler = ClassBalancedBatchSampler(
@@ -494,12 +521,12 @@ if __name__ == '__main__':
         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
         'model_path': 'models/deeplda_best.pth',
         'loss': 'LDA',
-        'lamb': 0.1,
+        'lamb': 0.01,
         'n_eig': 4,
         'margin': None,
-        'epochs': 25,
-        'k_classes': 64,
-        'n_samples': 128,
+        'epochs': 50,
+        'k_classes': 10,
+        'n_samples': 64,
         # Memory optimization parameters
         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
         'use_amp': True,                   # Use automatic mixed precision
diff --git a/wandb/latest-run b/wandb/latest-run
index 710b4c2..97a2ebc 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250501_172906-uyn62eav
\ No newline at end of file
+run-20250503_185113-8qbkiu28
\ No newline at end of file
