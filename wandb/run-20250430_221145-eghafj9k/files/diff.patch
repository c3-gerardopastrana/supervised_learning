Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/train.py b/train.py
index 213528a..42579c3 100644
--- a/train.py
+++ b/train.py
@@ -255,18 +255,18 @@ def train_worker(rank, world_size, config):
     warnings.simplefilter(action='ignore', category=FutureWarning)
     
     class ClassBalancedBatchSampler(Sampler):
-        def __init__(self, dataset, k_classes: int, n_samples: int,
-                     world_size: int = 1, rank: int = 0, seed: int = 42):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
             """
             Class-balanced batch sampler for distributed training.
-    
+            
             Args:
-                dataset: Dataset to sample from.
-                k_classes: Number of different classes in each batch.
-                n_samples: Number of samples per class.
-                world_size: Total number of distributed workers.
-                rank: Rank of the current worker.
-                seed: Random seed for reproducibility.
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
             """
             super().__init__(dataset)
             self.dataset = dataset
@@ -275,59 +275,82 @@ def train_worker(rank, world_size, config):
             self.world_size = world_size
             self.rank = rank
             self.seed = seed
-            self.epoch = 0  # Set externally before each epoch
+            self.epoch = 0  # must be set each epoch manually!
     
-            # Get target labels and build class-to-indices mapping
+            # Build mapping from class to list of indices
             if isinstance(dataset, torch.utils.data.Subset):
-                indices = dataset.indices
-                targets = [dataset.dataset.targets[i] for i in indices]
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
             else:
-                indices = range(len(dataset))
                 targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
     
-            self.class_to_indices = defaultdict(list)
-            for idx, label in zip(indices, targets):
-                self.class_to_indices[label].append(idx)
-    
-            # Filter out classes with insufficient samples
+            # Only keep classes that have enough samples
             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
                                       if len(idxs) >= n_samples]
-            if len(self.available_classes) < k_classes:
-                raise ValueError(f"Need at least {k_classes} classes with â‰¥{n_samples} samples each, "
-                                 f"but only {len(self.available_classes)} are available.")
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
     
-            # Estimate batches per epoch
+            # Compute approximately how many batches can fit
             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
-            batch_size = k_classes * n_samples
+            batch_size = self.k_classes * self.n_samples
             self.batches_per_epoch = total_samples // batch_size
     
-        def set_epoch(self, epoch: int):
+        def set_epoch(self, epoch):
             self.epoch = epoch
     
         def __iter__(self):
-            rng = random.Random(self.seed + self.epoch + self.rank)
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
             num_batches = 0
-            batch_size = self.k_classes * self.n_samples
-    
             while num_batches < self.batches_per_epoch:
-                selected_classes = rng.sample(self.available_classes, self.k_classes)
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
     
-                batch = np.empty(batch_size, dtype=int)
-                offset = 0
-                for cls in selected_classes:
-                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
-                    batch[offset:offset + self.n_samples] = sampled_indices
-                    offset += self.n_samples
+            # all_batches = []
     
-                # Shard to the correct worker
-                if num_batches % self.world_size == self.rank:
-                    yield batch.tolist()
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
     
-                num_batches += 1
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
+    
+            #     all_batches.append(batch)
+    
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
+    
+            # for batch in local_batches:
+            #     yield batch
     
         def __len__(self):
             return self.batches_per_epoch // self.world_size
-
             
     # Configure CUDA
     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
diff --git a/wandb/latest-run b/wandb/latest-run
index 08e9ddd..ae89468 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250430_215907-4anamnlf
\ No newline at end of file
+run-20250430_221145-eghafj9k
\ No newline at end of file
diff --git a/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb b/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb
index e69de29..e96792e 100644
Binary files a/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb and b/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb differ
