Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/approx_accurayc.ipynb b/approx_accurayc.ipynb
new file mode 100644
index 0000000..df2e7a0
--- /dev/null
+++ b/approx_accurayc.ipynb
@@ -0,0 +1,401 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "id": "cbc9b9ce-83cb-4097-af87-b439ce94b5ae",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#!pip install torch torchvision torchaudio\n",
+    "from torchvision import datasets, transforms\n",
+    "from torch.utils.data import Subset, DataLoader\n",
+    "import numpy as np\n",
+    "from pathlib import Path\n",
+    "import torch.nn.functional as F\n",
+    "\n",
+    "home = Path('/data')\n",
+    "train_dir = home / 'datasets/imagenet_full_size/061417/train'\n",
+    "test_dir = home / 'datasets/imagenet_full_size/061417/val'\n",
+    "\n",
+    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
+    "                                 std=[0.229, 0.224, 0.225])\n",
+    "\n",
+    "transform = transforms.Compose([\n",
+    "    transforms.Resize(256),\n",
+    "    transforms.CenterCrop(224),\n",
+    "    transforms.ToTensor(),\n",
+    "    normalize,\n",
+    "])\n",
+    "\n",
+    "# Apply transform_test for deterministic features\n",
+    "trainset = datasets.ImageFolder(train_dir, transform=transform)\n",
+    "testset = datasets.ImageFolder(test_dir, transform=transform)\n",
+    "\n",
+    "# Sample 10% of trainset\n",
+    "subset_size = int(0.1 * len(trainset))\n",
+    "indices = np.random.choice(len(trainset), subset_size, replace=False)\n",
+    "train_subset = Subset(trainset, indices)\n",
+    "\n",
+    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=False, num_workers=4)\n",
+    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "id": "06e85daf-364d-43ea-8bd2-9c2e9fe4bf7f",
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "ResNet(\n",
+       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
+       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "  (relu): ReLU(inplace=True)\n",
+       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
+       "  (layer1): Sequential(\n",
+       "    (0): BasicBlock(\n",
+       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential()\n",
+       "    )\n",
+       "    (1): BasicBlock(\n",
+       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential()\n",
+       "    )\n",
+       "  )\n",
+       "  (layer2): Sequential(\n",
+       "    (0): BasicBlock(\n",
+       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential(\n",
+       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
+       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      )\n",
+       "    )\n",
+       "    (1): BasicBlock(\n",
+       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential()\n",
+       "    )\n",
+       "  )\n",
+       "  (layer3): Sequential(\n",
+       "    (0): BasicBlock(\n",
+       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential(\n",
+       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
+       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      )\n",
+       "    )\n",
+       "    (1): BasicBlock(\n",
+       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential()\n",
+       "    )\n",
+       "  )\n",
+       "  (layer4): Sequential(\n",
+       "    (0): BasicBlock(\n",
+       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential(\n",
+       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
+       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      )\n",
+       "    )\n",
+       "    (1): BasicBlock(\n",
+       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
+       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
+       "      (shortcut): Sequential()\n",
+       "    )\n",
+       "  )\n",
+       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
+       "  (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
+       ")"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "import sklearn\n",
+    "import torch\n",
+    "from train import ResNet18\n",
+    "\n",
+    "# Set up device\n",
+    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
+    "\n",
+    "# Recreate model architecture\n",
+    "n_classes = 1000  # or whatever your original number was\n",
+    "lda_args = None   # or use the same lda_args you trained with\n",
+    "\n",
+    "checkpoint = torch.load(\"models/deeplda_best.pth\", map_location=device)\n",
+    "\n",
+    "# Rebuild model\n",
+    "embedding_model = ResNet18(num_classes=n_classes, lda_args=lda_args)\n",
+    "embedding_model.load_state_dict(checkpoint['state_dict'])\n",
+    "embedding_model.to(device)\n",
+    "embedding_model.eval()\n",
+    "\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "id": "d5cec810-12c5-424e-8626-73ae2dae21aa",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Test Accuracy: 8.20%\n"
+     ]
+    }
+   ],
+   "source": [
+    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "\n",
+    "# Initialize the LDA model\n",
+    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')  # Optional: you can tweak parameters\n",
+    "\n",
+    "# Fit the LDA model\n",
+    "lda.fit(X_train.cpu().numpy(), y_train.cpu().numpy())\n",
+    "\n",
+    "# Make predictions on the test set\n",
+    "y_pred = lda.predict(X_test.cpu().numpy())\n",
+    "\n",
+    "# Calculate accuracy\n",
+    "acc = accuracy_score(y_test.cpu().numpy(), y_pred)\n",
+    "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "id": "ef515d5d-2663-4698-b56c-37915ba8cc27",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
+      "  return fn(*args, **kwargs)\n",
+      "/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
+      "  warnings.warn(\n"
+     ]
+    }
+   ],
+   "source": [
+    "def extract_embeddings(dataloader, model, device):\n",
+    "    model.eval()\n",
+    "    embeddings, labels = [], []\n",
+    "    with torch.no_grad():\n",
+    "        for x, y in dataloader:\n",
+    "            x = x.to(device)\n",
+    "            y = y.to(device)\n",
+    "            feats = model._forward_features(x)  # Direct embeddings\n",
+    "            feats = F.normalize(feats, p=2, dim=1)  # L2 normalization\n",
+    "            embeddings.append(feats)\n",
+    "            labels.append(y)\n",
+    "    return torch.cat(embeddings), torch.cat(labels)\n",
+    "\n",
+    "X_train, y_train = extract_embeddings(train_loader, embedding_model, device)\n",
+    "X_test, y_test = extract_embeddings(test_loader, embedding_model, device)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "id": "a3a2a90b-5c07-45a5-ac51-cfff63d0fc51",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# import torch\n",
+    "# import torch.nn.functional as F\n",
+    "# from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "# # --- Compute class centroids (normalized) ---\n",
+    "# def compute_normalized_centroids_torch(X, y):\n",
+    "#     classes = torch.unique(y)\n",
+    "#     centroids = []\n",
+    "#     for cls in classes:\n",
+    "#         class_feats = X[y == cls]\n",
+    "#         mean = class_feats.mean(dim=0, keepdim=True)\n",
+    "#         mean = F.normalize(mean, p=2, dim=1)\n",
+    "#         centroids.append(mean)\n",
+    "#     return torch.cat(centroids, dim=0), classes\n",
+    "\n",
+    "# # --- Predict based on cosine similarity ---\n",
+    "# def predict_cosine_torch(X, centroids, classes):\n",
+    "#     X = F.normalize(X, p=2, dim=1)\n",
+    "#     sims = torch.matmul(X, centroids.T)\n",
+    "#     pred_indices = sims.argmax(dim=1)\n",
+    "#     return classes[pred_indices]\n",
+    "\n",
+    "# # --- Main ---\n",
+    "# # Make sure these are torch tensors on the right device\n",
+    "# X_train_torch = X_train.to(device)\n",
+    "# y_train_torch = y_train.to(device)\n",
+    "# X_test_torch = X_test.to(device)\n",
+    "# y_test_torch = y_test.to(device)\n",
+    "\n",
+    "# centroids, class_labels = compute_normalized_centroids_torch(X_train_torch, y_train_torch)\n",
+    "# y_pred_torch = predict_cosine_torch(X_test_torch, centroids, class_labels)\n",
+    "\n",
+    "# # Convert to CPU and NumPy for scoring\n",
+    "# acc = accuracy_score(y_test.cpu().numpy(), y_pred_torch.cpu().numpy())\n",
+    "# print(f\"Centroid Cosine Classifier Accuracy: {acc * 100:.2f}%\")\n",
+    "\n",
+    "model_state = embedding_model.state_dict()\n",
+    "checkpoint_state = checkpoint['state_dict']\n",
+    "\n",
+    "model_keys = set(model_state.keys())\n",
+    "checkpoint_keys = set(checkpoint_state.keys())\n",
+    "\n",
+    "\n",
+    "\n",
+    "\n",
+    "for name in model_state:\n",
+    "    if name in checkpoint_state:\n",
+    "        if not torch.allclose(model_state[name], checkpoint_state[name], atol=1e-6):\n",
+    "            print(f\"Value mismatch in {name}\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "a9377e5d-7614-4851-9720-6d73c2bdbcf6",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "86bff8db-249c-4a4b-bff2-97453fdd2ae8",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import torch\n",
+    "import torch.nn as nn\n",
+    "import torch.nn.functional as F\n",
+    "from torch.utils.data import TensorDataset, DataLoader\n",
+    "\n",
+    "class LinearClassifier(nn.Module):\n",
+    "    def __init__(self, input_dim, num_classes):\n",
+    "        super().__init__()\n",
+    "        self.linear = nn.Linear(input_dim, num_classes)\n",
+    "\n",
+    "    def forward(self, x):\n",
+    "        return self.linear(x)\n",
+    "\n",
+    "# --- Config ---\n",
+    "batch_size = 4096\n",
+    "lr = 1e-2\n",
+    "epochs = 10\n",
+    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
+    "\n",
+    "# --- Prepare data ---\n",
+    "train_ds = TensorDataset(X_train, y_train)\n",
+    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
+    "\n",
+    "test_ds = TensorDataset(X_test, y_test)\n",
+    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
+    "\n",
+    "# --- Model, Loss, Optimizer ---\n",
+    "model = LinearClassifier(X_train.shape[1], int(y_train.max().item()) + 1).to(device)\n",
+    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
+    "criterion = nn.CrossEntropyLoss()\n",
+    "\n",
+    "# --- Training ---\n",
+    "for epoch in range(epochs):\n",
+    "    model.train()\n",
+    "    correct, total = 0, 0\n",
+    "    for xb, yb in train_loader:\n",
+    "        xb, yb = xb.to(device), yb.to(device)\n",
+    "        out = model(xb)\n",
+    "        loss = criterion(out, yb)\n",
+    "\n",
+    "        optimizer.zero_grad()\n",
+    "        loss.backward()\n",
+    "        optimizer.step()\n",
+    "\n",
+    "        preds = out.argmax(dim=1)\n",
+    "        correct += (preds == yb).sum().item()\n",
+    "        total += yb.size(0)\n",
+    "\n",
+    "    acc = correct / total * 100\n",
+    "    print(f\"Epoch {epoch+1}: Train Accuracy = {acc:.2f}%\")\n",
+    "\n",
+    "# --- Evaluation ---\n",
+    "model.eval()\n",
+    "correct, total = 0, 0\n",
+    "with torch.no_grad():\n",
+    "    for xb, yb in test_loader:\n",
+    "        xb, yb = xb.to(device), yb.to(device)\n",
+    "        preds = model(xb).argmax(dim=1)\n",
+    "        correct += (preds == yb).sum().item()\n",
+    "        total += yb.size(0)\n",
+    "\n",
+    "test_acc = correct / total * 100\n",
+    "print(f\"Test Accuracy = {test_acc:.2f}%\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "868832fb-b1fa-4eac-9679-71b52c686c67",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3 (ipykernel)",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.10.16"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/eval.py b/eval.py
new file mode 100644
index 0000000..67d12b4
--- /dev/null
+++ b/eval.py
@@ -0,0 +1,52 @@
+import torch
+import torch.nn.functional as F
+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
+from sklearn.metrics import accuracy_score
+import torch.distributed as dist
+
+def gather_tensor(tensor):
+    world_size = dist.get_world_size()
+    tensors_gather = [torch.zeros_like(tensor) for _ in range(world_size)]
+    dist.all_gather(tensors_gather, tensor)
+    return torch.cat(tensors_gather, dim=0)
+
+def run_lda_on_embeddings(train_loader, val_loader, model, device=None, use_amp=True):
+    device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    model.to(device)
+    model.eval()
+
+    def extract_embeddings(loader):
+        embeddings, labels = [], []
+        with torch.no_grad():
+            for x, y in loader:
+                x = x.to(device, non_blocking=True)
+                y = y.to(device, non_blocking=True)
+                with torch.cuda.amp.autocast(enabled=use_amp):
+                    feats = model._forward_impl(x)
+                    feats = F.normalize(feats, p=2, dim=1)
+                embeddings.append(feats)
+                labels.append(y)
+        
+        embeddings = torch.cat(embeddings)
+        labels = torch.cat(labels)
+    
+        # Gather across processes
+        if dist.is_initialized():
+            embeddings = gather_tensor(embeddings)
+            labels = gather_tensor(labels)
+    
+        return embeddings.cpu().numpy(), labels.cpu().numpy()
+
+    X_train, y_train = extract_embeddings(train_loader)
+    X_val, y_val = extract_embeddings(val_loader)
+    print("rank done",dist.get_rank())
+    if dist.get_rank() == 0:
+        lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')
+        lda.fit(X_train, y_train)
+        y_pred = lda.predict(X_val)
+        acc = accuracy_score(y_val, y_pred)
+        print(f"LDA Test Accuracy: {acc * 100:.2f}%")
+        return acc
+    else:
+        return None  # only rank 0 computes LDA
+
diff --git a/lda.py b/lda.py
index d99fab3..89aac69 100644
--- a/lda.py
+++ b/lda.py
@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
     # # loss = torch.norm(diff, p='fro')**2
 
     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
 
     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
@@ -162,14 +162,8 @@ class LDA(nn.Module):
         self.n_components = n_classes - 1
         self.lamb = lamb
         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
-        self.running_stats = None  # Stores cumulative LDA stats
 
     def forward(self, X, y):
-        # Initialize or update running stats
-        if self.running_stats is None:
-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
-        self.running_stats.update(X, y)
-
         # Perform batch-wise LDA (temporary, not global yet)
         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
 
@@ -180,41 +174,6 @@ class LDA(nn.Module):
 
         return hasComplexEVal, evals, sigma_w_inv_b
 
-    def finalize_running_stats(self):
-        """Compute global LDA parameters from accumulated running stats."""
-        if self.running_stats is None:
-            raise RuntimeError("No running stats available. Call forward() with data first.")
-
-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
-
-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
-        temp = torch.linalg.solve(Sw, Sb)
-        evals_complex, evecs_complex = torch.linalg.eig(temp)
-
-        tol = 1e-6
-        is_complex = torch.abs(evals_complex.imag) > tol
-        real_idx = ~is_complex
-        evals = evals_complex[real_idx].real
-        evecs = evecs_complex[:, real_idx].real
-
-        if evals.numel() > 0:
-            evals, inc_idx = torch.sort(evals)
-            evecs = evecs[:, inc_idx]
-        else:
-            print("Warning: All eigenvalues were complex.")
-            evals = torch.tensor([], dtype=temp.dtype)
-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
-
-        self.scalings_ = evecs
-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
-
-        return evals  # Optional: return eigenvalues
-
-    def reset_running_stats(self):
-        """Reset accumulated running stats."""
-        self.running_stats = None
-
     def transform(self, X):
         return X.matmul(self.scalings_)[:, :self.n_components]
 
diff --git a/train.py b/train.py
index d179128..42579c3 100644
--- a/train.py
+++ b/train.py
@@ -30,6 +30,7 @@ import wandb
 from lda import LDA, lda_loss, sina_loss, SphericalLDA
 from models import ResNet, BasicBlock
 from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
 
 def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
@@ -56,7 +57,7 @@ class Solver:
         
         self.use_lda = True if lda_args else False
         if self.use_lda:
-            self.criterion = sina_loss  # Assuming this is defined elsewhere
+            self.criterion = sina_loss 
         else:
             self.criterion = nn.CrossEntropyLoss()
         
@@ -67,194 +68,176 @@ class Solver:
             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
 
         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
         self.model_path = model_path
         self.n_classes = n_classes
 
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
     def iterate(self, epoch, phase):
-        if isinstance(self.net, DDP):
-            self.net.module.train(phase == 'train')
-        else:
-            self.net.train(phase == 'train')
-            
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
         dataloader = self.dataloaders[phase]
         total_loss = 0
         correct = 0
         total = 0
         entropy_sum = 0.0
         entropy_count = 0
-
-        # Clear CUDA cache before each epoch
+    
         torch.cuda.empty_cache()
         gc.collect()
-        
+    
         for batch_idx, (inputs, targets) in enumerate(dataloader):
-            # Move data to device
             inputs = inputs.to(self.device, non_blocking=True)
             targets = targets.to(self.device, non_blocking=True)
-            
-            # For training with gradient accumulation
+    
             if phase == 'train':
-               
                 self.optimizer.zero_grad(set_to_none=True)
-                
-                # Apply mixed precision for training
                 with torch.cuda.amp.autocast(enabled=self.use_amp):
                     if self.use_lda:
-                        if isinstance(self.net, DDP):
-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
-                        else:
-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
-                        
-                        if not hasComplexEVal:
-                            # Stats calculation (same as original)
-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
-                            entropy_sum += metrics["entropy"]
-                            entropy_count += 1
-                            loss = self.criterion(sigma_w_inv_b)
-                            
-                            if isinstance(self.net, DDP):
-                                outputs = self.net.module.lda.predict_proba(feas)
-                            else:
-                                outputs = self.net.lda.predict_proba(feas)
-                            
-                            # Only log on rank 0 for efficiency
-                            if phase == 'train' and self.local_rank == 0:
-                                wandb.log(metrics, commit=False)
-                                wandb.log({
-                                    'loss': loss.item(),
-                                    'epoch': epoch,
-                                }, commit=False)
-                        else:
-                            if self.local_rank == 0:
-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
                             continue
+                        loss, outputs, feas, sigma_w_inv_b = result
                     else:
-                        outputs = self.net(inputs, targets, epoch)
+                        outputs = get_net(inputs, targets, epoch)
                         loss = self.criterion(outputs, targets)
-                
-                # Scale loss for gradient accumulation
-                #loss = loss / self.gradient_accumulation_steps
-                
-                if phase == 'train':
-                    # Use gradient scaler for mixed precision
-                    self.scaler.scale(loss).backward()
-                    
-                    # Step optimizer at effective batch boundaries
-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
-                    # Unscale before clipping
-                    self.scaler.unscale_(self.optimizer)
-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
-                    
-                    # Update with scaler
-                    self.scaler.step(self.optimizer)
-                    self.scaler.update()
-                    
-                    if self.local_rank == 0:
-                        wandb.log({"grad_norm": grad_norm.item()})
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
             else:
-                # Validation phase - no gradients needed
                 with torch.no_grad():
                     if self.use_lda:
-                        if isinstance(self.net, DDP):
-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
-                        else:
-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
-                        
-                        if not hasComplexEVal:
-                            loss = self.criterion(sigma_w_inv_b)
-                            
-                            if isinstance(self.net, DDP):
-                                outputs = self.net.module.lda.predict_proba(feas)
-                            else:
-                                outputs = self.net.lda.predict_proba(feas)
-                        else:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
                             continue
+                        loss, outputs, _, _ = result
                     else:
-                        outputs = self.net(inputs, targets, epoch)
+                        outputs = get_net(inputs, targets, epoch)
                         loss = self.criterion(outputs, targets)
-            
-            # Accumulate metrics
-            total_loss += loss.item()  if phase == 'train' else loss.item()
-            
-            outputs = torch.argmax(outputs.detach(), dim=1)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
             total += targets.size(0)
-            correct += outputs.eq(targets).sum().item()
-            
-            # Free memory after each batch
+            correct += pred.eq(targets).sum().item()
+    
             del inputs, targets, outputs
-            if phase == 'train' and self.use_lda and not hasComplexEVal:
+            if self.use_lda and phase == 'train' and result is not None:
                 del feas, sigma_w_inv_b
             torch.cuda.empty_cache()
-        
-        # Sync metrics across GPUs
-        if self.world_size > 1:
-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
-            total_loss, correct, total = metrics.tolist()
+    
             
-        total_loss /= (batch_idx + 1) * self.world_size
-        if total > 0:
-            total_acc = correct / total
-        else:
-            total_acc = 0 
-        
-        # Log metrics
-        if self.local_rank == 0:
-            if entropy_count > 0:
-                average_entropy = entropy_sum / entropy_count
-                print(f'Average Entropy: {average_entropy:.4f}')
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
             
-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
-            wandb.log({
-                f"epoch_{phase}": epoch,
-                f"loss_{phase}": total_loss,
-                f"acc_{phase}": 100.*total_acc
-            }) 
-        return total_loss, total_acc
 
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
 
     def train(self, epochs):
         best_loss = float('inf')
+    
         for epoch in range(epochs):
             # Set epoch for distributed samplers
             if self.world_size > 1:
                 for phase in self.dataloaders:
-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
-                        self.dataloaders[phase].sampler.set_epoch(epoch)
-            
-            # Training phase
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
             self.iterate(epoch, 'train')
-            
+    
             # Validation phase
             with torch.no_grad():
-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
-                    self.net.module.lda.finalize_running_stats()
                 val_loss, val_acc = self.iterate(epoch, 'val')
-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
-                    self.net.module.lda.reset_running_stats()
-                
-                
-            # Save best model
-            if val_loss < best_loss and self.local_rank == 0:
-                best_loss = val_loss
-                if isinstance(self.net, DDP):
-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
-                else:
-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
-                print('best val loss found')
-                torch.save(checkpoint, self.model_path)
             
+            # All processes run this to contribute their part of the embeddings
+            import time
+            start_time = time.time()
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
+                print(f"Total time: {elapsed_time:.2f} minutes")
+
+    
+            # Save best model
             if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
                 print()
-        
-        # Final save on main process
+    
+        # Final save
         if self.local_rank == 0:
-            if isinstance(self.net, DDP):
-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
-            else:
-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
 
 def setup(rank, world_size):
     os.environ['MASTER_ADDR'] = 'localhost'
@@ -268,6 +251,9 @@ def cleanup():
     dist.destroy_process_group()
     
 def train_worker(rank, world_size, config):
+    import warnings
+    warnings.simplefilter(action='ignore', category=FutureWarning)
+    
     class ClassBalancedBatchSampler(Sampler):
         def __init__(self, dataset, k_classes, n_samples,
                      world_size=1, rank=0, seed=42):
@@ -367,7 +353,6 @@ def train_worker(rank, world_size, config):
             return self.batches_per_epoch // self.world_size
             
     # Configure CUDA
-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
     
     # Setup process group
@@ -423,8 +408,8 @@ def train_worker(rank, world_size, config):
         transforms.ToTensor(),
         normalize,
     ])
-
-    # Create datasets
+    
+    # Create subset
     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
@@ -442,6 +427,8 @@ def train_worker(rank, world_size, config):
 
     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
 
     # Create dataloaders
     trainloader = torch.utils.data.DataLoader(
@@ -468,8 +455,16 @@ def train_worker(rank, world_size, config):
         num_workers=config['num_workers'],
         pin_memory=True,
     )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
 
-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
     
     if config['loss'] == 'LDA':
         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
@@ -509,7 +504,7 @@ if __name__ == '__main__':
         'seed': 42,
         'n_classes': 1000,
         'train_val_split': 0.1,
-        'batch_size': 4096,  # Global batch size
+        'batch_size': 8192,  # Global batch size
         'num_workers': 1,  # Adjust based on CPU cores
         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
@@ -520,8 +515,8 @@ if __name__ == '__main__':
         'n_eig': 4,
         'margin': None,
         'epochs': 20,
-        'k_classes':128 ,
-        'n_samples': 64,
+        'k_classes': 64,
+        'n_samples': 128,
         # Memory optimization parameters
         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
         'use_amp': True,                   # Use automatic mixed precision
diff --git a/utils.py b/utils.py
index 2f6d99a..e0ec60e 100644
--- a/utils.py
+++ b/utils.py
@@ -48,4 +48,5 @@ def compute_wandb_metrics(outputs, sigma_w_inv_b):
         "diag_var": diag_var,
     }
 
-    return metrics
\ No newline at end of file
+    return metrics
+
diff --git a/wandb/latest-run b/wandb/latest-run
index 409e3b2..ae89468 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250430_152714-naxvk16q
\ No newline at end of file
+run-20250430_221145-eghafj9k
\ No newline at end of file
diff --git a/wandb/run-20250428_205923-w58ooppt/files/config.yaml b/wandb/run-20250428_205923-w58ooppt/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250428_205923-w58ooppt/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250428_205923-w58ooppt/files/wandb-summary.json b/wandb/run-20250428_205923-w58ooppt/files/wandb-summary.json
new file mode 100644
index 0000000..82d1699
--- /dev/null
+++ b/wandb/run-20250428_205923-w58ooppt/files/wandb-summary.json
@@ -0,0 +1 @@
+{"quantile_25":-4.968658799953118e-07,"min normalized eigenvalue":-0.00034235214116051793,"epoch":4,"totaltrain":16381.171153846153,"quantile_75":-1.466399712768407e-08,"diag_var":1.1852546322188573e-06,"_wandb":{"runtime":6972},"_runtime":6968.252422623,"total_acc_traintrain":19.217211538461537,"total_grad_norm_encoder":16.350032806396484,"max normalized eigenvalue":0.40057581663131714,"trace":0.9409064054489136,"sum_squared_off_diag":0.19426485896110535,"_step":569,"quantile_50":-1.3776416096789035e-07,"epochval":3,"_timestamp":1.7458809320629966e+09,"loss":16381.365234375,"rank simga":43,"epochtrain":3,"totalval":16379.9658203125,"condition simga":9.50071744e+08,"total_acc_trainval":7.478263869721226,"entropy":1.9097437858581543}
\ No newline at end of file
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/code/train.py b/wandb/run-20250428_225639-tkccxcdu/files/code/train.py
new file mode 100644
index 0000000..e21e849
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/code/train.py
@@ -0,0 +1,682 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    config = {
+        'wandb_project': "DELETEME",#"DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': 'datasets/imagenet_full_size/061417/train',
+        'val_dir': 'datasets/imagenet_full_size/061417/val',
+        'test_dir': 'datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 40, 
+        'n_samples': 100, 
+
+    }
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/config.yaml b/wandb/run-20250428_225639-tkccxcdu/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/diff.patch b/wandb/run-20250428_225639-tkccxcdu/files/diff.patch
new file mode 100644
index 0000000..b4b4581
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/diff.patch
@@ -0,0 +1,57 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index eb7ea7d..e21e849 100644
+--- a/train.py
++++ b/train.py
+@@ -665,8 +665,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 100, 
+-        'n_samples': 40, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+ 
+     }
+     
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 2ab01da..d6f9e03 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250428_213549-4si4f9qg
+\ No newline at end of file
++run-20250428_225639-tkccxcdu
+\ No newline at end of file
+diff --git a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb
+index 840cb03..d64bfad 100644
+Binary files a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb and b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb differ
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch b/wandb/run-20250428_225639-tkccxcdu/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch
new file mode 100644
index 0000000..b4b4581
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch
@@ -0,0 +1,57 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index eb7ea7d..e21e849 100644
+--- a/train.py
++++ b/train.py
+@@ -665,8 +665,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 100, 
+-        'n_samples': 40, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+ 
+     }
+     
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 2ab01da..d6f9e03 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250428_213549-4si4f9qg
+\ No newline at end of file
++run-20250428_225639-tkccxcdu
+\ No newline at end of file
+diff --git a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb
+index 840cb03..d64bfad 100644
+Binary files a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb and b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb differ
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/requirements.txt b/wandb/run-20250428_225639-tkccxcdu/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/wandb-metadata.json b/wandb/run-20250428_225639-tkccxcdu/files/wandb-metadata.json
new file mode 100644
index 0000000..3e3d430
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-28T22:56:39.860747Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "328cc18a474dd4757b422b5a8a3c9e839dec3f77"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025315745792"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250428_225639-tkccxcdu/files/wandb-summary.json b/wandb/run-20250428_225639-tkccxcdu/files/wandb-summary.json
new file mode 100644
index 0000000..779a0b5
--- /dev/null
+++ b/wandb/run-20250428_225639-tkccxcdu/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":8}}
\ No newline at end of file
diff --git a/wandb/run-20250428_225639-tkccxcdu/run-tkccxcdu.wandb b/wandb/run-20250428_225639-tkccxcdu/run-tkccxcdu.wandb
new file mode 100644
index 0000000..07b40ae
Binary files /dev/null and b/wandb/run-20250428_225639-tkccxcdu/run-tkccxcdu.wandb differ
diff --git a/wandb/run-20250428_225720-12ybh16o/files/code/train.py b/wandb/run-20250428_225720-12ybh16o/files/code/train.py
new file mode 100644
index 0000000..e21e849
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/code/train.py
@@ -0,0 +1,682 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    config = {
+        'wandb_project': "DELETEME",#"DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': 'datasets/imagenet_full_size/061417/train',
+        'val_dir': 'datasets/imagenet_full_size/061417/val',
+        'test_dir': 'datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 40, 
+        'n_samples': 100, 
+
+    }
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250428_225720-12ybh16o/files/config.yaml b/wandb/run-20250428_225720-12ybh16o/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250428_225720-12ybh16o/files/diff.patch b/wandb/run-20250428_225720-12ybh16o/files/diff.patch
new file mode 100644
index 0000000..b107071
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/diff.patch
@@ -0,0 +1,57 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index eb7ea7d..e21e849 100644
+--- a/train.py
++++ b/train.py
+@@ -665,8 +665,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 100, 
+-        'n_samples': 40, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+ 
+     }
+     
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 2ab01da..1b7ae31 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250428_213549-4si4f9qg
+\ No newline at end of file
++run-20250428_225720-12ybh16o
+\ No newline at end of file
+diff --git a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb
+index 840cb03..d64bfad 100644
+Binary files a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb and b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb differ
diff --git a/wandb/run-20250428_225720-12ybh16o/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch b/wandb/run-20250428_225720-12ybh16o/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch
new file mode 100644
index 0000000..b107071
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/diff_328cc18a474dd4757b422b5a8a3c9e839dec3f77.patch
@@ -0,0 +1,57 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index eb7ea7d..e21e849 100644
+--- a/train.py
++++ b/train.py
+@@ -665,8 +665,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 100, 
+-        'n_samples': 40, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+ 
+     }
+     
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 2ab01da..1b7ae31 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250428_213549-4si4f9qg
+\ No newline at end of file
++run-20250428_225720-12ybh16o
+\ No newline at end of file
+diff --git a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb
+index 840cb03..d64bfad 100644
+Binary files a/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb and b/wandb/run-20250428_205923-w58ooppt/run-w58ooppt.wandb differ
diff --git a/wandb/run-20250428_225720-12ybh16o/files/requirements.txt b/wandb/run-20250428_225720-12ybh16o/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250428_225720-12ybh16o/files/wandb-metadata.json b/wandb/run-20250428_225720-12ybh16o/files/wandb-metadata.json
new file mode 100644
index 0000000..a633b6b
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-28T22:57:20.694324Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "328cc18a474dd4757b422b5a8a3c9e839dec3f77"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025315840000"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250428_225720-12ybh16o/files/wandb-summary.json b/wandb/run-20250428_225720-12ybh16o/files/wandb-summary.json
new file mode 100644
index 0000000..65515b3
--- /dev/null
+++ b/wandb/run-20250428_225720-12ybh16o/files/wandb-summary.json
@@ -0,0 +1 @@
+{"totaltrain":16211.121575342466,"sum_squared_off_diag":0.4605673551559448,"total_acc_trainval":10.722,"condition simga":2.7634733056e+10,"min normalized eigenvalue":-0.0002181880990974605,"epoch":14,"quantile_50":-1.6994505358525203e-07,"loss":16380.0478515625,"epochtrain":13,"entropy":2.101914644241333,"_timestamp":1.7459050463388631e+09,"quantile_75":-3.381287783099651e-08,"_runtime":24005.644956281,"diag_var":3.0262158361438196e-06,"_wandb":{"runtime":24020},"quantile_25":-5.760674639532226e-07,"max normalized eigenvalue":0.36342889070510864,"total_grad_norm_encoder":13.659651756286621,"totalval":16379.6904296875,"total_acc_traintrain":27.936332179930794,"epochval":13,"_step":2101,"rank simga":41,"trace":1.5611255168914795}
\ No newline at end of file
diff --git a/wandb/run-20250428_225720-12ybh16o/run-12ybh16o.wandb b/wandb/run-20250428_225720-12ybh16o/run-12ybh16o.wandb
new file mode 100644
index 0000000..b6edef1
Binary files /dev/null and b/wandb/run-20250428_225720-12ybh16o/run-12ybh16o.wandb differ
diff --git a/wandb/run-20250429_054850-6d97922s/run-6d97922s.wandb b/wandb/run-20250429_054850-6d97922s/run-6d97922s.wandb
new file mode 100644
index 0000000..e69de29
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/code/train.py b/wandb/run-20250429_055038-g6fmztm9/files/code/train.py
new file mode 100644
index 0000000..b4f65cc
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/code/train.py
@@ -0,0 +1,690 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                self.net.module.lda.reset_running_stats()
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        self.net.module.lda.finalize_running_stats()
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    from pathlib import Path
+
+    home = Path('/data')
+    
+    config = {
+        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
+        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
+        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 30, 
+        'n_samples': 100, 
+    }
+
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/config.yaml b/wandb/run-20250429_055038-g6fmztm9/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/diff.patch b/wandb/run-20250429_055038-g6fmztm9/files/diff.patch
new file mode 100644
index 0000000..f44a47a
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/diff.patch
@@ -0,0 +1,60 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..9ced0a3 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_055038-g6fmztm9
+\ No newline at end of file
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch b/wandb/run-20250429_055038-g6fmztm9/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
new file mode 100644
index 0000000..f44a47a
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
@@ -0,0 +1,60 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..9ced0a3 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_055038-g6fmztm9
+\ No newline at end of file
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/requirements.txt b/wandb/run-20250429_055038-g6fmztm9/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/wandb-metadata.json b/wandb/run-20250429_055038-g6fmztm9/files/wandb-metadata.json
new file mode 100644
index 0000000..8d0048f
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-29T05:50:38.043855Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "5a2aa782fc73431ac493eb9545615a928b1ada59"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025331482624"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250429_055038-g6fmztm9/files/wandb-summary.json b/wandb/run-20250429_055038-g6fmztm9/files/wandb-summary.json
new file mode 100644
index 0000000..ffec570
--- /dev/null
+++ b/wandb/run-20250429_055038-g6fmztm9/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_step":57,"entropy":1.3941478729248047,"_runtime":529.14588799,"rank simga":432,"loss":16383.689453125,"_timestamp":1.745906367189374e+09,"total_grad_norm_encoder":0,"condition simga":30328.302734375,"max normalized eigenvalue":0.7652758359909058,"epoch":0,"sum_squared_off_diag":0.0005052589694969356,"trace":0.02890744060277939,"diag_var":3.0033016074071384e-09,"_wandb":{"runtime":533},"quantile_50":-9.97966417344287e-05,"min normalized eigenvalue":-0.009403852745890617,"quantile_75":-5.2039169531781226e-05,"quantile_25":-0.0002151896187569946}
\ No newline at end of file
diff --git a/wandb/run-20250429_055038-g6fmztm9/run-g6fmztm9.wandb b/wandb/run-20250429_055038-g6fmztm9/run-g6fmztm9.wandb
new file mode 100644
index 0000000..953fbe9
Binary files /dev/null and b/wandb/run-20250429_055038-g6fmztm9/run-g6fmztm9.wandb differ
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/code/train.py b/wandb/run-20250429_060008-tjiaaqki/files/code/train.py
new file mode 100644
index 0000000..5301033
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/code/train.py
@@ -0,0 +1,690 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                self.net.module.lda.reset_running_stats()
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        self.net.module.lda.finalize_running_stats()
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    from pathlib import Path
+
+    home = Path('/data')
+    
+    config = {
+        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
+        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
+        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 40, 
+        'n_samples': 100, 
+    }
+
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/config.yaml b/wandb/run-20250429_060008-tjiaaqki/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/diff.patch b/wandb/run-20250429_060008-tjiaaqki/files/diff.patch
new file mode 100644
index 0000000..5c5ede7
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/diff.patch
@@ -0,0 +1,75 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..25471b4 100644
+--- a/lda.py
++++ b/lda.py
+@@ -246,8 +246,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..5301033 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..2148fe8 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_060008-tjiaaqki
+\ No newline at end of file
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch b/wandb/run-20250429_060008-tjiaaqki/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
new file mode 100644
index 0000000..5c5ede7
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
@@ -0,0 +1,75 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..25471b4 100644
+--- a/lda.py
++++ b/lda.py
+@@ -246,8 +246,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..5301033 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 40, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..2148fe8 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_060008-tjiaaqki
+\ No newline at end of file
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/requirements.txt b/wandb/run-20250429_060008-tjiaaqki/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/wandb-metadata.json b/wandb/run-20250429_060008-tjiaaqki/files/wandb-metadata.json
new file mode 100644
index 0000000..445da48
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-29T06:00:08.902624Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "5a2aa782fc73431ac493eb9545615a928b1ada59"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025331957760"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250429_060008-tjiaaqki/files/wandb-summary.json b/wandb/run-20250429_060008-tjiaaqki/files/wandb-summary.json
new file mode 100644
index 0000000..6b68566
--- /dev/null
+++ b/wandb/run-20250429_060008-tjiaaqki/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_runtime":45.252856488,"quantile_25":-0.00019074941519647837,"loss":16383.6494140625,"_wandb":{"runtime":73},"condition simga":27160.244140625,"rank simga":438,"_timestamp":1.7459064541550786e+09,"total_grad_norm_encoder":0,"min normalized eigenvalue":-0.006539130117744207,"diag_var":3.4491667300073914e-09,"sum_squared_off_diag":0.0005636392161250114,"quantile_50":-9.072326793102548e-05,"max normalized eigenvalue":0.734227180480957,"_step":1,"entropy":1.4324251413345337,"epoch":0,"quantile_75":-4.8801277443999425e-05,"trace":0.03167020529508591}
\ No newline at end of file
diff --git a/wandb/run-20250429_060008-tjiaaqki/run-tjiaaqki.wandb b/wandb/run-20250429_060008-tjiaaqki/run-tjiaaqki.wandb
new file mode 100644
index 0000000..61ebebe
Binary files /dev/null and b/wandb/run-20250429_060008-tjiaaqki/run-tjiaaqki.wandb differ
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/code/train.py b/wandb/run-20250429_060442-6tse6h0o/files/code/train.py
new file mode 100644
index 0000000..b4f65cc
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/code/train.py
@@ -0,0 +1,690 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                self.net.module.lda.reset_running_stats()
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        self.net.module.lda.finalize_running_stats()
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    from pathlib import Path
+
+    home = Path('/data')
+    
+    config = {
+        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
+        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
+        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 30, 
+        'n_samples': 100, 
+    }
+
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/config.yaml b/wandb/run-20250429_060442-6tse6h0o/files/config.yaml
new file mode 100644
index 0000000..7c7d3f9
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/config.yaml
@@ -0,0 +1,29 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/diff.patch b/wandb/run-20250429_060442-6tse6h0o/files/diff.patch
new file mode 100644
index 0000000..d13a29b
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/diff.patch
@@ -0,0 +1,75 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..25471b4 100644
+--- a/lda.py
++++ b/lda.py
+@@ -246,8 +246,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..0ffc503 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_060442-6tse6h0o
+\ No newline at end of file
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch b/wandb/run-20250429_060442-6tse6h0o/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
new file mode 100644
index 0000000..d13a29b
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
@@ -0,0 +1,75 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..25471b4 100644
+--- a/lda.py
++++ b/lda.py
+@@ -246,8 +246,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..0ffc503 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_060442-6tse6h0o
+\ No newline at end of file
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/requirements.txt b/wandb/run-20250429_060442-6tse6h0o/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/wandb-metadata.json b/wandb/run-20250429_060442-6tse6h0o/files/wandb-metadata.json
new file mode 100644
index 0000000..acc3653
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-29T06:04:42.119498Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "5a2aa782fc73431ac493eb9545615a928b1ada59"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025332547584"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250429_060442-6tse6h0o/files/wandb-summary.json b/wandb/run-20250429_060442-6tse6h0o/files/wandb-summary.json
new file mode 100644
index 0000000..d43dac6
--- /dev/null
+++ b/wandb/run-20250429_060442-6tse6h0o/files/wandb-summary.json
@@ -0,0 +1 @@
+{"epoch":0,"condition simga":29403.826171875,"max normalized eigenvalue":0.7944540977478027,"_timestamp":1.7459077070516038e+09,"trace":0.026989582926034927,"entropy":1.3773854970932007,"rank simga":435,"_step":111,"diag_var":2.9653011157648734e-09,"quantile_75":-5.531799979507923e-05,"quantile_50":-0.0001051654398906976,"loss":16383.7236328125,"_wandb":{"runtime":1033},"sum_squared_off_diag":0.00046854279935359955,"min normalized eigenvalue":-0.009942916221916676,"total_grad_norm_encoder":0,"quantile_25":-0.00022995847393758595,"_runtime":1024.970173615}
\ No newline at end of file
diff --git a/wandb/run-20250429_060442-6tse6h0o/run-6tse6h0o.wandb b/wandb/run-20250429_060442-6tse6h0o/run-6tse6h0o.wandb
new file mode 100644
index 0000000..b4da1c9
Binary files /dev/null and b/wandb/run-20250429_060442-6tse6h0o/run-6tse6h0o.wandb differ
diff --git a/wandb/run-20250429_063657-2wi22ja6/files/code/train.py b/wandb/run-20250429_063657-2wi22ja6/files/code/train.py
new file mode 100644
index 0000000..b4f65cc
--- /dev/null
+++ b/wandb/run-20250429_063657-2wi22ja6/files/code/train.py
@@ -0,0 +1,690 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                self.net.module.lda.reset_running_stats()
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        self.net.module.lda.finalize_running_stats()
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+    
+            all_batches = []
+    
+            while len(all_batches) < self.batches_per_epoch:
+                # Pick k_classes randomly
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+    
+                all_batches.append(batch)
+    
+            # Shard batches across GPUs
+            local_batches = all_batches[self.rank::self.world_size]
+    
+            for batch in local_batches:
+                yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    from pathlib import Path
+
+    home = Path('/data')
+    
+    config = {
+        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
+        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
+        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 30, 
+        'n_samples': 100, 
+    }
+
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250429_063657-2wi22ja6/files/diff.patch b/wandb/run-20250429_063657-2wi22ja6/files/diff.patch
new file mode 100644
index 0000000..abaeeda
--- /dev/null
+++ b/wandb/run-20250429_063657-2wi22ja6/files/diff.patch
@@ -0,0 +1,85 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..e14d5f8 100644
+--- a/lda.py
++++ b/lda.py
+@@ -165,9 +165,6 @@ class LDA(nn.Module):
+         self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        X = X.view(X.shape[0], -1).detach()
+-        y = y.detach()
+-
+         # Initialize or update running stats
+         if self.running_stats is None:
+             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+@@ -246,8 +243,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..d6b08b1 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_063657-2wi22ja6
+\ No newline at end of file
diff --git a/wandb/run-20250429_063657-2wi22ja6/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch b/wandb/run-20250429_063657-2wi22ja6/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
new file mode 100644
index 0000000..abaeeda
--- /dev/null
+++ b/wandb/run-20250429_063657-2wi22ja6/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
@@ -0,0 +1,85 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..e14d5f8 100644
+--- a/lda.py
++++ b/lda.py
+@@ -165,9 +165,6 @@ class LDA(nn.Module):
+         self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        X = X.view(X.shape[0], -1).detach()
+-        y = y.detach()
+-
+         # Initialize or update running stats
+         if self.running_stats is None:
+             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+@@ -246,8 +243,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..b4f65cc 100644
+--- a/train.py
++++ b/train.py
+@@ -673,13 +673,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..d6b08b1 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250429_063657-2wi22ja6
+\ No newline at end of file
diff --git a/wandb/run-20250429_063657-2wi22ja6/files/requirements.txt b/wandb/run-20250429_063657-2wi22ja6/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250429_063657-2wi22ja6/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250429_063657-2wi22ja6/files/wandb-metadata.json b/wandb/run-20250429_063657-2wi22ja6/files/wandb-metadata.json
new file mode 100644
index 0000000..9dcb284
--- /dev/null
+++ b/wandb/run-20250429_063657-2wi22ja6/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-29T06:36:57.603603Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "5a2aa782fc73431ac493eb9545615a928b1ada59"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2025333452800"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250429_063657-2wi22ja6/run-2wi22ja6.wandb b/wandb/run-20250429_063657-2wi22ja6/run-2wi22ja6.wandb
new file mode 100644
index 0000000..b7b1522
Binary files /dev/null and b/wandb/run-20250429_063657-2wi22ja6/run-2wi22ja6.wandb differ
diff --git a/wandb/run-20250430_040930-ay6bukuh/files/code/train.py b/wandb/run-20250430_040930-ay6bukuh/files/code/train.py
new file mode 100644
index 0000000..cc1eccc
--- /dev/null
+++ b/wandb/run-20250430_040930-ay6bukuh/files/code/train.py
@@ -0,0 +1,708 @@
+import os
+import random
+import numpy as np
+from collections import defaultdict
+np.set_printoptions(precision=4, suppress=True)
+from sklearn.metrics import accuracy_score
+from tqdm.notebook import tqdm
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from PIL import Image
+import torchvision
+from torchvision import transforms, datasets
+import torch.optim as optim
+import wandb
+from functools import partial
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.data.distributed import DistributedSampler
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.checkpoint import checkpoint
+
+
+class BasicBlock(nn.Module):
+    expansion = 1
+    def __init__(self, in_planes, planes, stride=1):
+        super(BasicBlock, self).__init__()
+        self.conv1 = nn.Conv2d(
+            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.conv2 = nn.Conv2d(
+            planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes,
+                          kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes)
+            )
+    
+    def _forward_impl(self, x):
+        out = F.relu(self.bn1(self.conv1(x)))
+        out = self.bn2(self.conv2(out))
+        out += self.shortcut(x)
+        out = F.relu(out)
+        return out
+        
+    def forward(self, x):
+        return checkpoint(self._forward_impl, x)
+
+class ResNet(nn.Module):
+    def __init__(self, block, num_blocks, num_classes=1000, lda_args=None, use_checkpoint=False):
+        super(ResNet, self).__init__()
+        self.lda_args = lda_args
+        self.in_planes = 64
+        self.use_checkpoint = use_checkpoint
+        
+        # ImageNet-style initial conv layer
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,
+                               stride=2, padding=3, bias=False)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu = nn.ReLU(inplace=True)
+        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+        
+        # Residual layers
+        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
+        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
+        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
+        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
+        
+        # Global average pooling and output
+        self.avgpool = nn.AdaptiveAvgPool2d(1)
+        self.linear = nn.Linear(512 * block.expansion, num_classes)
+        
+        # LDA branch (if enabled)
+        if self.lda_args:
+            self.lda = LDA(num_classes, lda_args['lamb'])  # your LDA class
+    
+    def _make_layer(self, block, planes, num_blocks, stride):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.in_planes, planes, stride))
+            self.in_planes = planes * block.expansion
+        return nn.Sequential(*layers)
+    
+    def _forward_features(self, x):
+        out = self.relu(self.bn1(self.conv1(x)))
+        out = self.maxpool(out)
+        
+        if self.use_checkpoint:
+            out = checkpoint(lambda x: self.layer1(x), out)
+            out = checkpoint(lambda x: self.layer2(x), out)
+            out = checkpoint(lambda x: self.layer3(x), out)
+            out = checkpoint(lambda x: self.layer4(x), out)
+        else:
+            out = self.layer1(out)
+            out = self.layer2(out)
+            out = self.layer3(out)
+            out = self.layer4(out)
+            
+        out = self.avgpool(out)  # output shape: [B, 512, 1, 1]
+        fea = out.view(out.size(0), -1)  # flatten to [B, 512]
+        return fea
+    
+    def forward(self, x, y=None, epoch=0):
+        fea = self._forward_features(x)
+        
+        if self.lda_args:
+            fea = F.normalize(fea, p=2, dim=1)
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)
+            return hasComplexEVal, fea, out, sigma_w_inv_b
+        else:
+            out = self.linear(fea)
+            return out
+
+
+def ResNet18(num_classes=1000, lda_args=None):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args)
+
+
+def ResNet34():
+    return ResNet(BasicBlock, [3, 4, 6, 3])
+
+
+def ResNet50():
+    return ResNet(Bottleneck, [3, 4, 6, 3])
+
+
+def ResNet101():
+    return ResNet(Bottleneck, [3, 4, 23, 3])
+
+
+def ResNet152():
+    return ResNet(Bottleneck, [3, 8, 36, 3])
+
+
+class CIFAR10:
+    def __init__(self, img_names, class_map, transform):
+        self.img_names = img_names
+        self.classes = [class_map[os.path.basename(os.path.dirname(n))] for n in img_names]
+        self.transform = transform
+    def __len__(self):
+        return len(self.img_names)
+    def __getitem__(self, idx):
+        img = Image.open(self.img_names[idx])
+        img = self.transform(img)
+        clazz = self.classes[idx]
+        return img, clazz
+
+
+
+def get_scaled_lr_sqrt(batch_size: int, base_batch_size: int = 128, base_lr: float = 1e-3) -> float:
+    """
+    Scales the learning rate with sqrt of batch size increase, where batch size is passed directly.
+
+    Args:
+        batch_size (int): new batch size
+        base_batch_size (int): original batch size corresponding to base_lr
+        base_lr (float): base learning rate at base_batch_size
+
+    Returns:
+        float: scaled learning rate
+    """
+    scale = torch.tensor(batch_size / base_batch_size, dtype=torch.float32)
+    return base_lr * scale.item()
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        
+        self.net = ResNet18(n_classes, lda_args)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank)
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = partial(lda_loss, n_classes=n_classes, 
+                                    n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(self.criterion)
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device)
+            targets = targets.to(self.device)
+            self.optimizer.zero_grad()
+        
+            if self.use_lda:
+                if isinstance(self.net, DDP):
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                else:
+                    hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                
+                if not hasComplexEVal:
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+    
+                    loss = self.criterion(sigma_w_inv_b)
+
+                    if isinstance(self.net, DDP):
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train' and self.local_rank == 0:
+                        wandb.log({
+                            'loss': loss,
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75,
+                            "epoch": epoch,
+                        })
+                    
+                else:
+                    if self.local_rank == 0:
+                        print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
+                    continue
+            else:
+                outputs = self.net(inputs, targets, epoch)
+                loss = nn.CrossEntropyLoss()(outputs, targets)
+        
+            if phase == 'train':
+                loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=100.0)
+                self.optimizer.step()
+                if self.local_rank == 0:
+                    wandb.log({"total_grad_norm_encoder": grad_norm.item()})
+            total_loss += loss.item()
+    
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
+                         % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+            wandb.log({
+                "epoch"+phase: epoch,
+                "total"+phase: total_loss,
+                "total_acc_train"+phase: 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+                
+            self.iterate(epoch, 'train')
+            with torch.no_grad():
+                self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                self.net.module.lda.reset_running_stats()
+                
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+    def test_iterate(self, epoch, phase):
+        self.net.module.lda.finalize_running_stats()
+        if isinstance(self.net, DDP):
+            self.net.module.eval()
+        else:
+            self.net.eval()
+            
+        dataloader = self.dataloaders[phase]
+        y_pred = []
+        y_true = []
+        
+        with torch.no_grad():
+            for inputs, targets in dataloader:
+                inputs = inputs.to(self.device)
+                targets = targets.to(self.device)
+                
+                if self.use_lda:
+                    if isinstance(self.net, DDP):
+                        _, feas, outputs = self.net.module(inputs, targets, epoch)
+                        outputs = self.net.module.lda.predict_proba(feas)
+                    else:
+                        _, feas, outputs = self.net(inputs, targets, epoch)
+                        outputs = self.net.lda.predict_proba(feas)
+                else:
+                    outputs = self.net(inputs, targets, epoch)
+                    
+                outputs = torch.argmax(outputs, dim=1)
+                y_pred.append(outputs.detach().cpu().numpy())
+                y_true.append(targets.detach().cpu().numpy())
+                
+        # Gather predictions from all GPUs
+        if self.world_size > 1:
+            all_y_pred = []
+            all_y_true = []
+            
+            # Convert lists to tensors for gathering
+            local_y_pred = torch.from_numpy(np.concatenate(y_pred)).to(self.device)
+            local_y_true = torch.from_numpy(np.concatenate(y_true)).to(self.device)
+            
+            # Get sizes from all processes
+            size_tensor = torch.tensor([local_y_pred.size(0)], device=self.device)
+            all_sizes = [torch.zeros_like(size_tensor) for _ in range(self.world_size)]
+            dist.all_gather(all_sizes, size_tensor)
+            
+            # Prepare tensors for gathering
+            max_size = max(size.item() for size in all_sizes)
+            padded_pred = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            padded_true = torch.zeros(max_size, dtype=torch.long, device=self.device)
+            
+            # Copy data to padded tensors
+            size = local_y_pred.size(0)
+            padded_pred[:size] = local_y_pred
+            padded_true[:size] = local_y_true
+            
+            # Gather padded tensors
+            gathered_pred = [torch.zeros_like(padded_pred) for _ in range(self.world_size)]
+            gathered_true = [torch.zeros_like(padded_true) for _ in range(self.world_size)]
+            
+            dist.all_gather(gathered_pred, padded_pred)
+            dist.all_gather(gathered_true, padded_true)
+            
+            # Truncate according to original sizes and convert to numpy
+            for i, size in enumerate(all_sizes):
+                all_y_pred.append(gathered_pred[i][:size.item()].cpu().numpy())
+                all_y_true.append(gathered_true[i][:size.item()].cpu().numpy())
+                
+            return np.concatenate(all_y_pred), np.concatenate(all_y_true)
+        else:
+            return np.concatenate(y_pred), np.concatenate(y_true)
+        
+    def test(self):
+        if self.local_rank == 0:
+            checkpoint = torch.load(self.model_path)
+            epoch = checkpoint['epoch']
+            val_loss = checkpoint['val_loss']
+            
+            if isinstance(self.net, DDP):
+                self.net.module.load_state_dict(checkpoint['state_dict'])
+            else:
+                self.net.load_state_dict(checkpoint['state_dict'])
+                
+            print('load model at epoch {}, with val loss: {:.3f}'.format(epoch, val_loss))
+            
+        # Synchronize all processes to ensure the model is loaded
+        if self.world_size > 1:
+            dist.barrier()
+            
+        y_pred, y_true = self.test_iterate(epoch, 'test')
+        
+        if self.local_rank == 0:
+            print(y_pred.shape, y_true.shape)
+            print('total', accuracy_score(y_true, y_pred))
+            for i in range(self.n_classes):
+                idx = y_true == i
+                if np.sum(idx) > 0:  # Only compute accuracy if there are samples
+                    print('class', i, accuracy_score(y_true[idx], y_pred[idx]))
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+
+
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+            def __init__(self, dataset, k_classes, n_samples,
+                         world_size=1, rank=0, seed=42):
+                """
+                Class-balanced batch sampler for distributed training.
+                
+                Args:
+                    dataset: Dataset to sample from
+                    k_classes: Number of classes per batch
+                    n_samples: Number of samples per class
+                    world_size: Number of processes (GPUs)
+                    rank: Local rank of this process
+                    seed: Random seed
+                """
+                super().__init__(dataset)
+                self.dataset = dataset
+                self.k_classes = k_classes
+                self.n_samples = n_samples
+                self.world_size = world_size
+                self.rank = rank
+                self.seed = seed
+                self.epoch = 0  # must be set each epoch manually!
+        
+                # Build mapping from class to list of indices
+                if isinstance(dataset, torch.utils.data.Subset):
+                    targets = [dataset.dataset.targets[i] for i in dataset.indices]
+                else:
+                    targets = dataset.targets
+                
+                self.class_to_indices = {}
+                for idx, target in enumerate(targets):
+                    if target not in self.class_to_indices:
+                        self.class_to_indices[target] = []
+                    self.class_to_indices[target].append(idx)
+        
+                # Only keep classes that have enough samples
+                self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                          if len(idxs) >= n_samples]
+                
+                assert len(self.available_classes) >= k_classes, \
+                    f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+        
+                # Compute approximately how many batches can fit
+                total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+                batch_size = self.k_classes * self.n_samples
+                self.batches_per_epoch = total_samples // batch_size
+        
+            def set_epoch(self, epoch):
+                self.epoch = epoch
+        
+            def __iter__(self):
+                g = torch.Generator()
+                g.manual_seed(self.seed + self.epoch + self.rank)
+    
+                num_batches = 0
+                while num_batches < self.batches_per_epoch:
+                    selected_classes = torch.tensor(self.available_classes)
+                    selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+                
+                    batch = []
+                    for cls in selected_classes.tolist():
+                        indices = self.class_to_indices[cls]
+                        indices_tensor = torch.tensor(indices)
+                        chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                        batch.extend(chosen_indices.tolist())
+                
+                    # Shard based on rank
+                    if num_batches % self.world_size == self.rank:
+                        yield batch
+                
+                    num_batches += 1
+    
+        
+                # all_batches = []
+        
+                # while len(all_batches) < self.batches_per_epoch:
+                #     # Pick k_classes randomly
+                #     selected_classes = torch.tensor(self.available_classes)
+                #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+        
+                #     batch = []
+                #     for cls in selected_classes.tolist():
+                #         indices = self.class_to_indices[cls]
+                #         indices_tensor = torch.tensor(indices)
+                #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                #         batch.extend(chosen_indices.tolist())
+        
+                #     all_batches.append(batch)
+        
+                # # Shard batches across GPUs
+                # local_batches = all_batches[self.rank::self.world_size]
+        
+                # for batch in local_batches:
+                #     yield batch
+        
+            def __len__(self):
+                return self.batches_per_epoch // self.world_size
+
+
+
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+        )
+    
+    # Set seed for reproducibility
+    torch.manual_seed(config['seed'])
+    
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+
+    # Create solver with distributed info
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args,
+        local_rank=rank,
+        world_size=world_size
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    from pathlib import Path
+
+    home = Path('/data')
+    
+    config = {
+        'wandb_project': "DELETEME",  # "DeepLDA",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,
+        'num_workers': 1,  
+        'train_dir': str(home / 'datasets/imagenet_full_size/061417/train'),
+        'val_dir': str(home / 'datasets/imagenet_full_size/061417/val'),
+        'test_dir': str(home / 'datasets/imagenet_full_size/061417/test'),
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 100,
+        'k_classes': 30, 
+        'n_samples': 100, 
+    }
+
+    
+    # Number of available GPUs
+    n_gpus = 4
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_040930-ay6bukuh/files/diff.patch b/wandb/run-20250430_040930-ay6bukuh/files/diff.patch
new file mode 100644
index 0000000..a541826
--- /dev/null
+++ b/wandb/run-20250430_040930-ay6bukuh/files/diff.patch
@@ -0,0 +1,278 @@
+diff --git a/Untitled.ipynb b/Untitled.ipynb
+deleted file mode 100644
+index 363fcab..0000000
+--- a/Untitled.ipynb
++++ /dev/null
+@@ -1,6 +0,0 @@
+-{
+- "cells": [],
+- "metadata": {},
+- "nbformat": 4,
+- "nbformat_minor": 5
+-}
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..e14d5f8 100644
+--- a/lda.py
++++ b/lda.py
+@@ -165,9 +165,6 @@ class LDA(nn.Module):
+         self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        X = X.view(X.shape[0], -1).detach()
+-        y = y.detach()
+-
+         # Initialize or update running stats
+         if self.running_stats is None:
+             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+@@ -246,8 +243,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..cc1eccc 100644
+--- a/train.py
++++ b/train.py
+@@ -462,85 +462,103 @@ def cleanup():
+ 
+ 
+ def train_worker(rank, world_size, config):
+-    
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
+-            """
+-            Class-balanced batch sampler for distributed training.
+-            
+-            Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
+-            """
+-            super().__init__(dataset)
+-            self.dataset = dataset
+-            self.k_classes = k_classes
+-            self.n_samples = n_samples
+-            self.world_size = world_size
+-            self.rank = rank
+-            self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
+-    
+-            # Build mapping from class to list of indices
+-            if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+-            else:
+-                targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+-    
+-            # Only keep classes that have enough samples
+-            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+-                                      if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+-    
+-            # Compute approximately how many batches can fit
+-            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
+-            self.batches_per_epoch = total_samples // batch_size
+-    
+-        def set_epoch(self, epoch):
+-            self.epoch = epoch
+-    
+-        def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-    
+-            all_batches = []
+-    
+-            while len(all_batches) < self.batches_per_epoch:
+-                # Pick k_classes randomly
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-    
+-                all_batches.append(batch)
+-    
+-            # Shard batches across GPUs
+-            local_batches = all_batches[self.rank::self.world_size]
+-    
+-            for batch in local_batches:
+-                yield batch
++            def __init__(self, dataset, k_classes, n_samples,
++                         world_size=1, rank=0, seed=42):
++                """
++                Class-balanced batch sampler for distributed training.
++                
++                Args:
++                    dataset: Dataset to sample from
++                    k_classes: Number of classes per batch
++                    n_samples: Number of samples per class
++                    world_size: Number of processes (GPUs)
++                    rank: Local rank of this process
++                    seed: Random seed
++                """
++                super().__init__(dataset)
++                self.dataset = dataset
++                self.k_classes = k_classes
++                self.n_samples = n_samples
++                self.world_size = world_size
++                self.rank = rank
++                self.seed = seed
++                self.epoch = 0  # must be set each epoch manually!
++        
++                # Build mapping from class to list of indices
++                if isinstance(dataset, torch.utils.data.Subset):
++                    targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                else:
++                    targets = dataset.targets
++                
++                self.class_to_indices = {}
++                for idx, target in enumerate(targets):
++                    if target not in self.class_to_indices:
++                        self.class_to_indices[target] = []
++                    self.class_to_indices[target].append(idx)
++        
++                # Only keep classes that have enough samples
++                self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
++                                          if len(idxs) >= n_samples]
++                
++                assert len(self.available_classes) >= k_classes, \
++                    f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++        
++                # Compute approximately how many batches can fit
++                total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
++                batch_size = self.k_classes * self.n_samples
++                self.batches_per_epoch = total_samples // batch_size
++        
++            def set_epoch(self, epoch):
++                self.epoch = epoch
++        
++            def __iter__(self):
++                g = torch.Generator()
++                g.manual_seed(self.seed + self.epoch + self.rank)
++    
++                num_batches = 0
++                while num_batches < self.batches_per_epoch:
++                    selected_classes = torch.tensor(self.available_classes)
++                    selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++                
++                    batch = []
++                    for cls in selected_classes.tolist():
++                        indices = self.class_to_indices[cls]
++                        indices_tensor = torch.tensor(indices)
++                        chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
++                        batch.extend(chosen_indices.tolist())
++                
++                    # Shard based on rank
++                    if num_batches % self.world_size == self.rank:
++                        yield batch
++                
++                    num_batches += 1
+     
+-        def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++        
++                # all_batches = []
++        
++                # while len(all_batches) < self.batches_per_epoch:
++                #     # Pick k_classes randomly
++                #     selected_classes = torch.tensor(self.available_classes)
++                #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++        
++                #     batch = []
++                #     for cls in selected_classes.tolist():
++                #         indices = self.class_to_indices[cls]
++                #         indices_tensor = torch.tensor(indices)
++                #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
++                #         batch.extend(chosen_indices.tolist())
++        
++                #     all_batches.append(batch)
++        
++                # # Shard batches across GPUs
++                # local_batches = all_batches[self.rank::self.world_size]
++        
++                # for batch in local_batches:
++                #     yield batch
++        
++            def __len__(self):
++                return self.batches_per_epoch // self.world_size
+ 
+ 
+ 
+@@ -673,13 +691,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..30735cf 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250430_040930-ay6bukuh
+\ No newline at end of file
diff --git a/wandb/run-20250430_040930-ay6bukuh/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch b/wandb/run-20250430_040930-ay6bukuh/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
new file mode 100644
index 0000000..a541826
--- /dev/null
+++ b/wandb/run-20250430_040930-ay6bukuh/files/diff_5a2aa782fc73431ac493eb9545615a928b1ada59.patch
@@ -0,0 +1,278 @@
+diff --git a/Untitled.ipynb b/Untitled.ipynb
+deleted file mode 100644
+index 363fcab..0000000
+--- a/Untitled.ipynb
++++ /dev/null
+@@ -1,6 +0,0 @@
+-{
+- "cells": [],
+- "metadata": {},
+- "nbformat": 4,
+- "nbformat_minor": 5
+-}
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index 5b8bae9..e14d5f8 100644
+--- a/lda.py
++++ b/lda.py
+@@ -165,9 +165,6 @@ class LDA(nn.Module):
+         self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        X = X.view(X.shape[0], -1).detach()
+-        y = y.detach()
+-
+         # Initialize or update running stats
+         if self.running_stats is None:
+             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+@@ -246,8 +243,8 @@ class RunningLDAStats:
+ 
+     @torch.no_grad()
+     def update(self, X, y):
+-        X = X.view(X.shape[0], -1).cpu()
+-        y = y.cpu()
++        X = X.view(X.shape[0], -1).detach().to('cpu')
++        y = y.detach().to('cpu')
+ 
+         for cls in range(self.n_classes):
+             mask = (y == cls)
+diff --git a/train.py b/train.py
+index 146c137..cc1eccc 100644
+--- a/train.py
++++ b/train.py
+@@ -462,85 +462,103 @@ def cleanup():
+ 
+ 
+ def train_worker(rank, world_size, config):
+-    
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
+-            """
+-            Class-balanced batch sampler for distributed training.
+-            
+-            Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
+-            """
+-            super().__init__(dataset)
+-            self.dataset = dataset
+-            self.k_classes = k_classes
+-            self.n_samples = n_samples
+-            self.world_size = world_size
+-            self.rank = rank
+-            self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
+-    
+-            # Build mapping from class to list of indices
+-            if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+-            else:
+-                targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+-    
+-            # Only keep classes that have enough samples
+-            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+-                                      if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+-    
+-            # Compute approximately how many batches can fit
+-            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
+-            self.batches_per_epoch = total_samples // batch_size
+-    
+-        def set_epoch(self, epoch):
+-            self.epoch = epoch
+-    
+-        def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-    
+-            all_batches = []
+-    
+-            while len(all_batches) < self.batches_per_epoch:
+-                # Pick k_classes randomly
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-    
+-                all_batches.append(batch)
+-    
+-            # Shard batches across GPUs
+-            local_batches = all_batches[self.rank::self.world_size]
+-    
+-            for batch in local_batches:
+-                yield batch
++            def __init__(self, dataset, k_classes, n_samples,
++                         world_size=1, rank=0, seed=42):
++                """
++                Class-balanced batch sampler for distributed training.
++                
++                Args:
++                    dataset: Dataset to sample from
++                    k_classes: Number of classes per batch
++                    n_samples: Number of samples per class
++                    world_size: Number of processes (GPUs)
++                    rank: Local rank of this process
++                    seed: Random seed
++                """
++                super().__init__(dataset)
++                self.dataset = dataset
++                self.k_classes = k_classes
++                self.n_samples = n_samples
++                self.world_size = world_size
++                self.rank = rank
++                self.seed = seed
++                self.epoch = 0  # must be set each epoch manually!
++        
++                # Build mapping from class to list of indices
++                if isinstance(dataset, torch.utils.data.Subset):
++                    targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                else:
++                    targets = dataset.targets
++                
++                self.class_to_indices = {}
++                for idx, target in enumerate(targets):
++                    if target not in self.class_to_indices:
++                        self.class_to_indices[target] = []
++                    self.class_to_indices[target].append(idx)
++        
++                # Only keep classes that have enough samples
++                self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
++                                          if len(idxs) >= n_samples]
++                
++                assert len(self.available_classes) >= k_classes, \
++                    f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++        
++                # Compute approximately how many batches can fit
++                total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
++                batch_size = self.k_classes * self.n_samples
++                self.batches_per_epoch = total_samples // batch_size
++        
++            def set_epoch(self, epoch):
++                self.epoch = epoch
++        
++            def __iter__(self):
++                g = torch.Generator()
++                g.manual_seed(self.seed + self.epoch + self.rank)
++    
++                num_batches = 0
++                while num_batches < self.batches_per_epoch:
++                    selected_classes = torch.tensor(self.available_classes)
++                    selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++                
++                    batch = []
++                    for cls in selected_classes.tolist():
++                        indices = self.class_to_indices[cls]
++                        indices_tensor = torch.tensor(indices)
++                        chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
++                        batch.extend(chosen_indices.tolist())
++                
++                    # Shard based on rank
++                    if num_batches % self.world_size == self.rank:
++                        yield batch
++                
++                    num_batches += 1
+     
+-        def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++        
++                # all_batches = []
++        
++                # while len(all_batches) < self.batches_per_epoch:
++                #     # Pick k_classes randomly
++                #     selected_classes = torch.tensor(self.available_classes)
++                #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++        
++                #     batch = []
++                #     for cls in selected_classes.tolist():
++                #         indices = self.class_to_indices[cls]
++                #         indices_tensor = torch.tensor(indices)
++                #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
++                #         batch.extend(chosen_indices.tolist())
++        
++                #     all_batches.append(batch)
++        
++                # # Shard batches across GPUs
++                # local_batches = all_batches[self.rank::self.world_size]
++        
++                # for batch in local_batches:
++                #     yield batch
++        
++            def __len__(self):
++                return self.batches_per_epoch // self.world_size
+ 
+ 
+ 
+@@ -673,13 +691,13 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 100,
+-        'k_classes': 64, 
+-        'n_samples': 64, 
++        'k_classes': 30, 
++        'n_samples': 100, 
+     }
+ 
+     
+     # Number of available GPUs
+-    n_gpus = 8
++    n_gpus = 4
+     
+     # Launch processes
+     mp.spawn(
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 538ff58..30735cf 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250429_053315-pm5qboq7
+\ No newline at end of file
++run-20250430_040930-ay6bukuh
+\ No newline at end of file
diff --git a/wandb/run-20250430_040930-ay6bukuh/files/requirements.txt b/wandb/run-20250430_040930-ay6bukuh/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_040930-ay6bukuh/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_040930-ay6bukuh/files/wandb-metadata.json b/wandb/run-20250430_040930-ay6bukuh/files/wandb-metadata.json
new file mode 100644
index 0000000..187b2c1
--- /dev/null
+++ b/wandb/run-20250430_040930-ay6bukuh/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T04:09:30.676709Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "5a2aa782fc73431ac493eb9545615a928b1ada59"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2031072690176"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_040930-ay6bukuh/run-ay6bukuh.wandb b/wandb/run-20250430_040930-ay6bukuh/run-ay6bukuh.wandb
new file mode 100644
index 0000000..1c4ab9a
Binary files /dev/null and b/wandb/run-20250430_040930-ay6bukuh/run-ay6bukuh.wandb differ
diff --git a/wandb/run-20250430_153540-v36647i9/files/code/train.py b/wandb/run-20250430_153540-v36647i9/files/code/train.py
new file mode 100644
index 0000000..8d0454d
--- /dev/null
+++ b/wandb/run-20250430_153540-v36647i9/files/code/train.py
@@ -0,0 +1,544 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss  # Assuming this is defined elsewhere
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        # Clear CUDA cache before each epoch
+        torch.cuda.empty_cache()
+        gc.collect()
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            # Move data to device
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+            
+            # For training with gradient accumulation
+            if phase == 'train':
+               
+                self.optimizer.zero_grad(set_to_none=True)
+                
+                # Apply mixed precision for training
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            # Stats calculation (same as original)
+                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+                            entropy_sum += metrics["entropy"]
+                            entropy_count += 1
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                            
+                            # Only log on rank 0 for efficiency
+                            if phase == 'train' and self.local_rank == 0:
+                                wandb.log(metrics, commit=False)
+                                wandb.log({
+                                    'loss': loss.item(),
+                                    'epoch': epoch,
+                                }, commit=False)
+                        else:
+                            if self.local_rank == 0:
+                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+                
+                # Scale loss for gradient accumulation
+                #loss = loss / self.gradient_accumulation_steps
+                
+                if phase == 'train':
+                    # Use gradient scaler for mixed precision
+                    self.scaler.scale(loss).backward()
+                    
+                    # Step optimizer at effective batch boundaries
+                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+                    # Unscale before clipping
+                    self.scaler.unscale_(self.optimizer)
+                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                    
+                    # Update with scaler
+                    self.scaler.step(self.optimizer)
+                    self.scaler.update()
+                    
+                    if self.local_rank == 0:
+                        wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                # Validation phase - no gradients needed
+                with torch.no_grad():
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                        else:
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+            
+            # Accumulate metrics
+            total_loss += loss.item()  if phase == 'train' else loss.item()
+            
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+            
+            # Free memory after each batch
+            del inputs, targets, outputs
+            if phase == 'train' and self.use_lda and not hasComplexEVal:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        # Log metrics
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+            wandb.log({
+                f"epoch_{phase}": epoch,
+                f"loss_{phase}": total_loss,
+                f"acc_{phase}": 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+            
+            # Training phase
+            self.iterate(epoch, 'train')
+            
+            # Validation phase
+            with torch.no_grad():
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+                    self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+                    self.net.module.lda.reset_running_stats()
+                
+                
+            # Save best model
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+    
+            # all_batches = []
+    
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
+    
+            #     all_batches.append(batch)
+    
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
+    
+            # for batch in local_batches:
+            #     yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+            
+    # Configure CUDA
+    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_153540-v36647i9/files/diff.patch b/wandb/run-20250430_153540-v36647i9/files/diff.patch
new file mode 100644
index 0000000..b707a4e
--- /dev/null
+++ b/wandb/run-20250430_153540-v36647i9/files/diff.patch
@@ -0,0 +1,54 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index d179128..8d0454d 100644
+--- a/train.py
++++ b/train.py
+@@ -520,8 +520,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..88023e0 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_153540-v36647i9
+\ No newline at end of file
diff --git a/wandb/run-20250430_153540-v36647i9/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_153540-v36647i9/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..b707a4e
--- /dev/null
+++ b/wandb/run-20250430_153540-v36647i9/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,54 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index d179128..8d0454d 100644
+--- a/train.py
++++ b/train.py
+@@ -520,8 +520,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..88023e0 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_153540-v36647i9
+\ No newline at end of file
diff --git a/wandb/run-20250430_153540-v36647i9/files/requirements.txt b/wandb/run-20250430_153540-v36647i9/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_153540-v36647i9/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_153540-v36647i9/files/wandb-metadata.json b/wandb/run-20250430_153540-v36647i9/files/wandb-metadata.json
new file mode 100644
index 0000000..f622469
--- /dev/null
+++ b/wandb/run-20250430_153540-v36647i9/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T15:35:40.503235Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2031173201920"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_153540-v36647i9/run-v36647i9.wandb b/wandb/run-20250430_153540-v36647i9/run-v36647i9.wandb
new file mode 100644
index 0000000..ac1d14e
Binary files /dev/null and b/wandb/run-20250430_153540-v36647i9/run-v36647i9.wandb differ
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/code/train.py b/wandb/run-20250430_155259-g11cpr5x/files/code/train.py
new file mode 100644
index 0000000..8d0454d
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/code/train.py
@@ -0,0 +1,544 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss  # Assuming this is defined elsewhere
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        # Clear CUDA cache before each epoch
+        torch.cuda.empty_cache()
+        gc.collect()
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            # Move data to device
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+            
+            # For training with gradient accumulation
+            if phase == 'train':
+               
+                self.optimizer.zero_grad(set_to_none=True)
+                
+                # Apply mixed precision for training
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            # Stats calculation (same as original)
+                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+                            entropy_sum += metrics["entropy"]
+                            entropy_count += 1
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                            
+                            # Only log on rank 0 for efficiency
+                            if phase == 'train' and self.local_rank == 0:
+                                wandb.log(metrics, commit=False)
+                                wandb.log({
+                                    'loss': loss.item(),
+                                    'epoch': epoch,
+                                }, commit=False)
+                        else:
+                            if self.local_rank == 0:
+                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+                
+                # Scale loss for gradient accumulation
+                #loss = loss / self.gradient_accumulation_steps
+                
+                if phase == 'train':
+                    # Use gradient scaler for mixed precision
+                    self.scaler.scale(loss).backward()
+                    
+                    # Step optimizer at effective batch boundaries
+                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+                    # Unscale before clipping
+                    self.scaler.unscale_(self.optimizer)
+                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                    
+                    # Update with scaler
+                    self.scaler.step(self.optimizer)
+                    self.scaler.update()
+                    
+                    if self.local_rank == 0:
+                        wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                # Validation phase - no gradients needed
+                with torch.no_grad():
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                        else:
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+            
+            # Accumulate metrics
+            total_loss += loss.item()  if phase == 'train' else loss.item()
+            
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+            
+            # Free memory after each batch
+            del inputs, targets, outputs
+            if phase == 'train' and self.use_lda and not hasComplexEVal:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        # Log metrics
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+            wandb.log({
+                f"epoch_{phase}": epoch,
+                f"loss_{phase}": total_loss,
+                f"acc_{phase}": 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+            
+            # Training phase
+            self.iterate(epoch, 'train')
+            
+            # Validation phase
+            with torch.no_grad():
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+                    self.net.module.lda.finalize_running_stats()
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+                    self.net.module.lda.reset_running_stats()
+                
+                
+            # Save best model
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+    
+            # all_batches = []
+    
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
+    
+            #     all_batches.append(batch)
+    
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
+    
+            # for batch in local_batches:
+            #     yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+            
+    # Configure CUDA
+    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/config.yaml b/wandb/run-20250430_155259-g11cpr5x/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/diff.patch b/wandb/run-20250430_155259-g11cpr5x/files/diff.patch
new file mode 100644
index 0000000..b2920ec
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/diff.patch
@@ -0,0 +1,54 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index d179128..8d0454d 100644
+--- a/train.py
++++ b/train.py
+@@ -520,8 +520,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..11c1c64 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_155259-g11cpr5x
+\ No newline at end of file
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_155259-g11cpr5x/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..b2920ec
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,54 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/train.py b/train.py
+index d179128..8d0454d 100644
+--- a/train.py
++++ b/train.py
+@@ -520,8 +520,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..11c1c64 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_155259-g11cpr5x
+\ No newline at end of file
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/requirements.txt b/wandb/run-20250430_155259-g11cpr5x/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/wandb-metadata.json b/wandb/run-20250430_155259-g11cpr5x/files/wandb-metadata.json
new file mode 100644
index 0000000..82c2b6d
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T15:52:59.060687Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2031173726208"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_155259-g11cpr5x/files/wandb-summary.json b/wandb/run-20250430_155259-g11cpr5x/files/wandb-summary.json
new file mode 100644
index 0000000..ec24620
--- /dev/null
+++ b/wandb/run-20250430_155259-g11cpr5x/files/wandb-summary.json
@@ -0,0 +1 @@
+{"loss":16380.9423828125,"entropy":1.8652870655059814,"trace_sigma":1.1717785596847534,"quantile_25":-2.5033386918948963e-05,"loss_val":16380.8896484375,"max_eigval_norm":0.42167022824287415,"quantile_75":3.0840790714137256e-05,"sum_squared_off_diag":0.32671570777893066,"condition_sigma":3.03851325e+06,"_timestamp":1.7460404132765458e+09,"_step":252,"acc_train":12.879283715647164,"epoch_train":5,"_runtime":12034.216252006,"quantile_50":2.2604831428907346e-06,"rank_sigma":273,"min_eigval_norm":-6.703336111968383e-05,"acc_val":6.6739999999999995,"grad_norm":11.907278060913086,"epoch":6,"epoch_val":5,"loss_train":16039.972222222223,"_wandb":{"runtime":12046},"diag_var":1.4549927982443478e-06}
\ No newline at end of file
diff --git a/wandb/run-20250430_155259-g11cpr5x/run-g11cpr5x.wandb b/wandb/run-20250430_155259-g11cpr5x/run-g11cpr5x.wandb
new file mode 100644
index 0000000..caf381c
Binary files /dev/null and b/wandb/run-20250430_155259-g11cpr5x/run-g11cpr5x.wandb differ
diff --git a/wandb/run-20250430_191426-3f58mphr/files/code/train.py b/wandb/run-20250430_191426-3f58mphr/files/code/train.py
new file mode 100644
index 0000000..3846a81
--- /dev/null
+++ b/wandb/run-20250430_191426-3f58mphr/files/code/train.py
@@ -0,0 +1,557 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss  # Assuming this is defined elsewhere
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def iterate(self, epoch, phase):
+        if isinstance(self.net, DDP):
+            self.net.module.train(phase == 'train')
+        else:
+            self.net.train(phase == 'train')
+            
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+
+        # Clear CUDA cache before each epoch
+        torch.cuda.empty_cache()
+        gc.collect()
+        
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            # Move data to device
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+            
+            # For training with gradient accumulation
+            if phase == 'train':
+               
+                self.optimizer.zero_grad(set_to_none=True)
+                
+                # Apply mixed precision for training
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            # Stats calculation (same as original)
+                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+                            entropy_sum += metrics["entropy"]
+                            entropy_count += 1
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                            
+                            # Only log on rank 0 for efficiency
+                            if phase == 'train' and self.local_rank == 0:
+                                wandb.log(metrics, commit=False)
+                                wandb.log({
+                                    'loss': loss.item(),
+                                    'epoch': epoch,
+                                }, commit=False)
+                        else:
+                            if self.local_rank == 0:
+                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+                
+                # Scale loss for gradient accumulation
+                #loss = loss / self.gradient_accumulation_steps
+                
+                if phase == 'train':
+                    # Use gradient scaler for mixed precision
+                    self.scaler.scale(loss).backward()
+                    
+                    # Step optimizer at effective batch boundaries
+                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+                    # Unscale before clipping
+                    self.scaler.unscale_(self.optimizer)
+                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                    
+                    # Update with scaler
+                    self.scaler.step(self.optimizer)
+                    self.scaler.update()
+                    
+                    if self.local_rank == 0:
+                        wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                # Validation phase - no gradients needed
+                with torch.no_grad():
+                    if self.use_lda:
+                        if isinstance(self.net, DDP):
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+                        else:
+                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+                        
+                        if not hasComplexEVal:
+                            loss = self.criterion(sigma_w_inv_b)
+                            
+                            if isinstance(self.net, DDP):
+                                outputs = self.net.module.lda.predict_proba(feas)
+                            else:
+                                outputs = self.net.lda.predict_proba(feas)
+                        else:
+                            continue
+                    else:
+                        outputs = self.net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+            
+            # Accumulate metrics
+            total_loss += loss.item()  if phase == 'train' else loss.item()
+            
+            outputs = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += outputs.eq(targets).sum().item()
+            
+            # Free memory after each batch
+            del inputs, targets, outputs
+            if phase == 'train' and self.use_lda and not hasComplexEVal:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+        
+        # Sync metrics across GPUs
+        if self.world_size > 1:
+            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+            total_loss, correct, total = metrics.tolist()
+            
+        total_loss /= (batch_idx + 1) * self.world_size
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        # Log metrics
+        if self.local_rank == 0:
+            if entropy_count > 0:
+                average_entropy = entropy_sum / entropy_count
+                print(f'Average Entropy: {average_entropy:.4f}')
+            
+            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+            wandb.log({
+                f"epoch_{phase}": epoch,
+                f"loss_{phase}": total_loss,
+                f"acc_{phase}": 100.*total_acc
+            }) 
+        return total_loss, total_acc
+
+
+    def train(self, epochs):
+        best_loss = float('inf')
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+                        self.dataloaders[phase].sampler.set_epoch(epoch)
+            
+            # Training phase
+            self.iterate(epoch, 'train')
+            
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+                
+                # Whole forward and validation accuracy
+                if self.local_rank == 0:
+                    lda_accuracy = run_lda_on_embeddings(
+                        self.dataloaders['complete_train'], 
+                        self.dataloaders['val'], 
+                        self.net.module
+                    )
+                    wand.log({'lda_accuracy':lda_accuracy})
+               
+            # Save best model
+            if val_loss < best_loss and self.local_rank == 0:
+                best_loss = val_loss
+                if isinstance(self.net, DDP):
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+                else:
+                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+                print('best val loss found')
+                torch.save(checkpoint, self.model_path)
+            
+            if self.local_rank == 0:
+                print()
+        
+        # Final save on main process
+        if self.local_rank == 0:
+            if isinstance(self.net, DDP):
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+            else:
+                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+    
+            # all_batches = []
+    
+            # while len(all_batches) < self.batches_per_epoch:
+            #     # Pick k_classes randomly
+            #     selected_classes = torch.tensor(self.available_classes)
+            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+    
+            #     batch = []
+            #     for cls in selected_classes.tolist():
+            #         indices = self.class_to_indices[cls]
+            #         indices_tensor = torch.tensor(indices)
+            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+            #         batch.extend(chosen_indices.tolist())
+    
+            #     all_batches.append(batch)
+    
+            # # Shard batches across GPUs
+            # local_batches = all_batches[self.rank::self.world_size]
+    
+            # for batch in local_batches:
+            #     yield batch
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size // 10
+            
+    # Configure CUDA
+    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset,
+        batch_size=4096,
+        num_workers=1,
+        pin_memory=True
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_191426-3f58mphr/files/diff.patch b/wandb/run-20250430_191426-3f58mphr/files/diff.patch
new file mode 100644
index 0000000..b347ae0
--- /dev/null
+++ b/wandb/run-20250430_191426-3f58mphr/files/diff.patch
@@ -0,0 +1,180 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..3846a81 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -228,13 +229,17 @@ class Solver:
+             
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+                 
++                # Whole forward and validation accuracy
++                if self.local_rank == 0:
++                    lda_accuracy = run_lda_on_embeddings(
++                        self.dataloaders['complete_train'], 
++                        self.dataloaders['val'], 
++                        self.net.module
++                    )
++                    wand.log({'lda_accuracy':lda_accuracy})
++               
+             # Save best model
+             if val_loss < best_loss and self.local_rank == 0:
+                 best_loss = val_loss
+@@ -364,7 +369,7 @@ def train_worker(rank, world_size, config):
+             #     yield batch
+     
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // self.world_size // 10
+             
+     # Configure CUDA
+     #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+@@ -442,6 +447,7 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +474,15 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset,
++        batch_size=4096,
++        num_workers=1,
++        pin_memory=True
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +533,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..34f448a 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_191426-3f58mphr
+\ No newline at end of file
diff --git a/wandb/run-20250430_191426-3f58mphr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_191426-3f58mphr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..b347ae0
--- /dev/null
+++ b/wandb/run-20250430_191426-3f58mphr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,180 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..3846a81 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -228,13 +229,17 @@ class Solver:
+             
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+                 
++                # Whole forward and validation accuracy
++                if self.local_rank == 0:
++                    lda_accuracy = run_lda_on_embeddings(
++                        self.dataloaders['complete_train'], 
++                        self.dataloaders['val'], 
++                        self.net.module
++                    )
++                    wand.log({'lda_accuracy':lda_accuracy})
++               
+             # Save best model
+             if val_loss < best_loss and self.local_rank == 0:
+                 best_loss = val_loss
+@@ -364,7 +369,7 @@ def train_worker(rank, world_size, config):
+             #     yield batch
+     
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // self.world_size // 10
+             
+     # Configure CUDA
+     #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+@@ -442,6 +447,7 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +474,15 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset,
++        batch_size=4096,
++        num_workers=1,
++        pin_memory=True
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +533,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..34f448a 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_191426-3f58mphr
+\ No newline at end of file
diff --git a/wandb/run-20250430_191426-3f58mphr/files/requirements.txt b/wandb/run-20250430_191426-3f58mphr/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_191426-3f58mphr/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_191426-3f58mphr/files/wandb-metadata.json b/wandb/run-20250430_191426-3f58mphr/files/wandb-metadata.json
new file mode 100644
index 0000000..c14e6ef
--- /dev/null
+++ b/wandb/run-20250430_191426-3f58mphr/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T19:14:26.212804Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2031178964992"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_191426-3f58mphr/run-3f58mphr.wandb b/wandb/run-20250430_191426-3f58mphr/run-3f58mphr.wandb
new file mode 100644
index 0000000..9e2c0ff
Binary files /dev/null and b/wandb/run-20250430_191426-3f58mphr/run-3f58mphr.wandb differ
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/code/train.py b/wandb/run-20250430_204513-n3x48jzv/files/code/train.py
new file mode 100644
index 0000000..7a69dab
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/code/train.py
@@ -0,0 +1,508 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast('cuda', enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net()
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/config.yaml b/wandb/run-20250430_204513-n3x48jzv/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/diff.patch b/wandb/run-20250430_204513-n3x48jzv/files/diff.patch
new file mode 100644
index 0000000..43e17d8
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/diff.patch
@@ -0,0 +1,501 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..7a69dab 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -71,190 +72,167 @@ class Solver:
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast('cuda', enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        valset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..e2f6b71 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_204513-n3x48jzv
+\ No newline at end of file
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_204513-n3x48jzv/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..43e17d8
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,501 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..7a69dab 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -71,190 +72,167 @@ class Solver:
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast('cuda', enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        valset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..e2f6b71 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_204513-n3x48jzv
+\ No newline at end of file
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/requirements.txt b/wandb/run-20250430_204513-n3x48jzv/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/wandb-metadata.json b/wandb/run-20250430_204513-n3x48jzv/files/wandb-metadata.json
new file mode 100644
index 0000000..69a6f11
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T20:45:13.030475Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059626283008"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_204513-n3x48jzv/files/wandb-summary.json b/wandb/run-20250430_204513-n3x48jzv/files/wandb-summary.json
new file mode 100644
index 0000000..bc0d771
--- /dev/null
+++ b/wandb/run-20250430_204513-n3x48jzv/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":78}}
\ No newline at end of file
diff --git a/wandb/run-20250430_204513-n3x48jzv/run-n3x48jzv.wandb b/wandb/run-20250430_204513-n3x48jzv/run-n3x48jzv.wandb
new file mode 100644
index 0000000..adba36a
Binary files /dev/null and b/wandb/run-20250430_204513-n3x48jzv/run-n3x48jzv.wandb differ
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/code/train.py b/wandb/run-20250430_204749-a2iukxs4/files/code/train.py
new file mode 100644
index 0000000..b308867
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/code/train.py
@@ -0,0 +1,508 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net()
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/config.yaml b/wandb/run-20250430_204749-a2iukxs4/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/diff.patch b/wandb/run-20250430_204749-a2iukxs4/files/diff.patch
new file mode 100644
index 0000000..96ee9a6
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/diff.patch
@@ -0,0 +1,500 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..b308867 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -71,190 +72,167 @@ class Solver:
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        valset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..aff989c 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_204749-a2iukxs4
+\ No newline at end of file
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_204749-a2iukxs4/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..96ee9a6
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,500 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..b308867 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -71,190 +72,167 @@ class Solver:
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        valset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..aff989c 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_204749-a2iukxs4
+\ No newline at end of file
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/requirements.txt b/wandb/run-20250430_204749-a2iukxs4/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/wandb-metadata.json b/wandb/run-20250430_204749-a2iukxs4/files/wandb-metadata.json
new file mode 100644
index 0000000..b0b6dc6
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T20:47:49.556846Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059626569728"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_204749-a2iukxs4/files/wandb-summary.json b/wandb/run-20250430_204749-a2iukxs4/files/wandb-summary.json
new file mode 100644
index 0000000..8c39314
--- /dev/null
+++ b/wandb/run-20250430_204749-a2iukxs4/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_runtime":116.9610895,"epoch":0,"_wandb":{"runtime":200},"loss":16383.2333984375,"sum_squared_off_diag":0.00034308992326259613,"loss_val":16383.2119140625,"epoch_val":0,"entropy":1.8857426643371582,"condition_sigma":3.0412688e+08,"quantile_50":-4.9638692871667445e-06,"trace_sigma":0.03740917891263962,"acc_val":4.96826171875,"_timestamp":1.7460461865177064e+09,"rank_sigma":298,"loss_train":16383.654296875,"grad_norm":NaN,"quantile_25":-3.694410406751558e-05,"diag_var":9.687768809385489e-09,"epoch_train":0,"acc_train":5.560302734375,"min_eigval_norm":-0.0026237822603434324,"quantile_75":2.208817022619769e-05,"max_eigval_norm":0.3633494973182678,"_step":2}
\ No newline at end of file
diff --git a/wandb/run-20250430_204749-a2iukxs4/run-a2iukxs4.wandb b/wandb/run-20250430_204749-a2iukxs4/run-a2iukxs4.wandb
new file mode 100644
index 0000000..fa473ef
Binary files /dev/null and b/wandb/run-20250430_204749-a2iukxs4/run-a2iukxs4.wandb differ
diff --git a/wandb/run-20250430_205346-lxv549lf/files/code/train.py b/wandb/run-20250430_205346-lxv549lf/files/code/train.py
new file mode 100644
index 0000000..09d00b2
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/code/train.py
@@ -0,0 +1,508 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(device_type='cuda', enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net()
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_205346-lxv549lf/files/config.yaml b/wandb/run-20250430_205346-lxv549lf/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_205346-lxv549lf/files/diff.patch b/wandb/run-20250430_205346-lxv549lf/files/diff.patch
new file mode 100644
index 0000000..b271c1e
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/diff.patch
@@ -0,0 +1,505 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..09d00b2 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,171 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(device_type='cuda', enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..61e1b31 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_205346-lxv549lf
+\ No newline at end of file
diff --git a/wandb/run-20250430_205346-lxv549lf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_205346-lxv549lf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..b271c1e
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,505 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..09d00b2 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,171 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(device_type='cuda', enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..61e1b31 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_205346-lxv549lf
+\ No newline at end of file
diff --git a/wandb/run-20250430_205346-lxv549lf/files/requirements.txt b/wandb/run-20250430_205346-lxv549lf/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_205346-lxv549lf/files/wandb-metadata.json b/wandb/run-20250430_205346-lxv549lf/files/wandb-metadata.json
new file mode 100644
index 0000000..817f69e
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T20:53:46.285241Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059627098112"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_205346-lxv549lf/files/wandb-summary.json b/wandb/run-20250430_205346-lxv549lf/files/wandb-summary.json
new file mode 100644
index 0000000..9f61df7
--- /dev/null
+++ b/wandb/run-20250430_205346-lxv549lf/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":11}}
\ No newline at end of file
diff --git a/wandb/run-20250430_205346-lxv549lf/run-lxv549lf.wandb b/wandb/run-20250430_205346-lxv549lf/run-lxv549lf.wandb
new file mode 100644
index 0000000..550cbbc
Binary files /dev/null and b/wandb/run-20250430_205346-lxv549lf/run-lxv549lf.wandb differ
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/code/train.py b/wandb/run-20250430_205509-fsvo28zr/files/code/train.py
new file mode 100644
index 0000000..441e99e
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/code/train.py
@@ -0,0 +1,508 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net()
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/config.yaml b/wandb/run-20250430_205509-fsvo28zr/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/diff.patch b/wandb/run-20250430_205509-fsvo28zr/files/diff.patch
new file mode 100644
index 0000000..550d666
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/diff.patch
@@ -0,0 +1,505 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..441e99e 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,171 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f6ca965 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_205509-fsvo28zr
+\ No newline at end of file
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_205509-fsvo28zr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..550d666
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,505 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..441e99e 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,171 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net()
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +318,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -442,6 +396,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +424,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +484,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f6ca965 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_205509-fsvo28zr
+\ No newline at end of file
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/requirements.txt b/wandb/run-20250430_205509-fsvo28zr/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/wandb-metadata.json b/wandb/run-20250430_205509-fsvo28zr/files/wandb-metadata.json
new file mode 100644
index 0000000..245204e
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T20:55:09.501927Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059627257856"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_205509-fsvo28zr/files/wandb-summary.json b/wandb/run-20250430_205509-fsvo28zr/files/wandb-summary.json
new file mode 100644
index 0000000..916d8a9
--- /dev/null
+++ b/wandb/run-20250430_205509-fsvo28zr/files/wandb-summary.json
@@ -0,0 +1 @@
+{"epoch":0,"epoch_val":0,"quantile_50":-4.9638692871667445e-06,"max_eigval_norm":0.3633494973182678,"condition_sigma":3.0412688e+08,"_wandb":{"runtime":1017},"epoch_train":0,"min_eigval_norm":-0.0026237822603434324,"loss":16383.2333984375,"_timestamp":1.7460466252579286e+09,"loss_train":16383.654296875,"sum_squared_off_diag":0.00034308992326259613,"_step":2,"acc_val":4.96826171875,"entropy":1.8857426643371582,"_runtime":115.756344401,"trace_sigma":0.03740917891263962,"loss_val":16383.2119140625,"quantile_75":2.208817022619769e-05,"diag_var":9.687768809385489e-09,"grad_norm":NaN,"quantile_25":-3.694410406751558e-05,"acc_train":5.560302734375,"rank_sigma":298}
\ No newline at end of file
diff --git a/wandb/run-20250430_205509-fsvo28zr/run-fsvo28zr.wandb b/wandb/run-20250430_205509-fsvo28zr/run-fsvo28zr.wandb
new file mode 100644
index 0000000..9ce56b2
Binary files /dev/null and b/wandb/run-20250430_205509-fsvo28zr/run-fsvo28zr.wandb differ
diff --git a/wandb/run-20250430_211558-o6jri0va/files/code/train.py b/wandb/run-20250430_211558-o6jri0va/files/code/train.py
new file mode 100644
index 0000000..287fe57
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/code/train.py
@@ -0,0 +1,523 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    # import torch
+    # from torchvision import datasets
+    # from torch.utils.data import Subset
+    # import random
+    
+    # Load full dataset
+    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    
+    # Compute 1% subset
+    num_samples = int(0.01 * len(full_trainset))
+    indices = random.sample(range(len(full_trainset)), num_samples)
+    
+    # Create subset
+    trainset = Subset(full_trainset, indices)
+
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':64 ,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_211558-o6jri0va/files/config.yaml b/wandb/run-20250430_211558-o6jri0va/files/config.yaml
new file mode 100644
index 0000000..aae3f7a
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_211558-o6jri0va/files/diff.patch b/wandb/run-20250430_211558-o6jri0va/files/diff.patch
new file mode 100644
index 0000000..994a5ec
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/diff.patch
@@ -0,0 +1,529 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..287fe57 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -425,7 +380,21 @@ def train_worker(rank, world_size, config):
+     ])
+ 
+     # Create datasets
+-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    # import torch
++    # from torchvision import datasets
++    # from torch.utils.data import Subset
++    # import random
++    
++    # Load full dataset
++    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    
++    # Compute 1% subset
++    num_samples = int(0.01 * len(full_trainset))
++    indices = random.sample(range(len(full_trainset)), num_samples)
++    
++    # Create subset
++    trainset = Subset(full_trainset, indices)
++
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+ 
+@@ -442,6 +411,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +439,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +499,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..252b619 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_211558-o6jri0va
+\ No newline at end of file
diff --git a/wandb/run-20250430_211558-o6jri0va/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_211558-o6jri0va/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..994a5ec
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,529 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..287fe57 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -425,7 +380,21 @@ def train_worker(rank, world_size, config):
+     ])
+ 
+     # Create datasets
+-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    # import torch
++    # from torchvision import datasets
++    # from torch.utils.data import Subset
++    # import random
++    
++    # Load full dataset
++    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    
++    # Compute 1% subset
++    num_samples = int(0.01 * len(full_trainset))
++    indices = random.sample(range(len(full_trainset)), num_samples)
++    
++    # Create subset
++    trainset = Subset(full_trainset, indices)
++
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+ 
+@@ -442,6 +411,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +439,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +499,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':64 ,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..252b619 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_211558-o6jri0va
+\ No newline at end of file
diff --git a/wandb/run-20250430_211558-o6jri0va/files/requirements.txt b/wandb/run-20250430_211558-o6jri0va/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_211558-o6jri0va/files/wandb-metadata.json b/wandb/run-20250430_211558-o6jri0va/files/wandb-metadata.json
new file mode 100644
index 0000000..fe02e4a
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:15:58.497456Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059628331008"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_211558-o6jri0va/files/wandb-summary.json b/wandb/run-20250430_211558-o6jri0va/files/wandb-summary.json
new file mode 100644
index 0000000..f6a1bdf
--- /dev/null
+++ b/wandb/run-20250430_211558-o6jri0va/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":9}}
\ No newline at end of file
diff --git a/wandb/run-20250430_211558-o6jri0va/run-o6jri0va.wandb b/wandb/run-20250430_211558-o6jri0va/run-o6jri0va.wandb
new file mode 100644
index 0000000..6759f2e
Binary files /dev/null and b/wandb/run-20250430_211558-o6jri0va/run-o6jri0va.wandb differ
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/code/train.py b/wandb/run-20250430_211650-wm4l2uys/files/code/train.py
new file mode 100644
index 0000000..c19294d
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/code/train.py
@@ -0,0 +1,523 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    # Create datasets
+    # import torch
+    # from torchvision import datasets
+    # from torch.utils.data import Subset
+    # import random
+    
+    # Load full dataset
+    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    
+    # Compute 1% subset
+    num_samples = int(0.01 * len(full_trainset))
+    indices = random.sample(range(len(full_trainset)), num_samples)
+    
+    # Create subset
+    trainset = Subset(full_trainset, indices)
+
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 4096,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes':10 ,#64
+        'n_samples': 10,#128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/config.yaml b/wandb/run-20250430_211650-wm4l2uys/files/config.yaml
new file mode 100644
index 0000000..85ce167
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 4096
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 10
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 10
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/diff.patch b/wandb/run-20250430_211650-wm4l2uys/files/diff.patch
new file mode 100644
index 0000000..ec950ed
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/diff.patch
@@ -0,0 +1,529 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..c19294d 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -425,7 +380,21 @@ def train_worker(rank, world_size, config):
+     ])
+ 
+     # Create datasets
+-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    # import torch
++    # from torchvision import datasets
++    # from torch.utils.data import Subset
++    # import random
++    
++    # Load full dataset
++    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    
++    # Compute 1% subset
++    num_samples = int(0.01 * len(full_trainset))
++    indices = random.sample(range(len(full_trainset)), num_samples)
++    
++    # Create subset
++    trainset = Subset(full_trainset, indices)
++
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+ 
+@@ -442,6 +411,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +439,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +499,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':10 ,#64
++        'n_samples': 10,#128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f643a86 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_211650-wm4l2uys
+\ No newline at end of file
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_211650-wm4l2uys/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..ec950ed
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,529 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..2b1bb24 100644
+--- a/lda.py
++++ b/lda.py
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..c19294d 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -425,7 +380,21 @@ def train_worker(rank, world_size, config):
+     ])
+ 
+     # Create datasets
+-    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    # import torch
++    # from torchvision import datasets
++    # from torch.utils.data import Subset
++    # import random
++    
++    # Load full dataset
++    full_trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
++    
++    # Compute 1% subset
++    num_samples = int(0.01 * len(full_trainset))
++    indices = random.sample(range(len(full_trainset)), num_samples)
++    
++    # Create subset
++    trainset = Subset(full_trainset, indices)
++
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+ 
+@@ -442,6 +411,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +439,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -520,8 +499,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes':10 ,#64
++        'n_samples': 10,#128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f643a86 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_211650-wm4l2uys
+\ No newline at end of file
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/requirements.txt b/wandb/run-20250430_211650-wm4l2uys/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/wandb-metadata.json b/wandb/run-20250430_211650-wm4l2uys/files/wandb-metadata.json
new file mode 100644
index 0000000..04d751a
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:16:50.503955Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2059628482560"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_211650-wm4l2uys/files/wandb-summary.json b/wandb/run-20250430_211650-wm4l2uys/files/wandb-summary.json
new file mode 100644
index 0000000..1f5f4d6
--- /dev/null
+++ b/wandb/run-20250430_211650-wm4l2uys/files/wandb-summary.json
@@ -0,0 +1 @@
+{"max_eigval_norm":0.40830719470977783,"_wandb":{"runtime":778},"quantile_50":-5.944227723375661e-06,"epoch_train":4,"_timestamp":1.7460485033592153e+09,"entropy":1.9637094736099243,"epoch":4,"sum_squared_off_diag":7.884326623752713e-05,"quantile_25":-5.7081979321083054e-05,"acc_train":43,"grad_norm":NaN,"loss_val":16383.2568359375,"rank_sigma":320,"acc_val":6.53076171875,"quantile_75":2.9094637284288183e-05,"condition_sigma":9.933233152e+09,"loss_train":16384.28125,"trace_sigma":0.017717719078063965,"lda_accuracy":0.01046,"epoch_val":4,"diag_var":2.2806798671837214e-09,"min_eigval_norm":-0.0028978551272302866,"loss":16383.28515625,"_step":18,"_runtime":692.855532754}
\ No newline at end of file
diff --git a/wandb/run-20250430_211650-wm4l2uys/run-wm4l2uys.wandb b/wandb/run-20250430_211650-wm4l2uys/run-wm4l2uys.wandb
new file mode 100644
index 0000000..154a7c8
Binary files /dev/null and b/wandb/run-20250430_211650-wm4l2uys/run-wm4l2uys.wandb differ
diff --git a/wandb/run-20250430_213408-e9jgiaw8/files/code/train.py b/wandb/run-20250430_213408-e9jgiaw8/files/code/train.py
new file mode 100644
index 0000000..4e22406
--- /dev/null
+++ b/wandb/run-20250430_213408-e9jgiaw8/files/code/train.py
@@ -0,0 +1,509 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 10,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_213408-e9jgiaw8/files/diff.patch b/wandb/run-20250430_213408-e9jgiaw8/files/diff.patch
new file mode 100644
index 0000000..975f584
--- /dev/null
+++ b/wandb/run-20250430_213408-e9jgiaw8/files/diff.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..4e22406 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 10,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..c35c3d5 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213408-e9jgiaw8
+\ No newline at end of file
diff --git a/wandb/run-20250430_213408-e9jgiaw8/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_213408-e9jgiaw8/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..975f584
--- /dev/null
+++ b/wandb/run-20250430_213408-e9jgiaw8/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..4e22406 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 10,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..c35c3d5 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213408-e9jgiaw8
+\ No newline at end of file
diff --git a/wandb/run-20250430_213408-e9jgiaw8/files/requirements.txt b/wandb/run-20250430_213408-e9jgiaw8/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_213408-e9jgiaw8/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_213408-e9jgiaw8/files/wandb-metadata.json b/wandb/run-20250430_213408-e9jgiaw8/files/wandb-metadata.json
new file mode 100644
index 0000000..dff71e8
--- /dev/null
+++ b/wandb/run-20250430_213408-e9jgiaw8/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:34:08.497792Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2064071098368"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_213408-e9jgiaw8/run-e9jgiaw8.wandb b/wandb/run-20250430_213408-e9jgiaw8/run-e9jgiaw8.wandb
new file mode 100644
index 0000000..fdf4320
Binary files /dev/null and b/wandb/run-20250430_213408-e9jgiaw8/run-e9jgiaw8.wandb differ
diff --git a/wandb/run-20250430_213801-spscmici/files/code/train.py b/wandb/run-20250430_213801-spscmici/files/code/train.py
new file mode 100644
index 0000000..6e5d2fd
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/code/train.py
@@ -0,0 +1,509 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 2,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_213801-spscmici/files/config.yaml b/wandb/run-20250430_213801-spscmici/files/config.yaml
new file mode 100644
index 0000000..490b4fe
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 8192
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 2
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_213801-spscmici/files/diff.patch b/wandb/run-20250430_213801-spscmici/files/diff.patch
new file mode 100644
index 0000000..78c34b7
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/diff.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..6e5d2fd 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..228e3dc 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213801-spscmici
+\ No newline at end of file
diff --git a/wandb/run-20250430_213801-spscmici/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_213801-spscmici/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..78c34b7
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..6e5d2fd 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..228e3dc 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213801-spscmici
+\ No newline at end of file
diff --git a/wandb/run-20250430_213801-spscmici/files/requirements.txt b/wandb/run-20250430_213801-spscmici/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_213801-spscmici/files/wandb-metadata.json b/wandb/run-20250430_213801-spscmici/files/wandb-metadata.json
new file mode 100644
index 0000000..e642067
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:38:01.016509Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2332824604672"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_213801-spscmici/files/wandb-summary.json b/wandb/run-20250430_213801-spscmici/files/wandb-summary.json
new file mode 100644
index 0000000..e711797
--- /dev/null
+++ b/wandb/run-20250430_213801-spscmici/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":10}}
\ No newline at end of file
diff --git a/wandb/run-20250430_213801-spscmici/run-spscmici.wandb b/wandb/run-20250430_213801-spscmici/run-spscmici.wandb
new file mode 100644
index 0000000..5a9d73f
Binary files /dev/null and b/wandb/run-20250430_213801-spscmici/run-spscmici.wandb differ
diff --git a/wandb/run-20250430_213949-xtpkw9oh/files/code/train.py b/wandb/run-20250430_213949-xtpkw9oh/files/code/train.py
new file mode 100644
index 0000000..6e5d2fd
--- /dev/null
+++ b/wandb/run-20250430_213949-xtpkw9oh/files/code/train.py
@@ -0,0 +1,509 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 2,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_213949-xtpkw9oh/files/diff.patch b/wandb/run-20250430_213949-xtpkw9oh/files/diff.patch
new file mode 100644
index 0000000..176444f
--- /dev/null
+++ b/wandb/run-20250430_213949-xtpkw9oh/files/diff.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..6e5d2fd 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..357f8f9 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213949-xtpkw9oh
+\ No newline at end of file
diff --git a/wandb/run-20250430_213949-xtpkw9oh/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_213949-xtpkw9oh/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..176444f
--- /dev/null
+++ b/wandb/run-20250430_213949-xtpkw9oh/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,537 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..6e5d2fd 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..357f8f9 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_213949-xtpkw9oh
+\ No newline at end of file
diff --git a/wandb/run-20250430_213949-xtpkw9oh/files/requirements.txt b/wandb/run-20250430_213949-xtpkw9oh/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_213949-xtpkw9oh/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_213949-xtpkw9oh/files/wandb-metadata.json b/wandb/run-20250430_213949-xtpkw9oh/files/wandb-metadata.json
new file mode 100644
index 0000000..4ab1e58
--- /dev/null
+++ b/wandb/run-20250430_213949-xtpkw9oh/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:39:49.943881Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2379463782400"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_213949-xtpkw9oh/run-xtpkw9oh.wandb b/wandb/run-20250430_213949-xtpkw9oh/run-xtpkw9oh.wandb
new file mode 100644
index 0000000..e69de29
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/code/train.py b/wandb/run-20250430_214220-eonsdbmz/files/code/train.py
new file mode 100644
index 0000000..d637b10
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/code/train.py
@@ -0,0 +1,509 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes, n_samples,
+                     world_size=1, rank=0, seed=42):
+            """
+            Class-balanced batch sampler for distributed training.
+            
+            Args:
+                dataset: Dataset to sample from
+                k_classes: Number of classes per batch
+                n_samples: Number of samples per class
+                world_size: Number of processes (GPUs)
+                rank: Local rank of this process
+                seed: Random seed
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # must be set each epoch manually!
+    
+            # Build mapping from class to list of indices
+            if isinstance(dataset, torch.utils.data.Subset):
+                targets = [dataset.dataset.targets[i] for i in dataset.indices]
+            else:
+                targets = dataset.targets
+            
+            self.class_to_indices = {}
+            for idx, target in enumerate(targets):
+                if target not in self.class_to_indices:
+                    self.class_to_indices[target] = []
+                self.class_to_indices[target].append(idx)
+    
+            # Only keep classes that have enough samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            
+            assert len(self.available_classes) >= k_classes, \
+                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+    
+            # Compute approximately how many batches can fit
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = self.k_classes * self.n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch + self.rank)
+
+            num_batches = 0
+            while num_batches < self.batches_per_epoch:
+                selected_classes = torch.tensor(self.available_classes)
+                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+            
+                batch = []
+                for cls in selected_classes.tolist():
+                    indices = self.class_to_indices[cls]
+                    indices_tensor = torch.tensor(indices)
+                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                    batch.extend(chosen_indices.tolist())
+            
+                # Shard based on rank
+                if num_batches % self.world_size == self.rank:
+                    yield batch
+            
+                num_batches += 1
+
+        def __len__(self):
+            return self.batches_per_epoch // (self.world_size * 10)
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/config.yaml b/wandb/run-20250430_214220-eonsdbmz/files/config.yaml
new file mode 100644
index 0000000..19098b7
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 8192
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/diff.patch b/wandb/run-20250430_214220-eonsdbmz/files/diff.patch
new file mode 100644
index 0000000..26184ad
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/diff.patch
@@ -0,0 +1,535 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..d637b10 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +474,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..7510d84 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_214220-eonsdbmz
+\ No newline at end of file
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_214220-eonsdbmz/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..26184ad
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,535 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..d637b10 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -340,34 +319,10 @@ def train_worker(rank, world_size, config):
+             
+                 num_batches += 1
+ 
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-    
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
+-    
+-            #     all_batches.append(batch)
+-    
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
+-    
+-            # for batch in local_batches:
+-            #     yield batch
+-    
+         def __len__(self):
+-            return self.batches_per_epoch // self.world_size
++            return self.batches_per_epoch // (self.world_size * 10)
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +474,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..7510d84 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_214220-eonsdbmz
+\ No newline at end of file
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/requirements.txt b/wandb/run-20250430_214220-eonsdbmz/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/wandb-metadata.json b/wandb/run-20250430_214220-eonsdbmz/files/wandb-metadata.json
new file mode 100644
index 0000000..dcf507d
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:42:20.698751Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2466966679552"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_214220-eonsdbmz/files/wandb-summary.json b/wandb/run-20250430_214220-eonsdbmz/files/wandb-summary.json
new file mode 100644
index 0000000..4259c02
--- /dev/null
+++ b/wandb/run-20250430_214220-eonsdbmz/files/wandb-summary.json
@@ -0,0 +1 @@
+{"diag_var":1.9620365776518156e-08,"loss_val":31.238101959228516,"acc_val":3.4515380859375,"quantile_50":-1.6120302461786196e-06,"trace_sigma":0.05366804450750351,"max_eigval_norm":0.4089950621128082,"min_eigval_norm":-0.0005289154360070825,"epoch":0,"_timestamp":1.7460494892913105e+09,"loss_train":31.653945922851562,"entropy":1.7254283428192139,"quantile_75":1.8443553926772438e-05,"_wandb":{"runtime":521},"acc_train":5.560302734375,"epoch_train":0,"rank_sigma":227,"sum_squared_off_diag":0.0007806429639458656,"epoch_val":0,"_runtime":148.59286183,"grad_norm":NaN,"quantile_25":-2.0531453628791496e-05,"loss":31.24972915649414,"_step":2,"condition_sigma":5.63762112e+08}
\ No newline at end of file
diff --git a/wandb/run-20250430_214220-eonsdbmz/run-eonsdbmz.wandb b/wandb/run-20250430_214220-eonsdbmz/run-eonsdbmz.wandb
new file mode 100644
index 0000000..1e7c198
Binary files /dev/null and b/wandb/run-20250430_214220-eonsdbmz/run-eonsdbmz.wandb differ
diff --git a/wandb/run-20250430_215147-pt6kqu6p/files/code/train.py b/wandb/run-20250430_215147-pt6kqu6p/files/code/train.py
new file mode 100644
index 0000000..3142c6c
--- /dev/null
+++ b/wandb/run-20250430_215147-pt6kqu6p/files/code/train.py
@@ -0,0 +1,509 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes: int, n_samples: int,
+                     world_size: int = 1, rank: int = 0, seed: int = 42):
+            """
+            Class-balanced batch sampler for distributed training.
+    
+            Args:
+                dataset: Dataset to sample from.
+                k_classes: Number of different classes in each batch.
+                n_samples: Number of samples per class.
+                world_size: Total number of distributed workers.
+                rank: Rank of the current worker.
+                seed: Random seed for reproducibility.
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # Set externally before each epoch
+    
+            # Get target labels and build class-to-indices mapping
+            if isinstance(dataset, torch.utils.data.Subset):
+                indices = dataset.indices
+                targets = [dataset.dataset.targets[i] for i in indices]
+            else:
+                indices = range(len(dataset))
+                targets = dataset.targets
+    
+            self.class_to_indices = defaultdict(list)
+            for idx, label in zip(indices, targets):
+                self.class_to_indices[label].append(idx)
+    
+            # Filter out classes with insufficient samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            if len(self.available_classes) < k_classes:
+                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
+                                 f"but only {len(self.available_classes)} are available.")
+    
+            # Estimate batches per epoch
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = k_classes * n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch: int):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            rng = random.Random(self.seed + self.epoch + self.rank)
+            num_batches = 0
+            batch_size = self.k_classes * self.n_samples
+    
+            while num_batches < self.batches_per_epoch:
+                selected_classes = rng.sample(self.available_classes, self.k_classes)
+    
+                batch = np.empty(batch_size, dtype=int)
+                offset = 0
+                for cls in selected_classes:
+                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
+                    batch[offset:offset + self.n_samples] = sampled_indices
+                    offset += self.n_samples
+    
+                # Shard to the correct worker
+                if num_batches % self.world_size == self.rank:
+                    yield batch.tolist()
+    
+                num_batches += 1
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 2,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_215147-pt6kqu6p/files/diff.patch b/wandb/run-20250430_215147-pt6kqu6p/files/diff.patch
new file mode 100644
index 0000000..a18c5cb
--- /dev/null
+++ b/wandb/run-20250430_215147-pt6kqu6p/files/diff.patch
@@ -0,0 +1,647 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..3142c6c 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -269,18 +248,18 @@ def cleanup():
+     
+ def train_worker(rank, world_size, config):
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +268,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..faf40dc 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215147-pt6kqu6p
+\ No newline at end of file
diff --git a/wandb/run-20250430_215147-pt6kqu6p/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_215147-pt6kqu6p/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..a18c5cb
--- /dev/null
+++ b/wandb/run-20250430_215147-pt6kqu6p/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,647 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..3142c6c 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,172 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -269,18 +248,18 @@ def cleanup():
+     
+ def train_worker(rank, world_size, config):
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +268,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +378,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +397,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +425,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,8 +474,8 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
+-        'num_workers': 1,  # Adjust based on CPU cores
++        'batch_size': 8192,  # Global batch size
++        'num_workers': 2,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+         'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+@@ -520,8 +485,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..faf40dc 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215147-pt6kqu6p
+\ No newline at end of file
diff --git a/wandb/run-20250430_215147-pt6kqu6p/files/requirements.txt b/wandb/run-20250430_215147-pt6kqu6p/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_215147-pt6kqu6p/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_215147-pt6kqu6p/files/wandb-metadata.json b/wandb/run-20250430_215147-pt6kqu6p/files/wandb-metadata.json
new file mode 100644
index 0000000..7c14b5b
--- /dev/null
+++ b/wandb/run-20250430_215147-pt6kqu6p/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:51:47.847113Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2466967478272"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_215147-pt6kqu6p/run-pt6kqu6p.wandb b/wandb/run-20250430_215147-pt6kqu6p/run-pt6kqu6p.wandb
new file mode 100644
index 0000000..fa18c98
Binary files /dev/null and b/wandb/run-20250430_215147-pt6kqu6p/run-pt6kqu6p.wandb differ
diff --git a/wandb/run-20250430_215545-go84tkqx/files/code/train.py b/wandb/run-20250430_215545-go84tkqx/files/code/train.py
new file mode 100644
index 0000000..d658be1
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/code/train.py
@@ -0,0 +1,513 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            import time
+            start_time = time.time()
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
+                print(f"Total time: {elapsed_time:.2f} minutes")
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes: int, n_samples: int,
+                     world_size: int = 1, rank: int = 0, seed: int = 42):
+            """
+            Class-balanced batch sampler for distributed training.
+    
+            Args:
+                dataset: Dataset to sample from.
+                k_classes: Number of different classes in each batch.
+                n_samples: Number of samples per class.
+                world_size: Total number of distributed workers.
+                rank: Rank of the current worker.
+                seed: Random seed for reproducibility.
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # Set externally before each epoch
+    
+            # Get target labels and build class-to-indices mapping
+            if isinstance(dataset, torch.utils.data.Subset):
+                indices = dataset.indices
+                targets = [dataset.dataset.targets[i] for i in indices]
+            else:
+                indices = range(len(dataset))
+                targets = dataset.targets
+    
+            self.class_to_indices = defaultdict(list)
+            for idx, label in zip(indices, targets):
+                self.class_to_indices[label].append(idx)
+    
+            # Filter out classes with insufficient samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            if len(self.available_classes) < k_classes:
+                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
+                                 f"but only {len(self.available_classes)} are available.")
+    
+            # Estimate batches per epoch
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = k_classes * n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch: int):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            rng = random.Random(self.seed + self.epoch + self.rank)
+            num_batches = 0
+            batch_size = self.k_classes * self.n_samples
+    
+            while num_batches < self.batches_per_epoch:
+                selected_classes = rng.sample(self.available_classes, self.k_classes)
+    
+                batch = np.empty(batch_size, dtype=int)
+                offset = 0
+                for cls in selected_classes:
+                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
+                    batch[offset:offset + self.n_samples] = sampled_indices
+                    offset += self.n_samples
+    
+                # Shard to the correct worker
+                if num_batches % self.world_size == self.rank:
+                    yield batch.tolist()
+    
+                num_batches += 1
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_215545-go84tkqx/files/config.yaml b/wandb/run-20250430_215545-go84tkqx/files/config.yaml
new file mode 100644
index 0000000..19098b7
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/config.yaml
@@ -0,0 +1,80 @@
+_wandb:
+    value:
+        cli_version: 0.19.9
+        code_path: code/train.py
+        m: []
+        python_version: 3.11.4
+        t:
+            "1":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "2":
+                - 1
+                - 5
+                - 41
+                - 53
+                - 55
+            "3":
+                - 16
+                - 23
+                - 55
+            "4": 3.11.4
+            "5": 0.19.9
+            "8":
+                - 5
+                - 9
+            "12": 0.19.9
+            "13": linux-x86_64
+base_batch_size:
+    value: 128
+base_lr:
+    value: 0.001
+batch_size:
+    value: 8192
+cuda_visible_devices:
+    value: ""
+epochs:
+    value: 20
+gradient_accumulation_steps:
+    value: 1
+k_classes:
+    value: 64
+lamb:
+    value: 0.1
+loss:
+    value: LDA
+margin:
+    value: null
+model_path:
+    value: models/deeplda_best.pth
+n_classes:
+    value: 1000
+n_eig:
+    value: 4
+n_samples:
+    value: 128
+num_workers:
+    value: 1
+seed:
+    value: 42
+test_dir:
+    value: /data/datasets/imagenet_full_size/061417/test
+train_dir:
+    value: /data/datasets/imagenet_full_size/061417/train
+train_val_split:
+    value: 0.1
+use_amp:
+    value: true
+use_checkpointing:
+    value: true
+val_dir:
+    value: /data/datasets/imagenet_full_size/061417/val
+wandb_entity:
+    value: gerardo-pastrana-c3-ai
+wandb_group:
+    value: gapLoss
+wandb_project:
+    value: DELETEME
diff --git a/wandb/run-20250430_215545-go84tkqx/files/diff.patch b/wandb/run-20250430_215545-go84tkqx/files/diff.patch
new file mode 100644
index 0000000..3c66381
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/diff.patch
@@ -0,0 +1,661 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..d658be1 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,176 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            import time
++            start_time = time.time()
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
++                print(f"Total time: {elapsed_time:.2f} minutes")
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -269,18 +252,18 @@ def cleanup():
+     
+ def train_worker(rank, world_size, config):
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +272,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +382,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +401,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +429,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +478,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +489,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/utils.py b/utils.py
+index 2f6d99a..e0ec60e 100644
+--- a/utils.py
++++ b/utils.py
+@@ -48,4 +48,5 @@ def compute_wandb_metrics(outputs, sigma_w_inv_b):
+         "diag_var": diag_var,
+     }
+ 
+-    return metrics
+\ No newline at end of file
++    return metrics
++
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f87ac4a 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215545-go84tkqx
+\ No newline at end of file
diff --git a/wandb/run-20250430_215545-go84tkqx/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_215545-go84tkqx/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..3c66381
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,661 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..d658be1 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,176 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+-                with torch.cuda.amp.autocast(enabled=self.use_amp):
++                with torch.cuda.amp.autocast(device_type="cuda", enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            import time
++            start_time = time.time()
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
++                print(f"Total time: {elapsed_time:.2f} minutes")
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -269,18 +252,18 @@ def cleanup():
+     
+ def train_worker(rank, world_size, config):
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +272,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +382,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +401,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +429,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +478,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +489,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/utils.py b/utils.py
+index 2f6d99a..e0ec60e 100644
+--- a/utils.py
++++ b/utils.py
+@@ -48,4 +48,5 @@ def compute_wandb_metrics(outputs, sigma_w_inv_b):
+         "diag_var": diag_var,
+     }
+ 
+-    return metrics
+\ No newline at end of file
++    return metrics
++
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..f87ac4a 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215545-go84tkqx
+\ No newline at end of file
diff --git a/wandb/run-20250430_215545-go84tkqx/files/requirements.txt b/wandb/run-20250430_215545-go84tkqx/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_215545-go84tkqx/files/wandb-metadata.json b/wandb/run-20250430_215545-go84tkqx/files/wandb-metadata.json
new file mode 100644
index 0000000..206ffaf
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:55:45.061409Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2537034747904"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_215545-go84tkqx/files/wandb-summary.json b/wandb/run-20250430_215545-go84tkqx/files/wandb-summary.json
new file mode 100644
index 0000000..2d57c2e
--- /dev/null
+++ b/wandb/run-20250430_215545-go84tkqx/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb":{"runtime":75}}
\ No newline at end of file
diff --git a/wandb/run-20250430_215545-go84tkqx/run-go84tkqx.wandb b/wandb/run-20250430_215545-go84tkqx/run-go84tkqx.wandb
new file mode 100644
index 0000000..be69d25
Binary files /dev/null and b/wandb/run-20250430_215545-go84tkqx/run-go84tkqx.wandb differ
diff --git a/wandb/run-20250430_215907-4anamnlf/files/code/train.py b/wandb/run-20250430_215907-4anamnlf/files/code/train.py
new file mode 100644
index 0000000..213528a
--- /dev/null
+++ b/wandb/run-20250430_215907-4anamnlf/files/code/train.py
@@ -0,0 +1,516 @@
+import os
+import random
+import gc
+from collections import defaultdict
+from functools import partial
+
+import numpy as np
+np.set_printoptions(precision=4, suppress=True)
+
+from PIL import Image
+from tqdm.notebook import tqdm
+from sklearn.metrics import accuracy_score
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import torch.distributed as dist
+import torch.multiprocessing as mp
+from torch.nn.parallel import DistributedDataParallel as DDP
+from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.data import DataLoader, random_split, Sampler, Subset
+from torch.utils.data.distributed import DistributedSampler
+
+import torchvision
+from torchvision import transforms, datasets
+
+import wandb
+
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+from models import ResNet, BasicBlock
+from utils import compute_wandb_metrics
+from eval import run_lda_on_embeddings
+
+def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+
+
+class Solver:
+    def __init__(self, dataloaders, model_path, n_classes, lda_args={}, local_rank=0, world_size=1, lr=1e-3, 
+                 gradient_accumulation_steps=1, use_amp=True, use_checkpoint=True):
+        self.dataloaders = dataloaders
+        self.local_rank = local_rank
+        self.world_size = world_size
+        self.device = torch.device(f'cuda:{local_rank}')
+        self.gradient_accumulation_steps = gradient_accumulation_steps
+        self.use_amp = use_amp
+        
+        # Create model with checkpointing enabled
+        self.net = ResNet18(n_classes, lda_args, use_checkpoint=use_checkpoint)
+        self.net = self.net.to(self.device)
+        
+        # Wrap model with DDP
+        if world_size > 1:
+            self.net = DDP(self.net, device_ids=[local_rank], output_device=local_rank,
+                           find_unused_parameters=False)  # Set to True only if needed
+        
+        self.use_lda = True if lda_args else False
+        if self.use_lda:
+            self.criterion = sina_loss 
+        else:
+            self.criterion = nn.CrossEntropyLoss()
+        
+        if local_rank == 0:
+            print(f"Using criterion: {self.criterion}")
+            print(f"Using checkpoint: {use_checkpoint}")
+            print(f"Using mixed precision: {use_amp}")
+            print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+        self.model_path = model_path
+        self.n_classes = n_classes
+
+    def get_net(self):
+        return self.net.module if isinstance(self.net, DDP) else self.net
+
+    def handle_lda(self, inputs, targets, epoch, batch_idx):
+        net = self.get_net()
+        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
+    
+        if hasComplexEVal:
+            if self.local_rank == 0:
+                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
+            return None, None, None
+    
+        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+        loss = self.criterion(sigma_w_inv_b)
+        outputs = net.lda.predict_proba(feas)
+    
+        if self.local_rank == 0:
+            wandb.log(metrics, commit=False)
+            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
+    
+        return loss, outputs, feas, sigma_w_inv_b
+
+    def iterate(self, epoch, phase):
+        get_net = self.get_net()
+        get_net.train(phase == 'train')
+    
+        dataloader = self.dataloaders[phase]
+        total_loss = 0
+        correct = 0
+        total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+    
+        torch.cuda.empty_cache()
+        gc.collect()
+    
+        for batch_idx, (inputs, targets) in enumerate(dataloader):
+            inputs = inputs.to(self.device, non_blocking=True)
+            targets = targets.to(self.device, non_blocking=True)
+    
+            if phase == 'train':
+                self.optimizer.zero_grad(set_to_none=True)
+                with torch.cuda.amp.autocast(enabled=self.use_amp):
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, feas, sigma_w_inv_b = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+                self.scaler.scale(loss).backward()
+                self.scaler.unscale_(self.optimizer)
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+                self.scaler.step(self.optimizer)
+                self.scaler.update()
+    
+                if self.local_rank == 0:
+                    wandb.log({"grad_norm": grad_norm.item()})
+            else:
+                with torch.no_grad():
+                    if self.use_lda:
+                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
+                        if result is None:
+                            continue
+                        loss, outputs, _, _ = result
+                    else:
+                        outputs = get_net(inputs, targets, epoch)
+                        loss = self.criterion(outputs, targets)
+    
+            total_loss += loss.item()
+            pred = torch.argmax(outputs.detach(), dim=1)
+            total += targets.size(0)
+            correct += pred.eq(targets).sum().item()
+    
+            del inputs, targets, outputs
+            if self.use_lda and phase == 'train' and result is not None:
+                del feas, sigma_w_inv_b
+            torch.cuda.empty_cache()
+    
+            
+            # Sync metrics across GPUs
+            if self.world_size > 1:
+                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+                total_loss, correct, total = metrics.tolist()
+                
+            total_loss /= (batch_idx + 1) * self.world_size
+            if total > 0:
+                total_acc = correct / total
+            else:
+                total_acc = 0 
+            
+            # Log metrics
+            if self.local_rank == 0:
+                if entropy_count > 0:
+                    average_entropy = entropy_sum / entropy_count
+                    print(f'Average Entropy: {average_entropy:.4f}')
+                
+                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+                wandb.log({
+                    f"epoch_{phase}": epoch,
+                    f"loss_{phase}": total_loss,
+                    f"acc_{phase}": 100.*total_acc
+                }) 
+            return total_loss, total_acc
+            
+
+    def save_checkpoint(self, epoch, val_loss, suffix=''):
+        checkpoint = {
+            'epoch': epoch,
+            'val_loss': val_loss,
+            'state_dict': self.get_net().state_dict()
+        }
+        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
+        torch.save(checkpoint, path)
+
+    def train(self, epochs):
+        best_loss = float('inf')
+    
+        for epoch in range(epochs):
+            # Set epoch for distributed samplers
+            if self.world_size > 1:
+                for phase in self.dataloaders:
+                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
+                    if hasattr(sampler, 'set_epoch'):
+                        sampler.set_epoch(epoch)
+    
+            # Training phase (we ignore returned values here)
+            self.iterate(epoch, 'train')
+    
+            # Validation phase
+            with torch.no_grad():
+                val_loss, val_acc = self.iterate(epoch, 'val')
+            
+            # All processes run this to contribute their part of the embeddings
+            import time
+            start_time = time.time()
+            lda_accuracy = run_lda_on_embeddings(
+                self.dataloaders['complete_train'],
+                self.dataloaders['val'],
+                self.get_net(),
+                use_amp=self.use_amp
+            )
+            
+            # Only rank 0 gets accuracy; others get None
+            if self.local_rank == 0 and lda_accuracy is not None:
+                wandb.log({'lda_accuracy': lda_accuracy})
+                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
+                print(f"Total time: {elapsed_time:.2f} minutes")
+
+    
+            # Save best model
+            if self.local_rank == 0:
+                if val_loss < best_loss:
+                    best_loss = val_loss
+                    print('Best val loss found')
+                    self.save_checkpoint(epoch, val_loss)
+    
+                print()
+    
+        # Final save
+        if self.local_rank == 0:
+            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
+
+
+def setup(rank, world_size):
+    os.environ['MASTER_ADDR'] = 'localhost'
+    os.environ['MASTER_PORT'] = '12355'
+    
+    # Initialize the process group
+    dist.init_process_group("nccl", rank=rank, world_size=world_size)
+
+
+def cleanup():
+    dist.destroy_process_group()
+    
+def train_worker(rank, world_size, config):
+    import warnings
+    warnings.simplefilter(action='ignore', category=FutureWarning)
+    
+    class ClassBalancedBatchSampler(Sampler):
+        def __init__(self, dataset, k_classes: int, n_samples: int,
+                     world_size: int = 1, rank: int = 0, seed: int = 42):
+            """
+            Class-balanced batch sampler for distributed training.
+    
+            Args:
+                dataset: Dataset to sample from.
+                k_classes: Number of different classes in each batch.
+                n_samples: Number of samples per class.
+                world_size: Total number of distributed workers.
+                rank: Rank of the current worker.
+                seed: Random seed for reproducibility.
+            """
+            super().__init__(dataset)
+            self.dataset = dataset
+            self.k_classes = k_classes
+            self.n_samples = n_samples
+            self.world_size = world_size
+            self.rank = rank
+            self.seed = seed
+            self.epoch = 0  # Set externally before each epoch
+    
+            # Get target labels and build class-to-indices mapping
+            if isinstance(dataset, torch.utils.data.Subset):
+                indices = dataset.indices
+                targets = [dataset.dataset.targets[i] for i in indices]
+            else:
+                indices = range(len(dataset))
+                targets = dataset.targets
+    
+            self.class_to_indices = defaultdict(list)
+            for idx, label in zip(indices, targets):
+                self.class_to_indices[label].append(idx)
+    
+            # Filter out classes with insufficient samples
+            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                      if len(idxs) >= n_samples]
+            if len(self.available_classes) < k_classes:
+                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
+                                 f"but only {len(self.available_classes)} are available.")
+    
+            # Estimate batches per epoch
+            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+            batch_size = k_classes * n_samples
+            self.batches_per_epoch = total_samples // batch_size
+    
+        def set_epoch(self, epoch: int):
+            self.epoch = epoch
+    
+        def __iter__(self):
+            rng = random.Random(self.seed + self.epoch + self.rank)
+            num_batches = 0
+            batch_size = self.k_classes * self.n_samples
+    
+            while num_batches < self.batches_per_epoch:
+                selected_classes = rng.sample(self.available_classes, self.k_classes)
+    
+                batch = np.empty(batch_size, dtype=int)
+                offset = 0
+                for cls in selected_classes:
+                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
+                    batch[offset:offset + self.n_samples] = sampled_indices
+                    offset += self.n_samples
+    
+                # Shard to the correct worker
+                if num_batches % self.world_size == self.rank:
+                    yield batch.tolist()
+    
+                num_batches += 1
+    
+        def __len__(self):
+            return self.batches_per_epoch // self.world_size
+
+            
+    # Configure CUDA
+    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+    
+    # Setup process group
+    setup(rank, world_size)
+    
+    # Set the device
+    torch.cuda.set_device(rank)
+    
+    # Initialize wandb only on rank 0
+    if rank == 0:
+        wandb.init(
+            project=config['wandb_project'],
+            entity=config['wandb_entity'],
+            group=config['wandb_group'],
+            config=config,  # Track configuration
+        )
+    
+    # Set seeds for reproducibility
+    seed = config['seed'] + rank  # Different seed per process
+    torch.manual_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    
+    # Calculate effective batch size and adjust learning rate
+    global_batch_size = config['k_classes'] * config['n_samples'] * world_size
+    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
+    effective_batch_size = global_batch_size * gradient_accumulation_steps
+    base_lr = config.get('base_lr', 1e-3)
+    lr = base_lr#get_scaled_lr_sqrt(effective_batch_size, base_batch_size=config.get('base_batch_size', 128), base_lr=base_lr)
+    
+    if rank == 0:
+        print(f"Global batch size: {global_batch_size}")
+        print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+        print(f"Effective batch size: {effective_batch_size}")
+        print(f"Learning rate: {lr}")
+    
+    # Data loading code (same as original)
+    # ImageNet normalization
+    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+    transform_train = transforms.Compose([
+        transforms.RandomResizedCrop(224),
+        transforms.RandomHorizontalFlip(),
+        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
+        transforms.ToTensor(),
+        normalize,
+    ])
+
+    transform_test = transforms.Compose([
+        transforms.Resize(256),
+        transforms.CenterCrop(224),
+        transforms.ToTensor(),
+        normalize,
+    ])
+    
+    # Create subset
+    trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+    valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+    testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+
+    # Create distributed samplers
+    train_sampler = ClassBalancedBatchSampler(
+        dataset=trainset,
+        k_classes=config['k_classes'],
+        n_samples=config['n_samples'],
+        world_size=world_size,
+        rank=rank,
+        seed=config['seed']
+    )
+
+
+    val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+    test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
+    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
+    
+
+    # Create dataloaders
+    trainloader = torch.utils.data.DataLoader(
+        trainset,
+        batch_sampler=train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+        #persistent_workers=False
+    )
+
+    
+    valloader = torch.utils.data.DataLoader(
+        valset, 
+        batch_size=config['batch_size'],
+        sampler=val_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+    
+    testloader = torch.utils.data.DataLoader(
+        testset, 
+        batch_size=config['batch_size'],
+        sampler=test_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+        
+    complete_train_loader = torch.utils.data.DataLoader(
+        trainset, 
+        batch_size=config['batch_size'],
+        sampler=complete_train_sampler,
+        num_workers=config['num_workers'],
+        pin_memory=True,
+    )
+
+    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+    
+    if config['loss'] == 'LDA':
+        lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+    else:
+        lda_args = {}
+        
+    # Create solver with optimized parameters
+    solver = Solver(
+        dataloaders=dataloaders, 
+        model_path=config['model_path'],
+        n_classes=config['n_classes'],
+        lda_args=lda_args if config['loss'] == 'LDA' else {},
+        local_rank=rank,
+        world_size=world_size,
+        lr=lr,
+        gradient_accumulation_steps=gradient_accumulation_steps,
+        use_amp=config.get('use_amp', True),
+        use_checkpoint=config.get('use_checkpointing', True)
+    )
+    
+    # Train
+    solver.train(config['epochs'])
+    
+    # Test
+    solver.test()
+    
+    # Clean up
+    cleanup()
+
+
+if __name__ == '__main__':
+    # Configuration with memory optimizations
+    config = {
+        'wandb_project': "DELETEME",
+        'wandb_entity': "gerardo-pastrana-c3-ai",
+        'wandb_group': "gapLoss",
+        'seed': 42,
+        'n_classes': 1000,
+        'train_val_split': 0.1,
+        'batch_size': 8192,  # Global batch size
+        'num_workers': 1,  # Adjust based on CPU cores
+        'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+        'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+        'test_dir': '/data/datasets/imagenet_full_size/061417/test',
+        'model_path': 'models/deeplda_best.pth',
+        'loss': 'LDA',
+        'lamb': 0.1,
+        'n_eig': 4,
+        'margin': None,
+        'epochs': 20,
+        'k_classes': 64,
+        'n_samples': 128,
+        # Memory optimization parameters
+        'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+        'use_amp': True,                   # Use automatic mixed precision
+        'use_checkpointing': True,         # Use gradient checkpointing
+        'base_lr': 1e-3,                   # Base learning rate
+        'base_batch_size': 128,            # Reference batch size for LR scaling
+        'cuda_visible_devices': '',        # Optional GPU restrictions
+    }
+    
+    # Number of available GPUs
+    n_gpus = torch.cuda.device_count()
+    print(f"Using {n_gpus} GPUs")
+    
+    # Launch processes
+    mp.spawn(
+        train_worker,
+        args=(n_gpus, config),
+        nprocs=n_gpus,
+        join=True
+    )
\ No newline at end of file
diff --git a/wandb/run-20250430_215907-4anamnlf/files/diff.patch b/wandb/run-20250430_215907-4anamnlf/files/diff.patch
new file mode 100644
index 0000000..740eade
--- /dev/null
+++ b/wandb/run-20250430_215907-4anamnlf/files/diff.patch
@@ -0,0 +1,664 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..213528a 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,176 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            import time
++            start_time = time.time()
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
++                print(f"Total time: {elapsed_time:.2f} minutes")
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -268,19 +251,22 @@ def cleanup():
+     dist.destroy_process_group()
+     
+ def train_worker(rank, world_size, config):
++    import warnings
++    warnings.simplefilter(action='ignore', category=FutureWarning)
++    
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +275,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +385,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +404,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +432,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +481,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +492,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/utils.py b/utils.py
+index 2f6d99a..e0ec60e 100644
+--- a/utils.py
++++ b/utils.py
+@@ -48,4 +48,5 @@ def compute_wandb_metrics(outputs, sigma_w_inv_b):
+         "diag_var": diag_var,
+     }
+ 
+-    return metrics
+\ No newline at end of file
++    return metrics
++
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..08e9ddd 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215907-4anamnlf
+\ No newline at end of file
diff --git a/wandb/run-20250430_215907-4anamnlf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch b/wandb/run-20250430_215907-4anamnlf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
new file mode 100644
index 0000000..740eade
--- /dev/null
+++ b/wandb/run-20250430_215907-4anamnlf/files/diff_f0ce46e354e145e1031feb27540f1ba9fa3e2aa5.patch
@@ -0,0 +1,664 @@
+Submodule apex contains modified content
+diff --git a/apex/setup.py b/apex/setup.py
+index 4aa6616..3e369a7 100644
+--- a/apex/setup.py
++++ b/apex/setup.py
+@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
+     print("\nCompiling cuda extensions with")
+     print(raw_output + "from " + cuda_dir + "/bin\n")
+ 
+-    if (bare_metal_version != torch_binary_version):
+-        raise RuntimeError(
+-            "Cuda extensions are being compiled with a version of Cuda that does "
+-            "not match the version used to compile Pytorch binaries.  "
+-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+-            + "In some cases, a minor-version mismatch will not cause later errors:  "
+-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+-            "You can try commenting out this check (at your own risk)."
+-        )
++    # if (bare_metal_version != torch_binary_version):
++    #     raise RuntimeError(
++    #         "Cuda extensions are being compiled with a version of Cuda that does "
++    #         "not match the version used to compile Pytorch binaries.  "
++    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
++    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
++    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
++    #         "You can try commenting out this check (at your own risk)."
++    #     )
+ 
+ 
+ def raise_if_cuda_home_none(global_option: str) -> None:
+diff --git a/lda.py b/lda.py
+index d99fab3..89aac69 100644
+--- a/lda.py
++++ b/lda.py
+@@ -135,7 +135,7 @@ def sina_loss(sigma_w_inv_b):
+     # # loss = torch.norm(diff, p='fro')**2
+ 
+     # penalty = (trace - lambda_target).pow(2)  # scale-free, minimal tuning
+-    lambda_target = torch.tensor(2**14, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
++    lambda_target = torch.tensor(2**5, dtype=sigma_w_inv_b.dtype, device=sigma_w_inv_b.device)
+     penalty = (trace - lambda_target).pow(2) / lambda_target  # scale-free, minimal tuning
+ 
+     loss = torch.log(max_frobenius_norm) -   torch.log(trace) + penalty
+@@ -162,14 +162,8 @@ class LDA(nn.Module):
+         self.n_components = n_classes - 1
+         self.lamb = lamb
+         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+-        self.running_stats = None  # Stores cumulative LDA stats
+ 
+     def forward(self, X, y):
+-        # Initialize or update running stats
+-        if self.running_stats is None:
+-            self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
+-        self.running_stats.update(X, y)
+-
+         # Perform batch-wise LDA (temporary, not global yet)
+         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+ 
+@@ -180,41 +174,6 @@ class LDA(nn.Module):
+ 
+         return hasComplexEVal, evals, sigma_w_inv_b
+ 
+-    def finalize_running_stats(self):
+-        """Compute global LDA parameters from accumulated running stats."""
+-        if self.running_stats is None:
+-            raise RuntimeError("No running stats available. Call forward() with data first.")
+-
+-        Sw, Sb, Xc_mean = self.running_stats.finalize(self.lamb)
+-
+-        Sw, Sb, Xc_mean = Sw.to("cuda"), Sb.to("cuda"), Xc_mean.to("cuda")
+-        temp = torch.linalg.solve(Sw, Sb)
+-        evals_complex, evecs_complex = torch.linalg.eig(temp)
+-
+-        tol = 1e-6
+-        is_complex = torch.abs(evals_complex.imag) > tol
+-        real_idx = ~is_complex
+-        evals = evals_complex[real_idx].real
+-        evecs = evecs_complex[:, real_idx].real
+-
+-        if evals.numel() > 0:
+-            evals, inc_idx = torch.sort(evals)
+-            evecs = evecs[:, inc_idx]
+-        else:
+-            print("Warning: All eigenvalues were complex.")
+-            evals = torch.tensor([], dtype=temp.dtype)
+-            evecs = torch.zeros((temp.shape[0], 0), dtype=temp.dtype)
+-
+-        self.scalings_ = evecs
+-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())
+-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t()))
+-
+-        return evals  # Optional: return eigenvalues
+-
+-    def reset_running_stats(self):
+-        """Reset accumulated running stats."""
+-        self.running_stats = None
+-
+     def transform(self, X):
+         return X.matmul(self.scalings_)[:, :self.n_components]
+ 
+diff --git a/train.py b/train.py
+index d179128..213528a 100644
+--- a/train.py
++++ b/train.py
+@@ -30,6 +30,7 @@ import wandb
+ from lda import LDA, lda_loss, sina_loss, SphericalLDA
+ from models import ResNet, BasicBlock
+ from utils import compute_wandb_metrics
++from eval import run_lda_on_embeddings
+ 
+ def ResNet18(num_classes=1000, lda_args=None, use_checkpoint=True, segments=4):
+     return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, lda_args, use_checkpoint, segments)
+@@ -56,7 +57,7 @@ class Solver:
+         
+         self.use_lda = True if lda_args else False
+         if self.use_lda:
+-            self.criterion = sina_loss  # Assuming this is defined elsewhere
++            self.criterion = sina_loss 
+         else:
+             self.criterion = nn.CrossEntropyLoss()
+         
+@@ -67,194 +68,176 @@ class Solver:
+             print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
+ 
+         self.optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=5e-4)
+-        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
++        self.scaler = torch.amp.GradScaler(enabled=use_amp)
+         self.model_path = model_path
+         self.n_classes = n_classes
+ 
++    def get_net(self):
++        return self.net.module if isinstance(self.net, DDP) else self.net
++
++    def handle_lda(self, inputs, targets, epoch, batch_idx):
++        net = self.get_net()
++        hasComplexEVal, feas, outputs, sigma_w_inv_b = net(inputs, targets, epoch)
++    
++        if hasComplexEVal:
++            if self.local_rank == 0:
++                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++            return None, None, None
++    
++        metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
++        loss = self.criterion(sigma_w_inv_b)
++        outputs = net.lda.predict_proba(feas)
++    
++        if self.local_rank == 0:
++            wandb.log(metrics, commit=False)
++            wandb.log({'loss': loss.item(), 'epoch': epoch}, commit=False)
++    
++        return loss, outputs, feas, sigma_w_inv_b
++
+     def iterate(self, epoch, phase):
+-        if isinstance(self.net, DDP):
+-            self.net.module.train(phase == 'train')
+-        else:
+-            self.net.train(phase == 'train')
+-            
++        get_net = self.get_net()
++        get_net.train(phase == 'train')
++    
+         dataloader = self.dataloaders[phase]
+         total_loss = 0
+         correct = 0
+         total = 0
+         entropy_sum = 0.0
+         entropy_count = 0
+-
+-        # Clear CUDA cache before each epoch
++    
+         torch.cuda.empty_cache()
+         gc.collect()
+-        
++    
+         for batch_idx, (inputs, targets) in enumerate(dataloader):
+-            # Move data to device
+             inputs = inputs.to(self.device, non_blocking=True)
+             targets = targets.to(self.device, non_blocking=True)
+-            
+-            # For training with gradient accumulation
++    
+             if phase == 'train':
+-               
+                 self.optimizer.zero_grad(set_to_none=True)
+-                
+-                # Apply mixed precision for training
+                 with torch.cuda.amp.autocast(enabled=self.use_amp):
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            # Stats calculation (same as original)
+-                            metrics = compute_wandb_metrics(outputs, sigma_w_inv_b)
+-                            entropy_sum += metrics["entropy"]
+-                            entropy_count += 1
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                            
+-                            # Only log on rank 0 for efficiency
+-                            if phase == 'train' and self.local_rank == 0:
+-                                wandb.log(metrics, commit=False)
+-                                wandb.log({
+-                                    'loss': loss.item(),
+-                                    'epoch': epoch,
+-                                }, commit=False)
+-                        else:
+-                            if self.local_rank == 0:
+-                                print(f'Complex Eigenvalues found, skipping batch {batch_idx}')
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, feas, sigma_w_inv_b = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-                
+-                # Scale loss for gradient accumulation
+-                #loss = loss / self.gradient_accumulation_steps
+-                
+-                if phase == 'train':
+-                    # Use gradient scaler for mixed precision
+-                    self.scaler.scale(loss).backward()
+-                    
+-                    # Step optimizer at effective batch boundaries
+-                    #if (effective_batch_idx == self.gradient_accumulation_steps - 1) or (batch_idx == len(dataloader) - 1):
+-                    # Unscale before clipping
+-                    self.scaler.unscale_(self.optimizer)
+-                    grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
+-                    
+-                    # Update with scaler
+-                    self.scaler.step(self.optimizer)
+-                    self.scaler.update()
+-                    
+-                    if self.local_rank == 0:
+-                        wandb.log({"grad_norm": grad_norm.item()})
++    
++                self.scaler.scale(loss).backward()
++                self.scaler.unscale_(self.optimizer)
++                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
++                self.scaler.step(self.optimizer)
++                self.scaler.update()
++    
++                if self.local_rank == 0:
++                    wandb.log({"grad_norm": grad_norm.item()})
+             else:
+-                # Validation phase - no gradients needed
+                 with torch.no_grad():
+                     if self.use_lda:
+-                        if isinstance(self.net, DDP):
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net.module(inputs, targets, epoch)
+-                        else:
+-                            hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets, epoch)
+-                        
+-                        if not hasComplexEVal:
+-                            loss = self.criterion(sigma_w_inv_b)
+-                            
+-                            if isinstance(self.net, DDP):
+-                                outputs = self.net.module.lda.predict_proba(feas)
+-                            else:
+-                                outputs = self.net.lda.predict_proba(feas)
+-                        else:
++                        result = self.handle_lda(inputs, targets, epoch, batch_idx)
++                        if result is None:
+                             continue
++                        loss, outputs, _, _ = result
+                     else:
+-                        outputs = self.net(inputs, targets, epoch)
++                        outputs = get_net(inputs, targets, epoch)
+                         loss = self.criterion(outputs, targets)
+-            
+-            # Accumulate metrics
+-            total_loss += loss.item()  if phase == 'train' else loss.item()
+-            
+-            outputs = torch.argmax(outputs.detach(), dim=1)
++    
++            total_loss += loss.item()
++            pred = torch.argmax(outputs.detach(), dim=1)
+             total += targets.size(0)
+-            correct += outputs.eq(targets).sum().item()
+-            
+-            # Free memory after each batch
++            correct += pred.eq(targets).sum().item()
++    
+             del inputs, targets, outputs
+-            if phase == 'train' and self.use_lda and not hasComplexEVal:
++            if self.use_lda and phase == 'train' and result is not None:
+                 del feas, sigma_w_inv_b
+             torch.cuda.empty_cache()
+-        
+-        # Sync metrics across GPUs
+-        if self.world_size > 1:
+-            metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
+-            dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
+-            total_loss, correct, total = metrics.tolist()
++    
+             
+-        total_loss /= (batch_idx + 1) * self.world_size
+-        if total > 0:
+-            total_acc = correct / total
+-        else:
+-            total_acc = 0 
+-        
+-        # Log metrics
+-        if self.local_rank == 0:
+-            if entropy_count > 0:
+-                average_entropy = entropy_sum / entropy_count
+-                print(f'Average Entropy: {average_entropy:.4f}')
++            # Sync metrics across GPUs
++            if self.world_size > 1:
++                metrics = torch.tensor([total_loss, correct, total], dtype=torch.float32, device=self.device)
++                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
++                total_loss, correct, total = metrics.tolist()
++                
++            total_loss /= (batch_idx + 1) * self.world_size
++            if total > 0:
++                total_acc = correct / total
++            else:
++                total_acc = 0 
++            
++            # Log metrics
++            if self.local_rank == 0:
++                if entropy_count > 0:
++                    average_entropy = entropy_sum / entropy_count
++                    print(f'Average Entropy: {average_entropy:.4f}')
++                
++                print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
++                wandb.log({
++                    f"epoch_{phase}": epoch,
++                    f"loss_{phase}": total_loss,
++                    f"acc_{phase}": 100.*total_acc
++                }) 
++            return total_loss, total_acc
+             
+-            print(f'\nepoch {epoch}: {phase} loss: {total_loss:.3f} | acc: {100.*total_acc:.2f}% ({correct}/{total})')
+-            wandb.log({
+-                f"epoch_{phase}": epoch,
+-                f"loss_{phase}": total_loss,
+-                f"acc_{phase}": 100.*total_acc
+-            }) 
+-        return total_loss, total_acc
+ 
++    def save_checkpoint(self, epoch, val_loss, suffix=''):
++        checkpoint = {
++            'epoch': epoch,
++            'val_loss': val_loss,
++            'state_dict': self.get_net().state_dict()
++        }
++        path = self.model_path if not suffix else self.model_path.replace('.pth', f'_{suffix}.pth')
++        torch.save(checkpoint, path)
+ 
+     def train(self, epochs):
+         best_loss = float('inf')
++    
+         for epoch in range(epochs):
+             # Set epoch for distributed samplers
+             if self.world_size > 1:
+                 for phase in self.dataloaders:
+-                    if hasattr(self.dataloaders[phase].sampler, 'set_epoch'):
+-                        self.dataloaders[phase].sampler.set_epoch(epoch)
+-            
+-            # Training phase
++                    sampler = getattr(self.dataloaders[phase], 'sampler', None)
++                    if hasattr(sampler, 'set_epoch'):
++                        sampler.set_epoch(epoch)
++    
++            # Training phase (we ignore returned values here)
+             self.iterate(epoch, 'train')
+-            
++    
+             # Validation phase
+             with torch.no_grad():
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'finalize_running_stats'):
+-                    self.net.module.lda.finalize_running_stats()
+                 val_loss, val_acc = self.iterate(epoch, 'val')
+-                if hasattr(self.net.module, 'lda') and hasattr(self.net.module.lda, 'reset_running_stats'):
+-                    self.net.module.lda.reset_running_stats()
+-                
+-                
+-            # Save best model
+-            if val_loss < best_loss and self.local_rank == 0:
+-                best_loss = val_loss
+-                if isinstance(self.net, DDP):
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-                else:
+-                    checkpoint = {'epoch': epoch, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-                print('best val loss found')
+-                torch.save(checkpoint, self.model_path)
+             
++            # All processes run this to contribute their part of the embeddings
++            import time
++            start_time = time.time()
++            lda_accuracy = run_lda_on_embeddings(
++                self.dataloaders['complete_train'],
++                self.dataloaders['val'],
++                self.get_net(),
++                use_amp=self.use_amp
++            )
++            
++            # Only rank 0 gets accuracy; others get None
++            if self.local_rank == 0 and lda_accuracy is not None:
++                wandb.log({'lda_accuracy': lda_accuracy})
++                elapsed_time = (time.time() - start_time) / 60  # convert to minutes
++                print(f"Total time: {elapsed_time:.2f} minutes")
++
++    
++            # Save best model
+             if self.local_rank == 0:
++                if val_loss < best_loss:
++                    best_loss = val_loss
++                    print('Best val loss found')
++                    self.save_checkpoint(epoch, val_loss)
++    
+                 print()
+-        
+-        # Final save on main process
++    
++        # Final save
+         if self.local_rank == 0:
+-            if isinstance(self.net, DDP):
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.module.state_dict()}
+-            else:
+-                checkpoint = {'epoch': epochs-1, 'val_loss': val_loss, 'state_dict': self.net.state_dict()}
+-            torch.save(checkpoint, self.model_path.replace('.pth', '_final.pth'))
++            self.save_checkpoint(epochs - 1, val_loss, suffix='final')
++
+ 
+ def setup(rank, world_size):
+     os.environ['MASTER_ADDR'] = 'localhost'
+@@ -268,19 +251,22 @@ def cleanup():
+     dist.destroy_process_group()
+     
+ def train_worker(rank, world_size, config):
++    import warnings
++    warnings.simplefilter(action='ignore', category=FutureWarning)
++    
+     class ClassBalancedBatchSampler(Sampler):
+-        def __init__(self, dataset, k_classes, n_samples,
+-                     world_size=1, rank=0, seed=42):
++        def __init__(self, dataset, k_classes: int, n_samples: int,
++                     world_size: int = 1, rank: int = 0, seed: int = 42):
+             """
+             Class-balanced batch sampler for distributed training.
+-            
++    
+             Args:
+-                dataset: Dataset to sample from
+-                k_classes: Number of classes per batch
+-                n_samples: Number of samples per class
+-                world_size: Number of processes (GPUs)
+-                rank: Local rank of this process
+-                seed: Random seed
++                dataset: Dataset to sample from.
++                k_classes: Number of different classes in each batch.
++                n_samples: Number of samples per class.
++                world_size: Total number of distributed workers.
++                rank: Rank of the current worker.
++                seed: Random seed for reproducibility.
+             """
+             super().__init__(dataset)
+             self.dataset = dataset
+@@ -289,85 +275,61 @@ def train_worker(rank, world_size, config):
+             self.world_size = world_size
+             self.rank = rank
+             self.seed = seed
+-            self.epoch = 0  # must be set each epoch manually!
++            self.epoch = 0  # Set externally before each epoch
+     
+-            # Build mapping from class to list of indices
++            # Get target labels and build class-to-indices mapping
+             if isinstance(dataset, torch.utils.data.Subset):
+-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
++                indices = dataset.indices
++                targets = [dataset.dataset.targets[i] for i in indices]
+             else:
++                indices = range(len(dataset))
+                 targets = dataset.targets
+-            
+-            self.class_to_indices = {}
+-            for idx, target in enumerate(targets):
+-                if target not in self.class_to_indices:
+-                    self.class_to_indices[target] = []
+-                self.class_to_indices[target].append(idx)
+     
+-            # Only keep classes that have enough samples
++            self.class_to_indices = defaultdict(list)
++            for idx, label in zip(indices, targets):
++                self.class_to_indices[label].append(idx)
++    
++            # Filter out classes with insufficient samples
+             self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                       if len(idxs) >= n_samples]
+-            
+-            assert len(self.available_classes) >= k_classes, \
+-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
++            if len(self.available_classes) < k_classes:
++                raise ValueError(f"Need at least {k_classes} classes with {n_samples} samples each, "
++                                 f"but only {len(self.available_classes)} are available.")
+     
+-            # Compute approximately how many batches can fit
++            # Estimate batches per epoch
+             total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+-            batch_size = self.k_classes * self.n_samples
++            batch_size = k_classes * n_samples
+             self.batches_per_epoch = total_samples // batch_size
+     
+-        def set_epoch(self, epoch):
++        def set_epoch(self, epoch: int):
+             self.epoch = epoch
+     
+         def __iter__(self):
+-            g = torch.Generator()
+-            g.manual_seed(self.seed + self.epoch + self.rank)
+-
++            rng = random.Random(self.seed + self.epoch + self.rank)
+             num_batches = 0
+-            while num_batches < self.batches_per_epoch:
+-                selected_classes = torch.tensor(self.available_classes)
+-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+-            
+-                batch = []
+-                for cls in selected_classes.tolist():
+-                    indices = self.class_to_indices[cls]
+-                    indices_tensor = torch.tensor(indices)
+-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-                    batch.extend(chosen_indices.tolist())
+-            
+-                # Shard based on rank
+-                if num_batches % self.world_size == self.rank:
+-                    yield batch
+-            
+-                num_batches += 1
+-
+-    
+-            # all_batches = []
+-    
+-            # while len(all_batches) < self.batches_per_epoch:
+-            #     # Pick k_classes randomly
+-            #     selected_classes = torch.tensor(self.available_classes)
+-            #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
++            batch_size = self.k_classes * self.n_samples
+     
+-            #     batch = []
+-            #     for cls in selected_classes.tolist():
+-            #         indices = self.class_to_indices[cls]
+-            #         indices_tensor = torch.tensor(indices)
+-            #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+-            #         batch.extend(chosen_indices.tolist())
++            while num_batches < self.batches_per_epoch:
++                selected_classes = rng.sample(self.available_classes, self.k_classes)
+     
+-            #     all_batches.append(batch)
++                batch = np.empty(batch_size, dtype=int)
++                offset = 0
++                for cls in selected_classes:
++                    sampled_indices = rng.sample(self.class_to_indices[cls], self.n_samples)
++                    batch[offset:offset + self.n_samples] = sampled_indices
++                    offset += self.n_samples
+     
+-            # # Shard batches across GPUs
+-            # local_batches = all_batches[self.rank::self.world_size]
++                # Shard to the correct worker
++                if num_batches % self.world_size == self.rank:
++                    yield batch.tolist()
+     
+-            # for batch in local_batches:
+-            #     yield batch
++                num_batches += 1
+     
+         def __len__(self):
+             return self.batches_per_epoch // self.world_size
++
+             
+     # Configure CUDA
+-    #os.environ['CUDA_VISIBLE_DEVICES'] = config.get('cuda_visible_devices', '')  # Optional GPU ID restrictions
+     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Help with fragmentation
+     
+     # Setup process group
+@@ -423,8 +385,8 @@ def train_worker(rank, world_size, config):
+         transforms.ToTensor(),
+         normalize,
+     ])
+-
+-    # Create datasets
++    
++    # Create subset
+     trainset = datasets.ImageFolder(config['train_dir'], transform=transform_train)
+     valset = datasets.ImageFolder(config['val_dir'], transform=transform_test)
+     testset = datasets.ImageFolder(config['test_dir'], transform=transform_test)
+@@ -442,6 +404,8 @@ def train_worker(rank, world_size, config):
+ 
+     val_sampler = DistributedSampler(valset, num_replicas=world_size, rank=rank, shuffle=False)
+     test_sampler = DistributedSampler(testset, num_replicas=world_size, rank=rank, shuffle=False)
++    complete_train_sampler = DistributedSampler(trainset, num_replicas=world_size, rank=rank, shuffle=False)
++    
+ 
+     # Create dataloaders
+     trainloader = torch.utils.data.DataLoader(
+@@ -468,8 +432,16 @@ def train_worker(rank, world_size, config):
+         num_workers=config['num_workers'],
+         pin_memory=True,
+     )
++        
++    complete_train_loader = torch.utils.data.DataLoader(
++        trainset, 
++        batch_size=config['batch_size'],
++        sampler=complete_train_sampler,
++        num_workers=config['num_workers'],
++        pin_memory=True,
++    )
+ 
+-    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader}
++    dataloaders = {'train': trainloader, 'val': valloader, 'test': testloader, 'complete_train':complete_train_loader}
+     
+     if config['loss'] == 'LDA':
+         lda_args = {'lamb': config['lamb'], 'n_eig': config['n_eig'], 'margin': config['margin']}
+@@ -509,7 +481,7 @@ if __name__ == '__main__':
+         'seed': 42,
+         'n_classes': 1000,
+         'train_val_split': 0.1,
+-        'batch_size': 4096,  # Global batch size
++        'batch_size': 8192,  # Global batch size
+         'num_workers': 1,  # Adjust based on CPU cores
+         'train_dir': '/data/datasets/imagenet_full_size/061417/train',
+         'val_dir': '/data/datasets/imagenet_full_size/061417/val',
+@@ -520,8 +492,8 @@ if __name__ == '__main__':
+         'n_eig': 4,
+         'margin': None,
+         'epochs': 20,
+-        'k_classes':128 ,
+-        'n_samples': 64,
++        'k_classes': 64,
++        'n_samples': 128,
+         # Memory optimization parameters
+         'gradient_accumulation_steps': 1,  # Accumulate gradients to save memory
+         'use_amp': True,                   # Use automatic mixed precision
+diff --git a/utils.py b/utils.py
+index 2f6d99a..e0ec60e 100644
+--- a/utils.py
++++ b/utils.py
+@@ -48,4 +48,5 @@ def compute_wandb_metrics(outputs, sigma_w_inv_b):
+         "diag_var": diag_var,
+     }
+ 
+-    return metrics
+\ No newline at end of file
++    return metrics
++
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 409e3b2..08e9ddd 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20250430_152714-naxvk16q
+\ No newline at end of file
++run-20250430_215907-4anamnlf
+\ No newline at end of file
diff --git a/wandb/run-20250430_215907-4anamnlf/files/requirements.txt b/wandb/run-20250430_215907-4anamnlf/files/requirements.txt
new file mode 100644
index 0000000..8431e5f
--- /dev/null
+++ b/wandb/run-20250430_215907-4anamnlf/files/requirements.txt
@@ -0,0 +1,101 @@
+GitPython==3.1.44
+portalocker==3.1.1
+charset-normalizer==3.4.1
+python3-openid==3.2.0
+platformdirs==4.3.7
+SQLAlchemy==2.0.40
+nvidia-nccl-cu12==2.21.5
+nvidia-cusparselt-cu12==0.6.2
+smmap==5.0.2
+pbkdf2==1.3
+zope.deprecation==5.1
+setproctitle==1.3.5
+plaster-pastedeploy==1.0.1
+sentry-sdk==2.26.1
+opencv-python==4.11.0.86
+click==8.1.8
+requests-oauthlib==2.0.0
+numpy==2.0.2
+velruse==1.1.1
+fsspec==2025.3.2
+pydantic==2.11.3
+nvidia-cuda-cupti-cu12==12.4.127
+certifi==2025.1.31
+mpmath==1.3.0
+urllib3==2.4.0
+zope.sqlalchemy==3.1
+MarkupSafe==3.0.2
+pydantic_core==2.33.1
+typing-inspection==0.4.0
+repoze.sendmail==4.4.1
+WebOb==1.8.9
+gitdb==4.0.12
+annotated-types==0.7.0
+nvidia-cufft-cu12==11.2.1.3
+torchvision==0.21.0
+translationstring==1.4
+apex==0.1
+nvidia-cuda-runtime-cu12==12.4.127
+psutil==7.0.0
+idna==3.10
+typing_extensions==4.13.2
+scipy==1.13.1
+requests==2.32.3
+nvidia-nvtx-cu12==12.4.127
+networkx==3.2.1
+nvidia-curand-cu12==10.3.5.147
+nvidia-cusolver-cu12==11.6.1.9
+pip==25.0.1
+nvidia-nvjitlink-cu12==12.4.127
+cryptacular==1.6.2
+six==1.17.0
+scikit-learn==1.6.1
+defusedxml==0.7.1
+triton==3.2.0
+setuptools==78.1.0
+sympy==1.13.1
+plaster==1.1.2
+nvidia-cusparse-cu12==12.3.1.170
+anykeystore==0.2
+protobuf==5.29.4
+venusian==3.1.1
+zope.interface==7.2
+wandb==0.19.9
+tqdm==4.67.1
+nvidia-cuda-nvrtc-cu12==12.4.127
+packaging==25.0
+greenlet==3.2.0
+PyYAML==6.0.2
+transaction==5.0
+hupper==1.12.1
+joblib==1.4.2
+Jinja2==3.1.6
+nvidia-cudnn-cu12==9.1.0.70
+oauthlib==3.2.2
+WTForms==3.2.1
+pyramid==2.0.2
+pyramid-mailer==0.15.1
+filelock==3.18.0
+nvidia-cublas-cu12==12.4.5.8
+wtforms-recaptcha==0.3.2
+pillow==11.2.1
+PasteDeploy==3.1.0
+docker-pycreds==0.4.0
+torch==2.6.0
+threadpoolctl==3.6.0
+typing_extensions==4.12.2
+tomli==2.0.1
+importlib_metadata==8.0.0
+jaraco.functools==4.0.1
+more-itertools==10.3.0
+autocommand==2.2.2
+backports.tarfile==1.2.0
+jaraco.collections==5.1.0
+typeguard==4.3.0
+jaraco.context==5.3.0
+inflect==7.3.1
+packaging==24.2
+jaraco.text==3.12.1
+wheel==0.45.1
+platformdirs==4.2.2
+zipp==3.19.2
diff --git a/wandb/run-20250430_215907-4anamnlf/files/wandb-metadata.json b/wandb/run-20250430_215907-4anamnlf/files/wandb-metadata.json
new file mode 100644
index 0000000..e7775d1
--- /dev/null
+++ b/wandb/run-20250430_215907-4anamnlf/files/wandb-metadata.json
@@ -0,0 +1,60 @@
+{
+  "os": "Linux-5.15.146+-x86_64-with-glibc2.31",
+  "python": "CPython 3.11.4",
+  "startedAt": "2025-04-30T21:59:07.602004Z",
+  "program": "/workspace/Utsav/DeepLDA/train.py",
+  "codePath": "train.py",
+  "git": {
+    "remote": "https://github.com/c3-gerardopastrana/supervised_learning.git",
+    "commit": "f0ce46e354e145e1031feb27540f1ba9fa3e2aa5"
+  },
+  "email": "utsavdutta98@gmail.com",
+  "root": "/workspace/Utsav/DeepLDA",
+  "host": "finetuning-80gb-4-4-6bd95cd6df-hszql",
+  "executable": "/root/.cache/pypoetry/virtualenvs/deeplda-project-Iynu0haq-py3.11/bin/python",
+  "codePathLocal": "train.py",
+  "cpu_count": 48,
+  "cpu_count_logical": 96,
+  "gpu": "NVIDIA A100-SXM4-80GB",
+  "gpu_count": 4,
+  "disk": {
+    "/": {
+      "total": "3168432029696",
+      "used": "2535768838144"
+    }
+  },
+  "memory": {
+    "total": "1437341159424"
+  },
+  "cpu": {
+    "count": 48,
+    "countLogical": 96
+  },
+  "gpu_nvidia": [
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    },
+    {
+      "name": "NVIDIA A100-SXM4-80GB",
+      "memoryTotal": "85899345920",
+      "cudaCores": 6912,
+      "architecture": "Ampere"
+    }
+  ],
+  "cudaVersion": "12.4"
+}
\ No newline at end of file
diff --git a/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb b/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb
new file mode 100644
index 0000000..e96792e
Binary files /dev/null and b/wandb/run-20250430_215907-4anamnlf/run-4anamnlf.wandb differ
