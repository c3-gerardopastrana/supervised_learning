diff --git a/Untitled.ipynb b/Untitled.ipynb
deleted file mode 100644
index 363fcab..0000000
--- a/Untitled.ipynb
+++ /dev/null
@@ -1,6 +0,0 @@
-{
- "cells": [],
- "metadata": {},
- "nbformat": 4,
- "nbformat_minor": 5
-}
Submodule apex contains modified content
diff --git a/apex/setup.py b/apex/setup.py
index 4aa6616..3e369a7 100644
--- a/apex/setup.py
+++ b/apex/setup.py
@@ -37,15 +37,15 @@ def check_cuda_torch_binary_vs_bare_metal(cuda_dir):
     print("\nCompiling cuda extensions with")
     print(raw_output + "from " + cuda_dir + "/bin\n")
 
-    if (bare_metal_version != torch_binary_version):
-        raise RuntimeError(
-            "Cuda extensions are being compiled with a version of Cuda that does "
-            "not match the version used to compile Pytorch binaries.  "
-            "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
-            + "In some cases, a minor-version mismatch will not cause later errors:  "
-            "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
-            "You can try commenting out this check (at your own risk)."
-        )
+    # if (bare_metal_version != torch_binary_version):
+    #     raise RuntimeError(
+    #         "Cuda extensions are being compiled with a version of Cuda that does "
+    #         "not match the version used to compile Pytorch binaries.  "
+    #         "Pytorch binaries were compiled with Cuda {}.\n".format(torch.version.cuda)
+    #         + "In some cases, a minor-version mismatch will not cause later errors:  "
+    #         "https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  "
+    #         "You can try commenting out this check (at your own risk)."
+    #     )
 
 
 def raise_if_cuda_home_none(global_option: str) -> None:
diff --git a/lda.py b/lda.py
index 5b8bae9..e14d5f8 100644
--- a/lda.py
+++ b/lda.py
@@ -165,9 +165,6 @@ class LDA(nn.Module):
         self.running_stats = None  # Stores cumulative LDA stats
 
     def forward(self, X, y):
-        X = X.view(X.shape[0], -1).detach()
-        y = y.detach()
-
         # Initialize or update running stats
         if self.running_stats is None:
             self.running_stats = RunningLDAStats(self.n_classes, X.shape[1], device='cpu')
@@ -246,8 +243,8 @@ class RunningLDAStats:
 
     @torch.no_grad()
     def update(self, X, y):
-        X = X.view(X.shape[0], -1).cpu()
-        y = y.cpu()
+        X = X.view(X.shape[0], -1).detach().to('cpu')
+        y = y.detach().to('cpu')
 
         for cls in range(self.n_classes):
             mask = (y == cls)
diff --git a/train.py b/train.py
index 146c137..cc1eccc 100644
--- a/train.py
+++ b/train.py
@@ -462,85 +462,103 @@ def cleanup():
 
 
 def train_worker(rank, world_size, config):
-    
     class ClassBalancedBatchSampler(Sampler):
-        def __init__(self, dataset, k_classes, n_samples,
-                     world_size=1, rank=0, seed=42):
-            """
-            Class-balanced batch sampler for distributed training.
-            
-            Args:
-                dataset: Dataset to sample from
-                k_classes: Number of classes per batch
-                n_samples: Number of samples per class
-                world_size: Number of processes (GPUs)
-                rank: Local rank of this process
-                seed: Random seed
-            """
-            super().__init__(dataset)
-            self.dataset = dataset
-            self.k_classes = k_classes
-            self.n_samples = n_samples
-            self.world_size = world_size
-            self.rank = rank
-            self.seed = seed
-            self.epoch = 0  # must be set each epoch manually!
-    
-            # Build mapping from class to list of indices
-            if isinstance(dataset, torch.utils.data.Subset):
-                targets = [dataset.dataset.targets[i] for i in dataset.indices]
-            else:
-                targets = dataset.targets
-            
-            self.class_to_indices = {}
-            for idx, target in enumerate(targets):
-                if target not in self.class_to_indices:
-                    self.class_to_indices[target] = []
-                self.class_to_indices[target].append(idx)
-    
-            # Only keep classes that have enough samples
-            self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
-                                      if len(idxs) >= n_samples]
-            
-            assert len(self.available_classes) >= k_classes, \
-                f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
-    
-            # Compute approximately how many batches can fit
-            total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
-            batch_size = self.k_classes * self.n_samples
-            self.batches_per_epoch = total_samples // batch_size
-    
-        def set_epoch(self, epoch):
-            self.epoch = epoch
-    
-        def __iter__(self):
-            g = torch.Generator()
-            g.manual_seed(self.seed + self.epoch + self.rank)
-    
-            all_batches = []
-    
-            while len(all_batches) < self.batches_per_epoch:
-                # Pick k_classes randomly
-                selected_classes = torch.tensor(self.available_classes)
-                selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
-    
-                batch = []
-                for cls in selected_classes.tolist():
-                    indices = self.class_to_indices[cls]
-                    indices_tensor = torch.tensor(indices)
-                    chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
-                    batch.extend(chosen_indices.tolist())
-    
-                all_batches.append(batch)
-    
-            # Shard batches across GPUs
-            local_batches = all_batches[self.rank::self.world_size]
-    
-            for batch in local_batches:
-                yield batch
+            def __init__(self, dataset, k_classes, n_samples,
+                         world_size=1, rank=0, seed=42):
+                """
+                Class-balanced batch sampler for distributed training.
+                
+                Args:
+                    dataset: Dataset to sample from
+                    k_classes: Number of classes per batch
+                    n_samples: Number of samples per class
+                    world_size: Number of processes (GPUs)
+                    rank: Local rank of this process
+                    seed: Random seed
+                """
+                super().__init__(dataset)
+                self.dataset = dataset
+                self.k_classes = k_classes
+                self.n_samples = n_samples
+                self.world_size = world_size
+                self.rank = rank
+                self.seed = seed
+                self.epoch = 0  # must be set each epoch manually!
+        
+                # Build mapping from class to list of indices
+                if isinstance(dataset, torch.utils.data.Subset):
+                    targets = [dataset.dataset.targets[i] for i in dataset.indices]
+                else:
+                    targets = dataset.targets
+                
+                self.class_to_indices = {}
+                for idx, target in enumerate(targets):
+                    if target not in self.class_to_indices:
+                        self.class_to_indices[target] = []
+                    self.class_to_indices[target].append(idx)
+        
+                # Only keep classes that have enough samples
+                self.available_classes = [cls for cls, idxs in self.class_to_indices.items()
+                                          if len(idxs) >= n_samples]
+                
+                assert len(self.available_classes) >= k_classes, \
+                    f"Only {len(self.available_classes)} classes have {n_samples}+ samples, but need {k_classes}"
+        
+                # Compute approximately how many batches can fit
+                total_samples = sum(len(self.class_to_indices[cls]) for cls in self.available_classes)
+                batch_size = self.k_classes * self.n_samples
+                self.batches_per_epoch = total_samples // batch_size
+        
+            def set_epoch(self, epoch):
+                self.epoch = epoch
+        
+            def __iter__(self):
+                g = torch.Generator()
+                g.manual_seed(self.seed + self.epoch + self.rank)
+    
+                num_batches = 0
+                while num_batches < self.batches_per_epoch:
+                    selected_classes = torch.tensor(self.available_classes)
+                    selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+                
+                    batch = []
+                    for cls in selected_classes.tolist():
+                        indices = self.class_to_indices[cls]
+                        indices_tensor = torch.tensor(indices)
+                        chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                        batch.extend(chosen_indices.tolist())
+                
+                    # Shard based on rank
+                    if num_batches % self.world_size == self.rank:
+                        yield batch
+                
+                    num_batches += 1
     
-        def __len__(self):
-            return self.batches_per_epoch // self.world_size
+        
+                # all_batches = []
+        
+                # while len(all_batches) < self.batches_per_epoch:
+                #     # Pick k_classes randomly
+                #     selected_classes = torch.tensor(self.available_classes)
+                #     selected_classes = selected_classes[torch.randperm(len(selected_classes), generator=g)][:self.k_classes]
+        
+                #     batch = []
+                #     for cls in selected_classes.tolist():
+                #         indices = self.class_to_indices[cls]
+                #         indices_tensor = torch.tensor(indices)
+                #         chosen_indices = indices_tensor[torch.randperm(len(indices_tensor), generator=g)][:self.n_samples]
+                #         batch.extend(chosen_indices.tolist())
+        
+                #     all_batches.append(batch)
+        
+                # # Shard batches across GPUs
+                # local_batches = all_batches[self.rank::self.world_size]
+        
+                # for batch in local_batches:
+                #     yield batch
+        
+            def __len__(self):
+                return self.batches_per_epoch // self.world_size
 
 
 
@@ -673,13 +691,13 @@ if __name__ == '__main__':
         'n_eig': 4,
         'margin': None,
         'epochs': 100,
-        'k_classes': 64, 
-        'n_samples': 64, 
+        'k_classes': 30, 
+        'n_samples': 100, 
     }
 
     
     # Number of available GPUs
-    n_gpus = 8
+    n_gpus = 4
     
     # Launch processes
     mp.spawn(
diff --git a/wandb/latest-run b/wandb/latest-run
index 538ff58..30735cf 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250429_053315-pm5qboq7
\ No newline at end of file
+run-20250430_040930-ay6bukuh
\ No newline at end of file
