diff --git a/lda.py b/lda.py
index cc368cf..e3b2195 100644
--- a/lda.py
+++ b/lda.py
@@ -1,7 +1,7 @@
 import torch
 import torch.nn as nn
 from functools import partial
-
+import torch.nn.functional as F
 
 def lda(X, y, n_classes, lamb):
     # flatten X
@@ -27,32 +27,62 @@ def lda(X, y, n_classes, lamb):
 
     # cope for numerical instability
     Sw += torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+    #Sw = (1-lamb)*Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+
+    # compute eigen decomposition
+    temp = torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) #torch.linalg.solve(sigma_b, Sw ) 
+    # # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
+    # evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
+    # print(evals.shape, evecs.shape)
+
+    # # remove complex eigen values and sort
+    # noncomplex_idx = evals[:, 1] == 0
+    # evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
+    # evecs = evecs[:, noncomplex_idx]
+    # evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
+    # evecs = evecs[:, inc_idx]
+    # print(evals.shape, evecs.shape)
+
+    # # flag to indicate if to skip backpropagation
+    # hasComplexEVal = evecs.shape[1] < evecs.shape[0]
 
+    # return hasComplexEVal, Xc_mean, evals, evecs
     # compute eigen decomposition
-    temp = Sw.pinverse().matmul(Sb)
     # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
-    evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
-    print(evals.shape, evecs.shape)
+    # Use the new torch.linalg.eig for general matrices
+    # It returns complex eigenvalues and eigenvectors by default
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
 
-    # remove complex eigen values and sort
-    noncomplex_idx = evals[:, 1] == 0
-    evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
-    evecs = evecs[:, noncomplex_idx]
-    evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
-    evecs = evecs[:, inc_idx]
-    print(evals.shape, evecs.shape)
+    # Process complex eigenvalues returned by torch.linalg.eig
+    # Check for eigenvalues with non-negligible imaginary parts
+    tol = 1e-6 # Tolerance for considering imaginary part zero
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex) # Flag if *any* eigenvalue was complex beyond tolerance
 
-    # flag to indicate if to skip backpropagation
-    hasComplexEVal = evecs.shape[1] < evecs.shape[0]
+    if hasComplexEVal:
+         # Optional: Print a warning if complex eigenvalues are detected
+         print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real eigenvalues.")
 
-    return hasComplexEVal, Xc_mean, evals, evecs
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real 
+    evecs = evecs_complex[:, real_idx].real
+
+    if evals.numel() > 0: # Check if any real eigenvalues are left
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex. Eigenvalue/vector tensors are empty.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        D = temp.shape[0]
+        evecs = torch.tensor([[] for _ in range(D)], dtype=temp.dtype, device=temp.device)
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
 
 
 def lda_loss(evals, n_classes, n_eig=None, margin=None):
     n_components = n_classes - 1
     evals = evals[-n_components:]
     # evecs = evecs[:, -n_components:]
-    print('evals', evals.shape, evals)
+    # print('evals', evals.shape, evals)
     # print('evecs', evecs.shape)
 
     # calculate loss
@@ -60,8 +90,19 @@ def lda_loss(evals, n_classes, n_eig=None, margin=None):
         threshold = torch.min(evals) + margin
         n_eig = torch.sum(evals < threshold)
     loss = -torch.mean(evals[:n_eig]) # small eigen values are on left
+    eigvals_norm = evals / evals.sum()
+    eps = 1e-10 
+    eigvals_norm = torch.clamp(eigvals_norm, min=eps)
+    oss = (eigvals_norm * eigvals_norm.log()).sum()
+    #loss = torch.log(eigvals_norm.max()-eigvals_norm.min())
+    return loss
+    
+def sina_loss(sigma_w_inv_b):
+    max_frobenius_norm = torch.linalg.norm(sigma_w_inv_b, ord='fro')
+    trace = torch.trace(sigma_w_inv_b)
+    loss = torch.log(max_frobenius_norm / trace)
+    loss = torch.log(1 / trace)
     return loss
-
 
 class LDA(nn.Module):
     def __init__(self, n_classes, lamb):
@@ -73,7 +114,7 @@ class LDA(nn.Module):
 
     def forward(self, X, y):
         # perform LDA
-        hasComplexEVal, Xc_mean, evals, evecs = self.lda_layer(X, y)  # CxD, D, DxD
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)  # CxD, D, DxD
 
         # compute LDA statistics
         self.scalings_ = evecs  # projection matrix, DxD
@@ -81,7 +122,7 @@ class LDA(nn.Module):
         self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t())) # C
 
         # return self.transform(X)
-        return hasComplexEVal, evals
+        return hasComplexEVal, evals, sigma_w_inv_b
 
     def transform(self, X):
         """ transform data """
@@ -104,6 +145,158 @@ class LDA(nn.Module):
 
 
 
+def spherical_lda(X, y, n_classes, lamb):
+    N, D = X.shape
+    labels, counts = torch.unique(y, return_counts=True)
+    assert len(labels) == n_classes  # require all classes to be present
+    
+    # Compute global mean direction and normalize
+    global_mean = torch.mean(X, 0)
+    global_mean = F.normalize(global_mean, p=2, dim=0)
+    
+    # Initialize containers
+    class_means_list = []
+    Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    Sb = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    
+    # Calculate all class means
+    for c in labels:
+        class_idx = int(c)
+        Xc = X[y == c]
+        class_mean = F.normalize(torch.mean(Xc, dim=0), p=2, dim=0)
+        class_means_list.append(class_mean)
+    
+    Xc_mean = torch.stack(class_means_list)
+    
+    # Compute scatter matrices
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        Xc = X[y == c]
+        class_mean = Xc_mean[i]
+
+        # Vectorized cosine similarities
+        cos_similarities = Xc @ class_mean
+        cos_similarities = torch.clamp(cos_similarities, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        # Vectorized difference: samples projected away from mean direction
+        diffs = Xc - cos_similarities.unsqueeze(1) * class_mean.unsqueeze(0)
+
+        # Vectorized scatter matrix
+        class_scatter = diffs.T @ diffs
+
+        Sw = Sw + class_scatter
+    
+    Sw = Sw / N  # Normalize by total number of samples
+    
+    # Compute between-class scatter
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        class_mean = Xc_mean[i]
+        cos_sim = torch.dot(class_mean, global_mean)
+        cos_sim = torch.clamp(cos_sim, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        diff = class_mean.unsqueeze(1) - cos_sim * global_mean.unsqueeze(1)
+        diff_outer = diff @ diff.T
+        Sb = Sb + (Nc / N) * diff_outer
+
+    
+    import numpy as np
+    from sklearn.covariance import LedoitWolf
+    
+    X_np = Sw.cpu().detach().numpy()
+
+    lw = LedoitWolf()
+    lw.fit(X_np)
+    lamb = lw.shrinkage_
+    lamb_scaled = lamb * torch.trace(Sw) / D
+    
+    #Sw_reg = Sw + torch.eye(D, dtype=X.dtype, device=X.device) * lamb_scaled
+    Sw_reg = (1-lamb_scaled) * Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb_scaled
+    
+    # Generalized eigenvalue problem
+    temp = torch.linalg.pinv(Sw_reg, hermitian=True) @ Sb
+    
+    # Eigen decomposition
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    tol = 1e-6
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex)
+    
+    if hasComplexEVal:
+        print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real ones.")
+    
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real
+    evecs = evecs_complex[:, real_idx].real
+    
+    if evals.numel() > 0:
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        evecs = torch.zeros((D, 0), dtype=temp.dtype, device=temp.device)
+    
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
+
+
+class SphericalLDA(nn.Module):
+    def __init__(self, n_classes, lamb=1e-4):
+        super(SphericalLDA, self).__init__()
+        self.n_classes = n_classes
+        self.n_components = n_classes - 1  # Maximum meaningful LDA dimensions
+        self.lamb = lamb
+        self.lda_layer = partial(spherical_lda, n_classes=n_classes, lamb=lamb)
+    
+    def forward(self, X, y):
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+        
+        # Store projection matrix 
+        self.scalings_ = evecs
+        
+        # Project class means and normalize to create prototypes
+        projected_means = Xc_mean.matmul(evecs)
+        
+        # Project back to original space and normalize to ensure they're on the hypersphere
+        self.coef_ = F.normalize(projected_means.matmul(evecs.t()), p=2, dim=1)
+        
+        # Intercept is not meaningful in spherical space when using cosine similarity
+        self.intercept_ = torch.zeros(self.n_classes, dtype=X.dtype, device=X.device)
+        
+        return hasComplexEVal, evals, sigma_w_inv_b
+    
+    def transform(self, X):
+        # Normalize input
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Project data
+        X_new = X.matmul(self.scalings_)
+        
+        # Return only the most discriminative components
+        return X_new[:, :self.n_components]
+    
+    def predict(self, X):
+        # Normalize input embeddings
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Compute cosine similarities with class prototypes
+        similarities = X.matmul(self.coef_.t())
+        
+        # Return class with highest similarity
+        return torch.argmax(similarities, dim=1)
+    
+    def predict_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        
+        # Convert similarities to probabilities using softmax
+        proba = nn.functional.softmax(similarities, dim=1)
+        return proba
+    
+    def predict_log_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        log_proba = nn.functional.log_softmax(similarities, dim=1)
+        return log_proba
+
 if __name__ == '__main__':
     import numpy as np
     np.set_printoptions(precision=4, suppress=True)
diff --git a/train.py b/train.py
index 7520acb..59fd83c 100644
--- a/train.py
+++ b/train.py
@@ -11,9 +11,10 @@ from PIL import Image
 import torchvision
 import torchvision.transforms as transforms
 import torch.optim as optim
-
+import wandb
 from functools import partial
-from lda import LDA, lda_loss
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+
 
 
 class BasicBlock(nn.Module):
@@ -114,9 +115,10 @@ class ResNet(nn.Module):
         out = self.layer4(out)
         out = F.avg_pool2d(out, 4)
         fea = out.view(out.size(0), -1)  # NxC
+        fea = F.normalize(fea, p=2, dim=1)
         if self.lda_args:
-            hasComplexEVal, out = self.lda(fea, y)  # evals
-            return hasComplexEVal, fea, out
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)  # evals
+            return hasComplexEVal, fea, out, sigma_w_inv_b
         else:
             out = self.linear(fea)
             return out
@@ -170,9 +172,13 @@ class Solver:
         if self.use_lda:
             self.criterion = partial(lda_loss, n_classes=n_classes, 
                                     n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
         else:
             self.criterion = nn.CrossEntropyLoss()
-        self.optimizer = optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
+        print(self.criterion)
+        
+        #self.optimizer = optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
+        self.optimizer = optim.AdamW(self.net.parameters(), lr=1e-3, weight_decay=5e-4)
         self.model_path = model_path
         self.n_classes = n_classes
 
@@ -182,39 +188,98 @@ class Solver:
         total_loss = 0
         correct = 0
         total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+        
         for batch_idx, (inputs, targets) in enumerate(dataloader):
             inputs = inputs.to(self.device)
             targets = targets.to(self.device)
             self.optimizer.zero_grad()
-            
+        
             if self.use_lda:
-                hasComplexEVal, feas, outputs = self.net(inputs, targets)
+                hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets)
                 if not hasComplexEVal:
-                    loss = self.criterion(outputs)
+
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+        
+                    #Loss
+                    loss = self.criterion(sigma_w_inv_b)
+                    #loss = (eigvals_norm * eigvals_norm.log()).sum()
                     outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train':
+                        wandb.log({
+                            'loss': loss,
+                            
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75})
+                    
                 else:
                     print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
                     continue
             else:
                 outputs = self.net(inputs, targets)
-                loss = self.criterion(outputs, targets)            
-            # print('\noutputs shape:', outputs.shape)
-            # print('loss:', loss)
+                loss = self.criterion(outputs, targets)
+        
             if phase == 'train':
+               
                 loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
                 self.optimizer.step()
+                wandb.log({"total_grad_norm_encoder":grad_norm.item()})
             total_loss += loss.item()
-
+    
             outputs = torch.argmax(outputs.detach(), dim=1)
-            # _, outputs = outputs.max(1)
             total += targets.size(0)
             correct += outputs.eq(targets).sum().item()
+        
         total_loss /= (batch_idx + 1)
-        total_acc = correct/total
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if entropy_count > 0:
+            average_entropy = entropy_sum / entropy_count
+            print(f'Average Entropy: {average_entropy:.4f}')
+        
         print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
                      % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+        wandb.log({
+            "epoch"+phase:epoch,
+             "total"+phase:total_loss,
+             "total_acc_train"+phase: 100.*total_acc
+        }) 
         return total_loss, total_acc
 
+
     def train(self, epochs):
         best_loss = float('inf')
         for epoch in range(epochs):
@@ -279,6 +344,12 @@ def parse_dir(img_dir, classes, randnum=-1):
 
 
 if __name__ == '__main__':
+    wandb.init(
+    project="DeepLDA",
+    entity="gerardo-pastrana-c3-ai",
+    group="gapLoss",
+    )
+    
     transform_train = transforms.Compose([
         transforms.RandomCrop(32, padding=2),
         transforms.RandomHorizontalFlip(),
@@ -296,17 +367,18 @@ if __name__ == '__main__':
     train_val_split = 0.2
     batch_size = 5000
     num_workers = 4
-    gpu = -1
+    gpu = 3
 
     train_dir = '../data/cifar10/imgs/train'
     test_dir = '../data/cifar10/imgs/test'
     model_path = '../data/cifar10/exp1015/deeplda_best.pth'
 
     loss = 'LDA' # CE or LDA
-    lamb = 0.0001
+    lamb = 0.1#0.0001
     n_eig = 4
     margin = None
     lda_args = {'lamb':lamb, 'n_eig':n_eig, 'margin':margin} if loss == 'LDA' else {}
+    
 
     class_map = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 
                  'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}
@@ -330,5 +402,6 @@ if __name__ == '__main__':
 
     dataloaders = {'train':trainloader, 'val':valloader, 'test':testloader}
     solver = Solver(dataloaders, model_path, n_classes, lda_args, gpu)
-    solver.train(20)
+    
+    solver.train(5000)
     solver.test()
