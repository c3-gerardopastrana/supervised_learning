diff --git a/lda.py b/lda.py
index cc368cf..4061f49 100644
--- a/lda.py
+++ b/lda.py
@@ -1,58 +1,88 @@
 import torch
 import torch.nn as nn
 from functools import partial
+import torch.nn.functional as F
 
+# def lda(X, y, n_classes, lamb):
+#     # flatten X
+#     X = X.view(X.shape[0], -1)
+#     N, D = X.shape
 
-def lda(X, y, n_classes, lamb):
-    # flatten X
-    X = X.view(X.shape[0], -1)
-    N, D = X.shape
+#     # count unique labels in y
+#     labels, counts = torch.unique(y, return_counts=True)
+#     assert len(labels) == n_classes  # require X,y cover all classes
 
-    # count unique labels in y
-    labels, counts = torch.unique(y, return_counts=True)
-    assert len(labels) == n_classes  # require X,y cover all classes
+#     # compute mean-centered observations and covariance matrix
+#     X_bar = X - torch.mean(X, 0)
+#     Xc_mean = torch.zeros((n_classes, D), dtype=X.dtype, device=X.device, requires_grad=False)
+#     St = X_bar.t().matmul(X_bar) / (N - 1)  # total scatter matrix
+#     Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device, requires_grad=True)  # within-class scatter matrix
+#     for c, Nc in zip(labels, counts):
+#         Xc = X[y == c]
+#         Xc_mean[int(c), :] = torch.mean(Xc, 0)
+#         Xc_bar = Xc - Xc_mean[int(c), :]
+#         Sw = Sw + Xc_bar.t().matmul(Xc_bar) / (Nc - 1)
+#     Sw /= n_classes
+#     Sb = St - Sw  # between scatter matrix
 
-    # compute mean-centered observations and covariance matrix
-    X_bar = X - torch.mean(X, 0)
-    Xc_mean = torch.zeros((n_classes, D), dtype=X.dtype, device=X.device, requires_grad=False)
-    St = X_bar.t().matmul(X_bar) / (N - 1)  # total scatter matrix
-    Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device, requires_grad=True)  # within-class scatter matrix
-    for c, Nc in zip(labels, counts):
-        Xc = X[y == c]
-        Xc_mean[int(c), :] = torch.mean(Xc, 0)
-        Xc_bar = Xc - Xc_mean[int(c), :]
-        Sw = Sw + Xc_bar.t().matmul(Xc_bar) / (Nc - 1)
-    Sw /= n_classes
-    Sb = St - Sw  # between scatter matrix
+#     # cope for numerical instability
+#     Sw += torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+#     #Sw = (1-lamb)*Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+
+#     # compute eigen decomposition
+#     temp = torch.linalg.pinv(Sw, hermitian=True).matmul(Sb) #torch.linalg.solve(sigma_b, Sw ) 
+#     # # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
+#     # evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
+#     # print(evals.shape, evecs.shape)
+
+#     # # remove complex eigen values and sort
+#     # noncomplex_idx = evals[:, 1] == 0
+#     # evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
+#     # evecs = evecs[:, noncomplex_idx]
+#     # evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
+#     # evecs = evecs[:, inc_idx]
+#     # print(evals.shape, evecs.shape)
 
-    # cope for numerical instability
-    Sw += torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+#     # # flag to indicate if to skip backpropagation
+#     # hasComplexEVal = evecs.shape[1] < evecs.shape[0]
 
-    # compute eigen decomposition
-    temp = Sw.pinverse().matmul(Sb)
-    # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
-    evals, evecs = torch.eig(temp, eigenvectors=True) # shipped from nightly-built version (1.8.0.dev20201015)
-    print(evals.shape, evecs.shape)
+#     # return hasComplexEVal, Xc_mean, evals, evecs
+#     # compute eigen decomposition
+#     # evals, evecs = torch.symeig(temp, eigenvectors=True) # only works for symmetric matrix
+#     # Use the new torch.linalg.eig for general matrices
+#     # It returns complex eigenvalues and eigenvectors by default
+#     evals_complex, evecs_complex = torch.linalg.eig(temp)
 
-    # remove complex eigen values and sort
-    noncomplex_idx = evals[:, 1] == 0
-    evals = evals[:, 0][noncomplex_idx] # take real part of eigen values
-    evecs = evecs[:, noncomplex_idx]
-    evals, inc_idx = torch.sort(evals) # sort by eigen values, in ascending order
-    evecs = evecs[:, inc_idx]
-    print(evals.shape, evecs.shape)
+#     # Process complex eigenvalues returned by torch.linalg.eig
+#     # Check for eigenvalues with non-negligible imaginary parts
+#     tol = 1e-6 # Tolerance for considering imaginary part zero
+#     is_complex = torch.abs(evals_complex.imag) > tol
+#     hasComplexEVal = torch.any(is_complex) # Flag if *any* eigenvalue was complex beyond tolerance
 
-    # flag to indicate if to skip backpropagation
-    hasComplexEVal = evecs.shape[1] < evecs.shape[0]
+#     if hasComplexEVal:
+#          # Optional: Print a warning if complex eigenvalues are detected
+#          print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real eigenvalues.")
 
-    return hasComplexEVal, Xc_mean, evals, evecs
+#     real_idx = ~is_complex
+#     evals = evals_complex[real_idx].real 
+#     evecs = evecs_complex[:, real_idx].real
+
+#     if evals.numel() > 0: # Check if any real eigenvalues are left
+#         evals, inc_idx = torch.sort(evals)
+#         evecs = evecs[:, inc_idx]
+#     else:
+#         print("Warning: All eigenvalues were complex. Eigenvalue/vector tensors are empty.")
+#         evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+#         D = temp.shape[0]
+#         evecs = torch.tensor([[] for _ in range(D)], dtype=temp.dtype, device=temp.device)
+#     return hasComplexEVal, Xc_mean, evals, evecs, temp
 
 
 def lda_loss(evals, n_classes, n_eig=None, margin=None):
     n_components = n_classes - 1
     evals = evals[-n_components:]
     # evecs = evecs[:, -n_components:]
-    print('evals', evals.shape, evals)
+    # print('evals', evals.shape, evals)
     # print('evecs', evecs.shape)
 
     # calculate loss
@@ -60,8 +90,111 @@ def lda_loss(evals, n_classes, n_eig=None, margin=None):
         threshold = torch.min(evals) + margin
         n_eig = torch.sum(evals < threshold)
     loss = -torch.mean(evals[:n_eig]) # small eigen values are on left
+    eigvals_norm = evals / evals.sum()
+    eps = 1e-10 
+    eigvals_norm = torch.clamp(eigvals_norm, min=eps)
+    oss = (eigvals_norm * eigvals_norm.log()).sum()
+    #loss = torch.log(eigvals_norm.max()-eigvals_norm.min())
+    return loss
+    
+def sina_loss(sigma_w_inv_b):
+    max_frobenius_norm = torch.linalg.norm(sigma_w_inv_b, ord='fro')
+    trace = torch.trace(sigma_w_inv_b)
+    loss = torch.log(max_frobenius_norm / trace)
     return loss
 
+# class LDA(nn.Module):
+#     def __init__(self, n_classes, lamb):
+#         super(LDA, self).__init__()
+#         self.n_classes = n_classes
+#         self.n_components = n_classes - 1
+#         self.lamb = lamb
+#         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
+
+#     def forward(self, X, y):
+#         # perform LDA
+#         hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)  # CxD, D, DxD
+
+#         # compute LDA statistics
+#         self.scalings_ = evecs  # projection matrix, DxD
+#         self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())  # CxD
+#         self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t())) # C
+
+#         # return self.transform(X)
+#         return hasComplexEVal, evals, sigma_w_inv_b
+
+#     def transform(self, X):
+#         """ transform data """
+#         X_new = X.matmul(self.scalings_)
+#         return X_new[:, :self.n_components]
+
+#     def predict(self, X):
+#         logit = X.matmul(self.coef_.t()) + self.intercept_
+#         return torch.argmax(logit, dim=1)
+
+#     def predict_proba(self, X):
+#         logit = X.matmul(self.coef_.t()) + self.intercept_
+#         proba = nn.functional.softmax(logit, dim=1)
+#         return proba
+
+#     def predict_log_proba(self, X):
+#         logit = X.matmul(self.coef_.t()) + self.intercept_
+#         log_proba = nn.functional.log_softmax(logit, dim=1)
+#         return log_proba
+
+def lda(X, y, n_classes, lamb):
+    # flatten X
+    X = X.view(X.shape[0], -1)
+    N, D = X.shape
+
+    labels, counts = torch.unique(y, return_counts=True)
+    assert len(labels) == n_classes  # require all classes to be present
+
+    X_bar = X - torch.mean(X, 0)  # global mean centering
+    Xc_mean = torch.zeros((n_classes, D), dtype=X.dtype, device=X.device, requires_grad=False)
+    St = X_bar.t().matmul(X_bar) / (N - 1)  # total scatter matrix
+    Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device, requires_grad=True)  # within-class scatter
+
+    for c, Nc in zip(labels, counts):
+        Xc = X[y == c]
+        Xc_mean[int(c), :] = F.normalize(torch.mean(Xc, dim=0), p=2, dim=0)  # Normalize class means
+        Xc_bar = Xc - Xc_mean[int(c), :]  # centered wrt normalized mean
+        Sw = Sw + Xc_bar.t().matmul(Xc_bar) / (Nc - 1)
+    Sw = Sw / n_classes
+    Sb = St - Sw  # between scatter matrix
+
+    # Stabilize the within-class scatter matrix
+    Sw = Sw + torch.eye(D, dtype=X.dtype, device=X.device) * lamb
+
+    temp = torch.linalg.pinv(Sw, hermitian=True).matmul(Sb)
+
+    # Eigen decomposition â€” filter out complex values
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    tol = 1e-6
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex)
+
+    if hasComplexEVal:
+        print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real ones.")
+
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real
+    evecs = evecs_complex[:, real_idx].real
+
+    if evals.numel() > 0:
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        D = temp.shape[0]
+        evecs = torch.tensor([[] for _ in range(D)], dtype=temp.dtype, device=temp.device)
+
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
+
+# ----------------------------------------
+# Loss functions stay the same.
+# ----------------------------------------
 
 class LDA(nn.Module):
     def __init__(self, n_classes, lamb):
@@ -72,38 +205,183 @@ class LDA(nn.Module):
         self.lda_layer = partial(lda, n_classes=n_classes, lamb=lamb)
 
     def forward(self, X, y):
-        # perform LDA
-        hasComplexEVal, Xc_mean, evals, evecs = self.lda_layer(X, y)  # CxD, D, DxD
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
 
-        # compute LDA statistics
-        self.scalings_ = evecs  # projection matrix, DxD
-        self.coef_ = Xc_mean.matmul(evecs).matmul(evecs.t())  # CxD
-        self.intercept_ = -0.5 * torch.diagonal(Xc_mean.matmul(self.coef_.t())) # C
+        # Store projection matrix and normalized class means
+        self.scalings_ = evecs  # DxD
+        self.coef_ = F.normalize(Xc_mean.matmul(evecs).matmul(evecs.t()), p=2, dim=1)  # Normalize prototypes
+        # Intercept becomes less meaningful with normalized embeddings, skip or zero it
+        self.intercept_ = torch.zeros(self.n_classes, dtype=X.dtype, device=X.device)
 
-        # return self.transform(X)
-        return hasComplexEVal, evals
+        return hasComplexEVal, evals, sigma_w_inv_b
 
     def transform(self, X):
-        """ transform data """
         X_new = X.matmul(self.scalings_)
         return X_new[:, :self.n_components]
 
     def predict(self, X):
-        logit = X.matmul(self.coef_.t()) + self.intercept_
+        # Normalize input embeddings before scoring
+        X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        logit = X.matmul(self.coef_.t())
         return torch.argmax(logit, dim=1)
 
     def predict_proba(self, X):
-        logit = X.matmul(self.coef_.t()) + self.intercept_
+        X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        logit = X.matmul(self.coef_.t())
         proba = nn.functional.softmax(logit, dim=1)
         return proba
 
     def predict_log_proba(self, X):
-        logit = X.matmul(self.coef_.t()) + self.intercept_
+        X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        logit = X.matmul(self.coef_.t())
         log_proba = nn.functional.log_softmax(logit, dim=1)
         return log_proba
 
+import torch
+import torch.nn.functional as F
+
+def spherical_lda(X, y, n_classes, lamb):
+    N, D = X.shape
+    labels, counts = torch.unique(y, return_counts=True)
+    assert len(labels) == n_classes  # require all classes to be present
+    
+    # Compute global mean direction and normalize
+    global_mean = torch.mean(X, 0)
+    global_mean = F.normalize(global_mean, p=2, dim=0)
+    
+    # Initialize containers
+    class_means_list = []
+    Sw = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    Sb = torch.zeros((D, D), dtype=X.dtype, device=X.device)
+    
+    # Calculate all class means
+    for c in labels:
+        class_idx = int(c)
+        Xc = X[y == c]
+        class_mean = F.normalize(torch.mean(Xc, dim=0), p=2, dim=0)
+        class_means_list.append(class_mean)
+    
+    Xc_mean = torch.stack(class_means_list)
+    
+    # Compute scatter matrices
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        Xc = X[y == c]
+        class_mean = Xc_mean[i]
+
+        # Vectorized cosine similarities
+        cos_similarities = Xc @ class_mean
+        cos_similarities = torch.clamp(cos_similarities, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        # Vectorized difference: samples projected away from mean direction
+        diffs = Xc - cos_similarities.unsqueeze(1) * class_mean.unsqueeze(0)
+
+        # Vectorized scatter matrix
+        class_scatter = diffs.T @ diffs
+
+        Sw = Sw + class_scatter
+    
+    Sw = Sw / N  # Normalize by total number of samples
+    
+    # Compute between-class scatter
+    for i, (c, Nc) in enumerate(zip(labels, counts)):
+        class_mean = Xc_mean[i]
+        cos_sim = torch.dot(class_mean, global_mean)
+        cos_sim = torch.clamp(cos_sim, -1.0 + 1e-6, 1.0 - 1e-6)
+
+        diff = class_mean.unsqueeze(1) - cos_sim * global_mean.unsqueeze(1)
+        diff_outer = diff @ diff.T
+        Sb = Sb + (Nc / N) * diff_outer
+    
+    # Stabilize the within-class scatter matrix
+    Sw_reg = Sw + torch.eye(D, dtype=X.dtype, device=X.device) * lamb
+    # Sw_reg = (1-lamb)*Sw + torch.eye(D, dtype=X.dtype, device=X.device, requires_grad=False) * lamb
+    
+    # Generalized eigenvalue problem
+    temp = torch.linalg.pinv(Sw_reg, hermitian=True) @ Sb
+    
+    # Eigen decomposition
+    evals_complex, evecs_complex = torch.linalg.eig(temp)
+    tol = 1e-6
+    is_complex = torch.abs(evals_complex.imag) > tol
+    hasComplexEVal = torch.any(is_complex)
+    
+    if hasComplexEVal:
+        print(f"Warning: Found {torch.sum(is_complex)} eigenvalues with imaginary part > {tol}. Keeping only real ones.")
+    
+    real_idx = ~is_complex
+    evals = evals_complex[real_idx].real
+    evecs = evecs_complex[:, real_idx].real
+    
+    if evals.numel() > 0:
+        evals, inc_idx = torch.sort(evals)
+        evecs = evecs[:, inc_idx]
+    else:
+        print("Warning: All eigenvalues were complex.")
+        evals = torch.tensor([], dtype=temp.dtype, device=temp.device)
+        evecs = torch.zeros((D, 0), dtype=temp.dtype, device=temp.device)
+    
+    return hasComplexEVal, Xc_mean, evals, evecs, temp
 
 
+class SphericalLDA(nn.Module):
+    def __init__(self, n_classes, lamb=1e-4):
+        super(SphericalLDA, self).__init__()
+        self.n_classes = n_classes
+        self.n_components = n_classes - 1  # Maximum meaningful LDA dimensions
+        self.lamb = lamb
+        self.lda_layer = partial(spherical_lda, n_classes=n_classes, lamb=lamb)
+    
+    def forward(self, X, y):
+        hasComplexEVal, Xc_mean, evals, evecs, sigma_w_inv_b = self.lda_layer(X, y)
+        
+        # Store projection matrix 
+        self.scalings_ = evecs
+        
+        # Project class means and normalize to create prototypes
+        projected_means = Xc_mean.matmul(evecs)
+        
+        # Project back to original space and normalize to ensure they're on the hypersphere
+        self.coef_ = F.normalize(projected_means.matmul(evecs.t()), p=2, dim=1)
+        
+        # Intercept is not meaningful in spherical space when using cosine similarity
+        self.intercept_ = torch.zeros(self.n_classes, dtype=X.dtype, device=X.device)
+        
+        return hasComplexEVal, evals, sigma_w_inv_b
+    
+    def transform(self, X):
+        # Normalize input
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Project data
+        X_new = X.matmul(self.scalings_)
+        
+        # Return only the most discriminative components
+        return X_new[:, :self.n_components]
+    
+    def predict(self, X):
+        # Normalize input embeddings
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        
+        # Compute cosine similarities with class prototypes
+        similarities = X.matmul(self.coef_.t())
+        
+        # Return class with highest similarity
+        return torch.argmax(similarities, dim=1)
+    
+    def predict_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        
+        # Convert similarities to probabilities using softmax
+        proba = nn.functional.softmax(similarities, dim=1)
+        return proba
+    
+    def predict_log_proba(self, X):
+        #X = F.normalize(X.view(X.shape[0], -1), p=2, dim=1)
+        similarities = X.matmul(self.coef_.t())
+        log_proba = nn.functional.log_softmax(similarities, dim=1)
+        return log_proba
+
 if __name__ == '__main__':
     import numpy as np
     np.set_printoptions(precision=4, suppress=True)
diff --git a/train.py b/train.py
index 7520acb..6e7fec0 100644
--- a/train.py
+++ b/train.py
@@ -11,9 +11,10 @@ from PIL import Image
 import torchvision
 import torchvision.transforms as transforms
 import torch.optim as optim
-
+import wandb
 from functools import partial
-from lda import LDA, lda_loss
+from lda import LDA, lda_loss, sina_loss, SphericalLDA
+
 
 
 class BasicBlock(nn.Module):
@@ -94,7 +95,7 @@ class ResNet(nn.Module):
         self.layer3 = self._make_layer(block, self.out_planes*4, num_blocks[2], stride=2)
         self.layer4 = self._make_layer(block, self.out_planes*8, num_blocks[3], stride=2)
         if self.lda_args:
-            self.lda = LDA(n_classes, lda_args['lamb'])
+            self.lda = SphericalLDA(n_classes, lda_args['lamb'])
         else:
             self.linear = nn.Linear(self.out_planes*8*block.expansion, n_classes)
 
@@ -114,9 +115,10 @@ class ResNet(nn.Module):
         out = self.layer4(out)
         out = F.avg_pool2d(out, 4)
         fea = out.view(out.size(0), -1)  # NxC
+        fea = F.normalize(fea, p=2, dim=1)
         if self.lda_args:
-            hasComplexEVal, out = self.lda(fea, y)  # evals
-            return hasComplexEVal, fea, out
+            hasComplexEVal, out, sigma_w_inv_b = self.lda(fea, y)  # evals
+            return hasComplexEVal, fea, out, sigma_w_inv_b
         else:
             out = self.linear(fea)
             return out
@@ -170,9 +172,11 @@ class Solver:
         if self.use_lda:
             self.criterion = partial(lda_loss, n_classes=n_classes, 
                                     n_eig=lda_args['n_eig'], margin=lda_args['margin'])
+            self.criterion = sina_loss
         else:
             self.criterion = nn.CrossEntropyLoss()
-        self.optimizer = optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
+        
+        self.optimizer = optim.SGD(self.net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
         self.model_path = model_path
         self.n_classes = n_classes
 
@@ -182,39 +186,98 @@ class Solver:
         total_loss = 0
         correct = 0
         total = 0
+        entropy_sum = 0.0
+        entropy_count = 0
+        
         for batch_idx, (inputs, targets) in enumerate(dataloader):
             inputs = inputs.to(self.device)
             targets = targets.to(self.device)
             self.optimizer.zero_grad()
-            
+        
             if self.use_lda:
-                hasComplexEVal, feas, outputs = self.net(inputs, targets)
+                hasComplexEVal, feas, outputs, sigma_w_inv_b = self.net(inputs, targets)
                 if not hasComplexEVal:
-                    loss = self.criterion(outputs)
+
+                    #stats
+                    eigvals_norm = outputs / outputs.sum()
+                    eps = 1e-10 
+                    max_eigval_norm = eigvals_norm.max().item()
+                    min_eigval_norm = eigvals_norm.min().item()
+                    quantile_25 = torch.quantile(eigvals_norm, 0.25).item()
+                    quantile_50 = torch.quantile(eigvals_norm, 0.5).item()
+                    quantile_75 = torch.quantile(eigvals_norm, 0.75).item()
+                    eigvals_norm = torch.clamp(outputs / outputs.sum(), min=eps, max=1.0)
+                    eigvals_norm /= eigvals_norm.sum()
+                    entropy = -(eigvals_norm * eigvals_norm.log()).sum().item()
+                    entropy_sum += entropy
+                    entropy_count += 1
+                    trace = torch.trace(sigma_w_inv_b)
+                    rank_sigma = torch.linalg.matrix_rank(sigma_w_inv_b).item()
+                    condition_sigma = torch.linalg.cond(sigma_w_inv_b).item()     
+                    off_diag = sigma_w_inv_b - torch.diag(torch.diagonal(sigma_w_inv_b))
+                    sum_squared_off_diag = torch.sum(off_diag ** 2).item()
+                    diag_var = torch.var(torch.diagonal(sigma_w_inv_b)).item()
+        
+                    #Loss
+                    loss = self.criterion(sigma_w_inv_b)
+                    #loss = (eigvals_norm * eigvals_norm.log()).sum()
                     outputs = self.net.lda.predict_proba(feas)
+
+                    if phase == 'train':
+                        wandb.log({
+                            'loss': loss,
+                            
+                            "rank simga": rank_sigma,
+                            "condition simga": condition_sigma,
+                            "entropy": entropy,
+                            "sum_squared_off_diag": sum_squared_off_diag,
+                            "diag_var": diag_var,
+                            "trace": trace,
+                            "max normalized eigenvalue": max_eigval_norm,
+                            "min normalized eigenvalue": min_eigval_norm,
+                            "quantile_25": quantile_25,
+                            "quantile_50": quantile_50,
+                            "quantile_75": quantile_75})
+                    
                 else:
                     print('Complex Eigen values found, skip backpropagation of {}th batch'.format(batch_idx))
                     continue
             else:
                 outputs = self.net(inputs, targets)
-                loss = self.criterion(outputs, targets)            
-            # print('\noutputs shape:', outputs.shape)
-            # print('loss:', loss)
+                loss = self.criterion(outputs, targets)
+        
             if phase == 'train':
+               
                 loss.backward()
+                grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)
                 self.optimizer.step()
+                wandb.log({"total_grad_norm_encoder":grad_norm.item()})
             total_loss += loss.item()
-
+    
             outputs = torch.argmax(outputs.detach(), dim=1)
-            # _, outputs = outputs.max(1)
             total += targets.size(0)
             correct += outputs.eq(targets).sum().item()
+        
         total_loss /= (batch_idx + 1)
-        total_acc = correct/total
+        if total > 0:
+            total_acc = correct / total
+        else:
+            total_acc = 0 
+        
+        if entropy_count > 0:
+            average_entropy = entropy_sum / entropy_count
+            print(f'Average Entropy: {average_entropy:.4f}')
+        
         print('\nepoch %d: %s loss: %.3f | acc: %.2f%% (%d/%d)'
                      % (epoch, phase, total_loss, 100.*total_acc, correct, total))
+        wandb.log({
+            "epoch"+phase:epoch,
+             "total"+phase:total_loss,
+             "total_acc_train"+phase: 100.*total_acc
+        }) 
         return total_loss, total_acc
 
+
     def train(self, epochs):
         best_loss = float('inf')
         for epoch in range(epochs):
@@ -279,6 +342,12 @@ def parse_dir(img_dir, classes, randnum=-1):
 
 
 if __name__ == '__main__':
+    wandb.init(
+    project="DeepLDA",
+    entity="gerardo-pastrana-c3-ai",
+    group="gapLoss",
+    )
+    
     transform_train = transforms.Compose([
         transforms.RandomCrop(32, padding=2),
         transforms.RandomHorizontalFlip(),
@@ -296,14 +365,14 @@ if __name__ == '__main__':
     train_val_split = 0.2
     batch_size = 5000
     num_workers = 4
-    gpu = -1
+    gpu = 1
 
     train_dir = '../data/cifar10/imgs/train'
     test_dir = '../data/cifar10/imgs/test'
     model_path = '../data/cifar10/exp1015/deeplda_best.pth'
 
     loss = 'LDA' # CE or LDA
-    lamb = 0.0001
+    lamb = 0.1#0.0001
     n_eig = 4
     margin = None
     lda_args = {'lamb':lamb, 'n_eig':n_eig, 'margin':margin} if loss == 'LDA' else {}
@@ -330,5 +399,5 @@ if __name__ == '__main__':
 
     dataloaders = {'train':trainloader, 'val':valloader, 'test':testloader}
     solver = Solver(dataloaders, model_path, n_classes, lda_args, gpu)
-    solver.train(20)
+    solver.train(500)
     solver.test()
